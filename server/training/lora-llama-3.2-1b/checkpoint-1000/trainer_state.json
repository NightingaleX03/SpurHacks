{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.4013605442176873,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003401360544217687,
      "grad_norm": 0.46842172741889954,
      "learning_rate": 0.001,
      "loss": 0.6796,
      "step": 1
    },
    {
      "epoch": 0.006802721088435374,
      "grad_norm": 0.3631330132484436,
      "learning_rate": 0.000999,
      "loss": 0.4199,
      "step": 2
    },
    {
      "epoch": 0.01020408163265306,
      "grad_norm": 0.35835063457489014,
      "learning_rate": 0.000998,
      "loss": 0.4543,
      "step": 3
    },
    {
      "epoch": 0.013605442176870748,
      "grad_norm": 0.8503932952880859,
      "learning_rate": 0.000997,
      "loss": 0.8282,
      "step": 4
    },
    {
      "epoch": 0.017006802721088437,
      "grad_norm": 0.5346705913543701,
      "learning_rate": 0.000996,
      "loss": 0.653,
      "step": 5
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 0.3741210997104645,
      "learning_rate": 0.000995,
      "loss": 0.3782,
      "step": 6
    },
    {
      "epoch": 0.023809523809523808,
      "grad_norm": 0.49521464109420776,
      "learning_rate": 0.000994,
      "loss": 0.5399,
      "step": 7
    },
    {
      "epoch": 0.027210884353741496,
      "grad_norm": 0.36519643664360046,
      "learning_rate": 0.000993,
      "loss": 0.4753,
      "step": 8
    },
    {
      "epoch": 0.030612244897959183,
      "grad_norm": 0.4282812774181366,
      "learning_rate": 0.000992,
      "loss": 0.5331,
      "step": 9
    },
    {
      "epoch": 0.034013605442176874,
      "grad_norm": 0.4106682538986206,
      "learning_rate": 0.000991,
      "loss": 0.5663,
      "step": 10
    },
    {
      "epoch": 0.03741496598639456,
      "grad_norm": 0.43403470516204834,
      "learning_rate": 0.00099,
      "loss": 0.5059,
      "step": 11
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 0.5073560476303101,
      "learning_rate": 0.000989,
      "loss": 0.5695,
      "step": 12
    },
    {
      "epoch": 0.04421768707482993,
      "grad_norm": 0.48309022188186646,
      "learning_rate": 0.000988,
      "loss": 0.6101,
      "step": 13
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.705559253692627,
      "learning_rate": 0.000987,
      "loss": 0.7212,
      "step": 14
    },
    {
      "epoch": 0.05102040816326531,
      "grad_norm": 0.5031465888023376,
      "learning_rate": 0.0009860000000000001,
      "loss": 0.6781,
      "step": 15
    },
    {
      "epoch": 0.05442176870748299,
      "grad_norm": 0.46306467056274414,
      "learning_rate": 0.000985,
      "loss": 0.541,
      "step": 16
    },
    {
      "epoch": 0.05782312925170068,
      "grad_norm": 0.7624348998069763,
      "learning_rate": 0.000984,
      "loss": 0.7037,
      "step": 17
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 0.4178459942340851,
      "learning_rate": 0.000983,
      "loss": 0.5142,
      "step": 18
    },
    {
      "epoch": 0.06462585034013606,
      "grad_norm": 0.5023642182350159,
      "learning_rate": 0.000982,
      "loss": 0.6295,
      "step": 19
    },
    {
      "epoch": 0.06802721088435375,
      "grad_norm": 0.4436871111392975,
      "learning_rate": 0.000981,
      "loss": 0.4757,
      "step": 20
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 0.44202640652656555,
      "learning_rate": 0.00098,
      "loss": 0.5182,
      "step": 21
    },
    {
      "epoch": 0.07482993197278912,
      "grad_norm": 0.4970921576023102,
      "learning_rate": 0.000979,
      "loss": 0.6249,
      "step": 22
    },
    {
      "epoch": 0.0782312925170068,
      "grad_norm": 0.4258936643600464,
      "learning_rate": 0.000978,
      "loss": 0.5054,
      "step": 23
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 0.7775118947029114,
      "learning_rate": 0.000977,
      "loss": 0.7552,
      "step": 24
    },
    {
      "epoch": 0.08503401360544217,
      "grad_norm": 0.42248907685279846,
      "learning_rate": 0.000976,
      "loss": 0.495,
      "step": 25
    },
    {
      "epoch": 0.08843537414965986,
      "grad_norm": 1.013814091682434,
      "learning_rate": 0.000975,
      "loss": 0.5563,
      "step": 26
    },
    {
      "epoch": 0.09183673469387756,
      "grad_norm": 0.44731956720352173,
      "learning_rate": 0.000974,
      "loss": 0.5526,
      "step": 27
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.42465120553970337,
      "learning_rate": 0.000973,
      "loss": 0.4836,
      "step": 28
    },
    {
      "epoch": 0.09863945578231292,
      "grad_norm": 0.4383610188961029,
      "learning_rate": 0.000972,
      "loss": 0.5046,
      "step": 29
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 0.42858198285102844,
      "learning_rate": 0.000971,
      "loss": 0.5282,
      "step": 30
    },
    {
      "epoch": 0.1054421768707483,
      "grad_norm": 0.4669109284877777,
      "learning_rate": 0.0009699999999999999,
      "loss": 0.6726,
      "step": 31
    },
    {
      "epoch": 0.10884353741496598,
      "grad_norm": 0.4299252927303314,
      "learning_rate": 0.000969,
      "loss": 0.5655,
      "step": 32
    },
    {
      "epoch": 0.11224489795918367,
      "grad_norm": 0.37906938791275024,
      "learning_rate": 0.000968,
      "loss": 0.4999,
      "step": 33
    },
    {
      "epoch": 0.11564625850340136,
      "grad_norm": 0.5010660886764526,
      "learning_rate": 0.000967,
      "loss": 0.5964,
      "step": 34
    },
    {
      "epoch": 0.11904761904761904,
      "grad_norm": 0.4942256212234497,
      "learning_rate": 0.000966,
      "loss": 0.5956,
      "step": 35
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 0.6718779802322388,
      "learning_rate": 0.000965,
      "loss": 0.724,
      "step": 36
    },
    {
      "epoch": 0.12585034013605442,
      "grad_norm": 0.9084522724151611,
      "learning_rate": 0.000964,
      "loss": 0.642,
      "step": 37
    },
    {
      "epoch": 0.1292517006802721,
      "grad_norm": 0.8130795359611511,
      "learning_rate": 0.000963,
      "loss": 0.9104,
      "step": 38
    },
    {
      "epoch": 0.1326530612244898,
      "grad_norm": 0.44317999482154846,
      "learning_rate": 0.000962,
      "loss": 0.5366,
      "step": 39
    },
    {
      "epoch": 0.1360544217687075,
      "grad_norm": 0.4879254996776581,
      "learning_rate": 0.0009609999999999999,
      "loss": 0.4687,
      "step": 40
    },
    {
      "epoch": 0.13945578231292516,
      "grad_norm": 0.48123809695243835,
      "learning_rate": 0.00096,
      "loss": 0.5798,
      "step": 41
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.47326019406318665,
      "learning_rate": 0.000959,
      "loss": 0.6423,
      "step": 42
    },
    {
      "epoch": 0.14625850340136054,
      "grad_norm": 0.4517482817173004,
      "learning_rate": 0.000958,
      "loss": 0.5323,
      "step": 43
    },
    {
      "epoch": 0.14965986394557823,
      "grad_norm": 0.4259222745895386,
      "learning_rate": 0.000957,
      "loss": 0.6567,
      "step": 44
    },
    {
      "epoch": 0.15306122448979592,
      "grad_norm": 0.6442036628723145,
      "learning_rate": 0.0009559999999999999,
      "loss": 0.708,
      "step": 45
    },
    {
      "epoch": 0.1564625850340136,
      "grad_norm": 0.3883812427520752,
      "learning_rate": 0.000955,
      "loss": 0.5273,
      "step": 46
    },
    {
      "epoch": 0.1598639455782313,
      "grad_norm": 0.4168815016746521,
      "learning_rate": 0.000954,
      "loss": 0.512,
      "step": 47
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 0.4050484597682953,
      "learning_rate": 0.000953,
      "loss": 0.5411,
      "step": 48
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.5884613394737244,
      "learning_rate": 0.0009519999999999999,
      "loss": 0.7827,
      "step": 49
    },
    {
      "epoch": 0.17006802721088435,
      "grad_norm": 0.3722395896911621,
      "learning_rate": 0.000951,
      "loss": 0.4846,
      "step": 50
    },
    {
      "epoch": 0.17346938775510204,
      "grad_norm": 0.35864830017089844,
      "learning_rate": 0.00095,
      "loss": 0.4856,
      "step": 51
    },
    {
      "epoch": 0.17687074829931973,
      "grad_norm": 0.671035885810852,
      "learning_rate": 0.000949,
      "loss": 0.7155,
      "step": 52
    },
    {
      "epoch": 0.18027210884353742,
      "grad_norm": 0.40516361594200134,
      "learning_rate": 0.000948,
      "loss": 0.5755,
      "step": 53
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 0.7692261934280396,
      "learning_rate": 0.0009469999999999999,
      "loss": 0.8243,
      "step": 54
    },
    {
      "epoch": 0.1870748299319728,
      "grad_norm": 0.4506324827671051,
      "learning_rate": 0.000946,
      "loss": 0.5313,
      "step": 55
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.6643946766853333,
      "learning_rate": 0.000945,
      "loss": 0.5714,
      "step": 56
    },
    {
      "epoch": 0.19387755102040816,
      "grad_norm": 0.5340960025787354,
      "learning_rate": 0.000944,
      "loss": 0.6249,
      "step": 57
    },
    {
      "epoch": 0.19727891156462585,
      "grad_norm": 0.5717903971672058,
      "learning_rate": 0.0009429999999999999,
      "loss": 0.6668,
      "step": 58
    },
    {
      "epoch": 0.20068027210884354,
      "grad_norm": 0.41195476055145264,
      "learning_rate": 0.000942,
      "loss": 0.5318,
      "step": 59
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 0.47604575753211975,
      "learning_rate": 0.000941,
      "loss": 0.4725,
      "step": 60
    },
    {
      "epoch": 0.20748299319727892,
      "grad_norm": 0.44280746579170227,
      "learning_rate": 0.00094,
      "loss": 0.5448,
      "step": 61
    },
    {
      "epoch": 0.2108843537414966,
      "grad_norm": 0.4673989713191986,
      "learning_rate": 0.000939,
      "loss": 0.6873,
      "step": 62
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 0.42896386981010437,
      "learning_rate": 0.0009379999999999999,
      "loss": 0.5114,
      "step": 63
    },
    {
      "epoch": 0.21768707482993196,
      "grad_norm": 0.36840900778770447,
      "learning_rate": 0.0009370000000000001,
      "loss": 0.4623,
      "step": 64
    },
    {
      "epoch": 0.22108843537414966,
      "grad_norm": 0.420291006565094,
      "learning_rate": 0.0009360000000000001,
      "loss": 0.4949,
      "step": 65
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 0.4096580743789673,
      "learning_rate": 0.0009350000000000001,
      "loss": 0.542,
      "step": 66
    },
    {
      "epoch": 0.22789115646258504,
      "grad_norm": 0.3501364588737488,
      "learning_rate": 0.000934,
      "loss": 0.4266,
      "step": 67
    },
    {
      "epoch": 0.23129251700680273,
      "grad_norm": 0.4640824496746063,
      "learning_rate": 0.000933,
      "loss": 0.6141,
      "step": 68
    },
    {
      "epoch": 0.23469387755102042,
      "grad_norm": 0.49367985129356384,
      "learning_rate": 0.0009320000000000001,
      "loss": 0.5425,
      "step": 69
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.31807151436805725,
      "learning_rate": 0.0009310000000000001,
      "loss": 0.408,
      "step": 70
    },
    {
      "epoch": 0.24149659863945577,
      "grad_norm": 0.46671897172927856,
      "learning_rate": 0.00093,
      "loss": 0.6207,
      "step": 71
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 0.7889489531517029,
      "learning_rate": 0.000929,
      "loss": 0.8598,
      "step": 72
    },
    {
      "epoch": 0.24829931972789115,
      "grad_norm": 0.49509310722351074,
      "learning_rate": 0.0009280000000000001,
      "loss": 0.5327,
      "step": 73
    },
    {
      "epoch": 0.25170068027210885,
      "grad_norm": 0.49096232652664185,
      "learning_rate": 0.0009270000000000001,
      "loss": 0.6727,
      "step": 74
    },
    {
      "epoch": 0.25510204081632654,
      "grad_norm": 0.39854469895362854,
      "learning_rate": 0.0009260000000000001,
      "loss": 0.4692,
      "step": 75
    },
    {
      "epoch": 0.2585034013605442,
      "grad_norm": 0.5816152095794678,
      "learning_rate": 0.000925,
      "loss": 0.6625,
      "step": 76
    },
    {
      "epoch": 0.2619047619047619,
      "grad_norm": 0.500560998916626,
      "learning_rate": 0.000924,
      "loss": 0.662,
      "step": 77
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 0.8556554317474365,
      "learning_rate": 0.0009230000000000001,
      "loss": 0.9645,
      "step": 78
    },
    {
      "epoch": 0.2687074829931973,
      "grad_norm": 0.5385645031929016,
      "learning_rate": 0.0009220000000000001,
      "loss": 0.4666,
      "step": 79
    },
    {
      "epoch": 0.272108843537415,
      "grad_norm": 0.4537877142429352,
      "learning_rate": 0.000921,
      "loss": 0.5795,
      "step": 80
    },
    {
      "epoch": 0.2755102040816326,
      "grad_norm": 0.43597081303596497,
      "learning_rate": 0.00092,
      "loss": 0.5677,
      "step": 81
    },
    {
      "epoch": 0.2789115646258503,
      "grad_norm": 0.4968318045139313,
      "learning_rate": 0.0009190000000000001,
      "loss": 0.6255,
      "step": 82
    },
    {
      "epoch": 0.282312925170068,
      "grad_norm": 0.4848770201206207,
      "learning_rate": 0.0009180000000000001,
      "loss": 0.6722,
      "step": 83
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.4799615740776062,
      "learning_rate": 0.0009170000000000001,
      "loss": 0.6328,
      "step": 84
    },
    {
      "epoch": 0.2891156462585034,
      "grad_norm": 0.8479726314544678,
      "learning_rate": 0.000916,
      "loss": 0.8555,
      "step": 85
    },
    {
      "epoch": 0.2925170068027211,
      "grad_norm": 1.110615849494934,
      "learning_rate": 0.000915,
      "loss": 0.8041,
      "step": 86
    },
    {
      "epoch": 0.29591836734693877,
      "grad_norm": 2.709050178527832,
      "learning_rate": 0.0009140000000000001,
      "loss": 0.5665,
      "step": 87
    },
    {
      "epoch": 0.29931972789115646,
      "grad_norm": 0.45154210925102234,
      "learning_rate": 0.0009130000000000001,
      "loss": 0.5111,
      "step": 88
    },
    {
      "epoch": 0.30272108843537415,
      "grad_norm": 0.4911249577999115,
      "learning_rate": 0.000912,
      "loss": 0.7374,
      "step": 89
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 0.6405946016311646,
      "learning_rate": 0.000911,
      "loss": 0.5732,
      "step": 90
    },
    {
      "epoch": 0.30952380952380953,
      "grad_norm": 0.5978400707244873,
      "learning_rate": 0.00091,
      "loss": 0.6387,
      "step": 91
    },
    {
      "epoch": 0.3129251700680272,
      "grad_norm": 0.44405436515808105,
      "learning_rate": 0.0009090000000000001,
      "loss": 0.5414,
      "step": 92
    },
    {
      "epoch": 0.3163265306122449,
      "grad_norm": 0.4900505244731903,
      "learning_rate": 0.0009080000000000001,
      "loss": 0.5547,
      "step": 93
    },
    {
      "epoch": 0.3197278911564626,
      "grad_norm": 1.0735807418823242,
      "learning_rate": 0.000907,
      "loss": 0.7383,
      "step": 94
    },
    {
      "epoch": 0.3231292517006803,
      "grad_norm": 5.2036051750183105,
      "learning_rate": 0.000906,
      "loss": 0.9754,
      "step": 95
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 1.138692855834961,
      "learning_rate": 0.0009050000000000001,
      "loss": 0.695,
      "step": 96
    },
    {
      "epoch": 0.3299319727891156,
      "grad_norm": 0.773774266242981,
      "learning_rate": 0.0009040000000000001,
      "loss": 0.6742,
      "step": 97
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.9881916642189026,
      "learning_rate": 0.000903,
      "loss": 0.5489,
      "step": 98
    },
    {
      "epoch": 0.336734693877551,
      "grad_norm": 0.4458598792552948,
      "learning_rate": 0.000902,
      "loss": 0.6799,
      "step": 99
    },
    {
      "epoch": 0.3401360544217687,
      "grad_norm": 0.9103279113769531,
      "learning_rate": 0.000901,
      "loss": 0.5539,
      "step": 100
    },
    {
      "epoch": 0.3435374149659864,
      "grad_norm": 0.4327065348625183,
      "learning_rate": 0.0009000000000000001,
      "loss": 0.4982,
      "step": 101
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 0.38510918617248535,
      "learning_rate": 0.0008990000000000001,
      "loss": 0.4915,
      "step": 102
    },
    {
      "epoch": 0.35034013605442177,
      "grad_norm": 0.620218813419342,
      "learning_rate": 0.000898,
      "loss": 0.7116,
      "step": 103
    },
    {
      "epoch": 0.35374149659863946,
      "grad_norm": 0.5185489058494568,
      "learning_rate": 0.000897,
      "loss": 0.5491,
      "step": 104
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 0.5080968141555786,
      "learning_rate": 0.000896,
      "loss": 0.5919,
      "step": 105
    },
    {
      "epoch": 0.36054421768707484,
      "grad_norm": 0.3961605131626129,
      "learning_rate": 0.0008950000000000001,
      "loss": 0.6001,
      "step": 106
    },
    {
      "epoch": 0.36394557823129253,
      "grad_norm": 0.4091762900352478,
      "learning_rate": 0.000894,
      "loss": 0.4642,
      "step": 107
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 0.4066634774208069,
      "learning_rate": 0.000893,
      "loss": 0.4746,
      "step": 108
    },
    {
      "epoch": 0.3707482993197279,
      "grad_norm": 0.372152715921402,
      "learning_rate": 0.000892,
      "loss": 0.4597,
      "step": 109
    },
    {
      "epoch": 0.3741496598639456,
      "grad_norm": 0.6449524164199829,
      "learning_rate": 0.0008910000000000001,
      "loss": 0.609,
      "step": 110
    },
    {
      "epoch": 0.37755102040816324,
      "grad_norm": 0.4361805021762848,
      "learning_rate": 0.0008900000000000001,
      "loss": 0.5948,
      "step": 111
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.8918788433074951,
      "learning_rate": 0.000889,
      "loss": 0.8709,
      "step": 112
    },
    {
      "epoch": 0.3843537414965986,
      "grad_norm": 0.48547887802124023,
      "learning_rate": 0.000888,
      "loss": 0.6309,
      "step": 113
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 0.6728871464729309,
      "learning_rate": 0.000887,
      "loss": 0.8393,
      "step": 114
    },
    {
      "epoch": 0.391156462585034,
      "grad_norm": 0.4220476448535919,
      "learning_rate": 0.0008860000000000001,
      "loss": 0.4403,
      "step": 115
    },
    {
      "epoch": 0.3945578231292517,
      "grad_norm": 0.49221378564834595,
      "learning_rate": 0.000885,
      "loss": 0.6908,
      "step": 116
    },
    {
      "epoch": 0.3979591836734694,
      "grad_norm": 0.5244646668434143,
      "learning_rate": 0.000884,
      "loss": 0.5147,
      "step": 117
    },
    {
      "epoch": 0.4013605442176871,
      "grad_norm": 0.43523547053337097,
      "learning_rate": 0.000883,
      "loss": 0.5294,
      "step": 118
    },
    {
      "epoch": 0.40476190476190477,
      "grad_norm": 0.6706169247627258,
      "learning_rate": 0.000882,
      "loss": 0.5779,
      "step": 119
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 0.40306827425956726,
      "learning_rate": 0.0008810000000000001,
      "loss": 0.4984,
      "step": 120
    },
    {
      "epoch": 0.41156462585034015,
      "grad_norm": 0.4703420400619507,
      "learning_rate": 0.00088,
      "loss": 0.6248,
      "step": 121
    },
    {
      "epoch": 0.41496598639455784,
      "grad_norm": 0.5773721933364868,
      "learning_rate": 0.000879,
      "loss": 0.7725,
      "step": 122
    },
    {
      "epoch": 0.41836734693877553,
      "grad_norm": 0.6289457678794861,
      "learning_rate": 0.000878,
      "loss": 0.662,
      "step": 123
    },
    {
      "epoch": 0.4217687074829932,
      "grad_norm": 0.8298843502998352,
      "learning_rate": 0.0008770000000000001,
      "loss": 0.6539,
      "step": 124
    },
    {
      "epoch": 0.42517006802721086,
      "grad_norm": 0.42063164710998535,
      "learning_rate": 0.000876,
      "loss": 0.4719,
      "step": 125
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.7033669948577881,
      "learning_rate": 0.000875,
      "loss": 0.7885,
      "step": 126
    },
    {
      "epoch": 0.43197278911564624,
      "grad_norm": 0.40324094891548157,
      "learning_rate": 0.000874,
      "loss": 0.5367,
      "step": 127
    },
    {
      "epoch": 0.43537414965986393,
      "grad_norm": 0.4298097491264343,
      "learning_rate": 0.000873,
      "loss": 0.551,
      "step": 128
    },
    {
      "epoch": 0.4387755102040816,
      "grad_norm": 0.3886118233203888,
      "learning_rate": 0.000872,
      "loss": 0.5134,
      "step": 129
    },
    {
      "epoch": 0.4421768707482993,
      "grad_norm": 0.40859082341194153,
      "learning_rate": 0.000871,
      "loss": 0.572,
      "step": 130
    },
    {
      "epoch": 0.445578231292517,
      "grad_norm": 0.4575904309749603,
      "learning_rate": 0.00087,
      "loss": 0.4804,
      "step": 131
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 0.4352478086948395,
      "learning_rate": 0.000869,
      "loss": 0.5856,
      "step": 132
    },
    {
      "epoch": 0.4523809523809524,
      "grad_norm": 0.41063934564590454,
      "learning_rate": 0.0008680000000000001,
      "loss": 0.4226,
      "step": 133
    },
    {
      "epoch": 0.4557823129251701,
      "grad_norm": 0.3916734755039215,
      "learning_rate": 0.000867,
      "loss": 0.4995,
      "step": 134
    },
    {
      "epoch": 0.45918367346938777,
      "grad_norm": 0.3796500265598297,
      "learning_rate": 0.000866,
      "loss": 0.5334,
      "step": 135
    },
    {
      "epoch": 0.46258503401360546,
      "grad_norm": 0.4801912009716034,
      "learning_rate": 0.000865,
      "loss": 0.5988,
      "step": 136
    },
    {
      "epoch": 0.46598639455782315,
      "grad_norm": 0.3994099497795105,
      "learning_rate": 0.000864,
      "loss": 0.5461,
      "step": 137
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 0.8546873927116394,
      "learning_rate": 0.000863,
      "loss": 0.6949,
      "step": 138
    },
    {
      "epoch": 0.47278911564625853,
      "grad_norm": 0.8096653819084167,
      "learning_rate": 0.000862,
      "loss": 0.7387,
      "step": 139
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.362610399723053,
      "learning_rate": 0.000861,
      "loss": 0.4729,
      "step": 140
    },
    {
      "epoch": 0.47959183673469385,
      "grad_norm": 0.42951613664627075,
      "learning_rate": 0.00086,
      "loss": 0.5137,
      "step": 141
    },
    {
      "epoch": 0.48299319727891155,
      "grad_norm": 0.3973493278026581,
      "learning_rate": 0.000859,
      "loss": 0.446,
      "step": 142
    },
    {
      "epoch": 0.48639455782312924,
      "grad_norm": 0.7863590121269226,
      "learning_rate": 0.000858,
      "loss": 0.7221,
      "step": 143
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 0.6596571207046509,
      "learning_rate": 0.000857,
      "loss": 0.7031,
      "step": 144
    },
    {
      "epoch": 0.4931972789115646,
      "grad_norm": 0.4755595326423645,
      "learning_rate": 0.000856,
      "loss": 0.5957,
      "step": 145
    },
    {
      "epoch": 0.4965986394557823,
      "grad_norm": 0.7196676731109619,
      "learning_rate": 0.000855,
      "loss": 0.8489,
      "step": 146
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7916440367698669,
      "learning_rate": 0.000854,
      "loss": 0.5839,
      "step": 147
    },
    {
      "epoch": 0.5034013605442177,
      "grad_norm": 0.5213470458984375,
      "learning_rate": 0.000853,
      "loss": 0.7111,
      "step": 148
    },
    {
      "epoch": 0.5068027210884354,
      "grad_norm": 0.5599958896636963,
      "learning_rate": 0.000852,
      "loss": 0.6081,
      "step": 149
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 0.7466906309127808,
      "learning_rate": 0.000851,
      "loss": 0.73,
      "step": 150
    },
    {
      "epoch": 0.5136054421768708,
      "grad_norm": 0.8952766060829163,
      "learning_rate": 0.00085,
      "loss": 0.5591,
      "step": 151
    },
    {
      "epoch": 0.5170068027210885,
      "grad_norm": 0.5403317809104919,
      "learning_rate": 0.000849,
      "loss": 0.5053,
      "step": 152
    },
    {
      "epoch": 0.5204081632653061,
      "grad_norm": 0.48391151428222656,
      "learning_rate": 0.000848,
      "loss": 0.6483,
      "step": 153
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 0.5426804423332214,
      "learning_rate": 0.000847,
      "loss": 0.6133,
      "step": 154
    },
    {
      "epoch": 0.5272108843537415,
      "grad_norm": 0.5312603116035461,
      "learning_rate": 0.000846,
      "loss": 0.5437,
      "step": 155
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 0.5586584210395813,
      "learning_rate": 0.0008449999999999999,
      "loss": 0.6505,
      "step": 156
    },
    {
      "epoch": 0.5340136054421769,
      "grad_norm": 0.5029276013374329,
      "learning_rate": 0.000844,
      "loss": 0.5842,
      "step": 157
    },
    {
      "epoch": 0.5374149659863946,
      "grad_norm": 0.4707266390323639,
      "learning_rate": 0.000843,
      "loss": 0.5634,
      "step": 158
    },
    {
      "epoch": 0.5408163265306123,
      "grad_norm": 0.4570353031158447,
      "learning_rate": 0.000842,
      "loss": 0.6511,
      "step": 159
    },
    {
      "epoch": 0.54421768707483,
      "grad_norm": 0.4948376715183258,
      "learning_rate": 0.000841,
      "loss": 0.6198,
      "step": 160
    },
    {
      "epoch": 0.5476190476190477,
      "grad_norm": 0.4323652982711792,
      "learning_rate": 0.00084,
      "loss": 0.6074,
      "step": 161
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 0.4535468518733978,
      "learning_rate": 0.000839,
      "loss": 0.5824,
      "step": 162
    },
    {
      "epoch": 0.5544217687074829,
      "grad_norm": 0.5101315379142761,
      "learning_rate": 0.000838,
      "loss": 0.508,
      "step": 163
    },
    {
      "epoch": 0.5578231292517006,
      "grad_norm": 0.4521951377391815,
      "learning_rate": 0.000837,
      "loss": 0.5589,
      "step": 164
    },
    {
      "epoch": 0.5612244897959183,
      "grad_norm": 0.3999443054199219,
      "learning_rate": 0.0008359999999999999,
      "loss": 0.6064,
      "step": 165
    },
    {
      "epoch": 0.564625850340136,
      "grad_norm": 0.7777063250541687,
      "learning_rate": 0.000835,
      "loss": 0.7753,
      "step": 166
    },
    {
      "epoch": 0.5680272108843537,
      "grad_norm": 0.6959479451179504,
      "learning_rate": 0.000834,
      "loss": 0.8326,
      "step": 167
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.4508742094039917,
      "learning_rate": 0.000833,
      "loss": 0.5364,
      "step": 168
    },
    {
      "epoch": 0.5748299319727891,
      "grad_norm": 0.6335943341255188,
      "learning_rate": 0.000832,
      "loss": 0.5447,
      "step": 169
    },
    {
      "epoch": 0.5782312925170068,
      "grad_norm": 0.560329020023346,
      "learning_rate": 0.0008309999999999999,
      "loss": 0.4861,
      "step": 170
    },
    {
      "epoch": 0.5816326530612245,
      "grad_norm": 0.4687478244304657,
      "learning_rate": 0.00083,
      "loss": 0.5508,
      "step": 171
    },
    {
      "epoch": 0.5850340136054422,
      "grad_norm": 0.4403657913208008,
      "learning_rate": 0.000829,
      "loss": 0.5597,
      "step": 172
    },
    {
      "epoch": 0.5884353741496599,
      "grad_norm": 0.4398176372051239,
      "learning_rate": 0.000828,
      "loss": 0.5433,
      "step": 173
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 0.6232575178146362,
      "learning_rate": 0.0008269999999999999,
      "loss": 0.6538,
      "step": 174
    },
    {
      "epoch": 0.5952380952380952,
      "grad_norm": 0.4217747151851654,
      "learning_rate": 0.000826,
      "loss": 0.5884,
      "step": 175
    },
    {
      "epoch": 0.5986394557823129,
      "grad_norm": 0.5993406176567078,
      "learning_rate": 0.000825,
      "loss": 0.4654,
      "step": 176
    },
    {
      "epoch": 0.6020408163265306,
      "grad_norm": 0.5215851068496704,
      "learning_rate": 0.000824,
      "loss": 0.5073,
      "step": 177
    },
    {
      "epoch": 0.6054421768707483,
      "grad_norm": 0.47402846813201904,
      "learning_rate": 0.000823,
      "loss": 0.5223,
      "step": 178
    },
    {
      "epoch": 0.608843537414966,
      "grad_norm": 0.4390645921230316,
      "learning_rate": 0.0008219999999999999,
      "loss": 0.5203,
      "step": 179
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 0.48606935143470764,
      "learning_rate": 0.000821,
      "loss": 0.5916,
      "step": 180
    },
    {
      "epoch": 0.6156462585034014,
      "grad_norm": 0.3998280465602875,
      "learning_rate": 0.00082,
      "loss": 0.4721,
      "step": 181
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 0.45313242077827454,
      "learning_rate": 0.000819,
      "loss": 0.5034,
      "step": 182
    },
    {
      "epoch": 0.6224489795918368,
      "grad_norm": 0.8496217727661133,
      "learning_rate": 0.0008179999999999999,
      "loss": 0.8142,
      "step": 183
    },
    {
      "epoch": 0.6258503401360545,
      "grad_norm": 0.4444381296634674,
      "learning_rate": 0.000817,
      "loss": 0.6844,
      "step": 184
    },
    {
      "epoch": 0.6292517006802721,
      "grad_norm": 0.4521678686141968,
      "learning_rate": 0.000816,
      "loss": 0.6096,
      "step": 185
    },
    {
      "epoch": 0.6326530612244898,
      "grad_norm": 0.4417201578617096,
      "learning_rate": 0.000815,
      "loss": 0.7029,
      "step": 186
    },
    {
      "epoch": 0.6360544217687075,
      "grad_norm": 0.4757210612297058,
      "learning_rate": 0.0008139999999999999,
      "loss": 0.5134,
      "step": 187
    },
    {
      "epoch": 0.6394557823129252,
      "grad_norm": 0.4086219370365143,
      "learning_rate": 0.0008129999999999999,
      "loss": 0.4325,
      "step": 188
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 0.8403242230415344,
      "learning_rate": 0.0008120000000000001,
      "loss": 0.743,
      "step": 189
    },
    {
      "epoch": 0.6462585034013606,
      "grad_norm": 0.38338351249694824,
      "learning_rate": 0.0008110000000000001,
      "loss": 0.5334,
      "step": 190
    },
    {
      "epoch": 0.6496598639455783,
      "grad_norm": 0.4552784860134125,
      "learning_rate": 0.0008100000000000001,
      "loss": 0.6822,
      "step": 191
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 0.36349254846572876,
      "learning_rate": 0.000809,
      "loss": 0.4191,
      "step": 192
    },
    {
      "epoch": 0.6564625850340136,
      "grad_norm": 0.341215580701828,
      "learning_rate": 0.000808,
      "loss": 0.4718,
      "step": 193
    },
    {
      "epoch": 0.6598639455782312,
      "grad_norm": 0.5238330960273743,
      "learning_rate": 0.0008070000000000001,
      "loss": 0.6313,
      "step": 194
    },
    {
      "epoch": 0.6632653061224489,
      "grad_norm": 0.4237443804740906,
      "learning_rate": 0.0008060000000000001,
      "loss": 0.5253,
      "step": 195
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.47524088621139526,
      "learning_rate": 0.000805,
      "loss": 0.5053,
      "step": 196
    },
    {
      "epoch": 0.6700680272108843,
      "grad_norm": 0.3773857355117798,
      "learning_rate": 0.000804,
      "loss": 0.4603,
      "step": 197
    },
    {
      "epoch": 0.673469387755102,
      "grad_norm": 0.35802558064460754,
      "learning_rate": 0.0008030000000000001,
      "loss": 0.4565,
      "step": 198
    },
    {
      "epoch": 0.6768707482993197,
      "grad_norm": 0.46079176664352417,
      "learning_rate": 0.0008020000000000001,
      "loss": 0.6772,
      "step": 199
    },
    {
      "epoch": 0.6802721088435374,
      "grad_norm": 0.43258973956108093,
      "learning_rate": 0.0008010000000000001,
      "loss": 0.6383,
      "step": 200
    },
    {
      "epoch": 0.6836734693877551,
      "grad_norm": 0.7454959154129028,
      "learning_rate": 0.0008,
      "loss": 0.7447,
      "step": 201
    },
    {
      "epoch": 0.6870748299319728,
      "grad_norm": 0.4353199899196625,
      "learning_rate": 0.000799,
      "loss": 0.5965,
      "step": 202
    },
    {
      "epoch": 0.6904761904761905,
      "grad_norm": 0.46191248297691345,
      "learning_rate": 0.0007980000000000001,
      "loss": 0.524,
      "step": 203
    },
    {
      "epoch": 0.6938775510204082,
      "grad_norm": 0.39592698216438293,
      "learning_rate": 0.0007970000000000001,
      "loss": 0.4885,
      "step": 204
    },
    {
      "epoch": 0.6972789115646258,
      "grad_norm": 0.3633870482444763,
      "learning_rate": 0.000796,
      "loss": 0.4203,
      "step": 205
    },
    {
      "epoch": 0.7006802721088435,
      "grad_norm": 0.41419318318367004,
      "learning_rate": 0.000795,
      "loss": 0.577,
      "step": 206
    },
    {
      "epoch": 0.7040816326530612,
      "grad_norm": 0.40295445919036865,
      "learning_rate": 0.0007940000000000001,
      "loss": 0.4561,
      "step": 207
    },
    {
      "epoch": 0.7074829931972789,
      "grad_norm": 0.6872772574424744,
      "learning_rate": 0.0007930000000000001,
      "loss": 0.6933,
      "step": 208
    },
    {
      "epoch": 0.7108843537414966,
      "grad_norm": 0.7654209733009338,
      "learning_rate": 0.0007920000000000001,
      "loss": 0.8641,
      "step": 209
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.41748252511024475,
      "learning_rate": 0.000791,
      "loss": 0.4935,
      "step": 210
    },
    {
      "epoch": 0.717687074829932,
      "grad_norm": 0.45505937933921814,
      "learning_rate": 0.00079,
      "loss": 0.7024,
      "step": 211
    },
    {
      "epoch": 0.7210884353741497,
      "grad_norm": 0.5157936811447144,
      "learning_rate": 0.0007890000000000001,
      "loss": 0.6239,
      "step": 212
    },
    {
      "epoch": 0.7244897959183674,
      "grad_norm": 0.42971137166023254,
      "learning_rate": 0.0007880000000000001,
      "loss": 0.4968,
      "step": 213
    },
    {
      "epoch": 0.7278911564625851,
      "grad_norm": 0.3999349772930145,
      "learning_rate": 0.000787,
      "loss": 0.5321,
      "step": 214
    },
    {
      "epoch": 0.7312925170068028,
      "grad_norm": 0.6991533041000366,
      "learning_rate": 0.000786,
      "loss": 0.8594,
      "step": 215
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 0.46479085087776184,
      "learning_rate": 0.000785,
      "loss": 0.5617,
      "step": 216
    },
    {
      "epoch": 0.7380952380952381,
      "grad_norm": 0.5971791744232178,
      "learning_rate": 0.0007840000000000001,
      "loss": 0.846,
      "step": 217
    },
    {
      "epoch": 0.7414965986394558,
      "grad_norm": 0.42266321182250977,
      "learning_rate": 0.0007830000000000001,
      "loss": 0.5782,
      "step": 218
    },
    {
      "epoch": 0.7448979591836735,
      "grad_norm": 0.3929232954978943,
      "learning_rate": 0.000782,
      "loss": 0.4886,
      "step": 219
    },
    {
      "epoch": 0.7482993197278912,
      "grad_norm": 1.5034475326538086,
      "learning_rate": 0.000781,
      "loss": 0.5633,
      "step": 220
    },
    {
      "epoch": 0.7517006802721088,
      "grad_norm": 0.682817816734314,
      "learning_rate": 0.0007800000000000001,
      "loss": 0.8089,
      "step": 221
    },
    {
      "epoch": 0.7551020408163265,
      "grad_norm": 0.5081644654273987,
      "learning_rate": 0.0007790000000000001,
      "loss": 0.6039,
      "step": 222
    },
    {
      "epoch": 0.7585034013605442,
      "grad_norm": 0.44126033782958984,
      "learning_rate": 0.000778,
      "loss": 0.5219,
      "step": 223
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.5504689812660217,
      "learning_rate": 0.000777,
      "loss": 0.5832,
      "step": 224
    },
    {
      "epoch": 0.7653061224489796,
      "grad_norm": 0.5257557034492493,
      "learning_rate": 0.000776,
      "loss": 0.5846,
      "step": 225
    },
    {
      "epoch": 0.7687074829931972,
      "grad_norm": 0.3957529366016388,
      "learning_rate": 0.0007750000000000001,
      "loss": 0.4462,
      "step": 226
    },
    {
      "epoch": 0.7721088435374149,
      "grad_norm": 0.6865153312683105,
      "learning_rate": 0.0007740000000000001,
      "loss": 0.6522,
      "step": 227
    },
    {
      "epoch": 0.7755102040816326,
      "grad_norm": 0.610405683517456,
      "learning_rate": 0.000773,
      "loss": 0.7777,
      "step": 228
    },
    {
      "epoch": 0.7789115646258503,
      "grad_norm": 0.41833752393722534,
      "learning_rate": 0.000772,
      "loss": 0.5478,
      "step": 229
    },
    {
      "epoch": 0.782312925170068,
      "grad_norm": 0.4200676679611206,
      "learning_rate": 0.000771,
      "loss": 0.5516,
      "step": 230
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 0.41704386472702026,
      "learning_rate": 0.0007700000000000001,
      "loss": 0.5972,
      "step": 231
    },
    {
      "epoch": 0.7891156462585034,
      "grad_norm": 1.6573227643966675,
      "learning_rate": 0.000769,
      "loss": 0.5343,
      "step": 232
    },
    {
      "epoch": 0.7925170068027211,
      "grad_norm": 0.4647781252861023,
      "learning_rate": 0.000768,
      "loss": 0.6936,
      "step": 233
    },
    {
      "epoch": 0.7959183673469388,
      "grad_norm": 0.366345077753067,
      "learning_rate": 0.000767,
      "loss": 0.4488,
      "step": 234
    },
    {
      "epoch": 0.7993197278911565,
      "grad_norm": 0.36986783146858215,
      "learning_rate": 0.0007660000000000001,
      "loss": 0.5007,
      "step": 235
    },
    {
      "epoch": 0.8027210884353742,
      "grad_norm": 0.36957231163978577,
      "learning_rate": 0.0007650000000000001,
      "loss": 0.4614,
      "step": 236
    },
    {
      "epoch": 0.8061224489795918,
      "grad_norm": 0.557102620601654,
      "learning_rate": 0.000764,
      "loss": 0.6023,
      "step": 237
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 0.5772720575332642,
      "learning_rate": 0.000763,
      "loss": 0.6865,
      "step": 238
    },
    {
      "epoch": 0.8129251700680272,
      "grad_norm": 0.7752324342727661,
      "learning_rate": 0.000762,
      "loss": 0.8171,
      "step": 239
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 0.5332349538803101,
      "learning_rate": 0.0007610000000000001,
      "loss": 0.4888,
      "step": 240
    },
    {
      "epoch": 0.8197278911564626,
      "grad_norm": 0.5531999468803406,
      "learning_rate": 0.00076,
      "loss": 0.5957,
      "step": 241
    },
    {
      "epoch": 0.8231292517006803,
      "grad_norm": 0.45295214653015137,
      "learning_rate": 0.000759,
      "loss": 0.5659,
      "step": 242
    },
    {
      "epoch": 0.826530612244898,
      "grad_norm": 0.4537789821624756,
      "learning_rate": 0.000758,
      "loss": 0.5744,
      "step": 243
    },
    {
      "epoch": 0.8299319727891157,
      "grad_norm": 0.4327966272830963,
      "learning_rate": 0.000757,
      "loss": 0.541,
      "step": 244
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.40038564801216125,
      "learning_rate": 0.000756,
      "loss": 0.5558,
      "step": 245
    },
    {
      "epoch": 0.8367346938775511,
      "grad_norm": 0.45399463176727295,
      "learning_rate": 0.000755,
      "loss": 0.562,
      "step": 246
    },
    {
      "epoch": 0.8401360544217688,
      "grad_norm": 0.42038923501968384,
      "learning_rate": 0.000754,
      "loss": 0.5093,
      "step": 247
    },
    {
      "epoch": 0.8435374149659864,
      "grad_norm": 0.40306568145751953,
      "learning_rate": 0.000753,
      "loss": 0.5367,
      "step": 248
    },
    {
      "epoch": 0.8469387755102041,
      "grad_norm": 0.3830789029598236,
      "learning_rate": 0.0007520000000000001,
      "loss": 0.5147,
      "step": 249
    },
    {
      "epoch": 0.8503401360544217,
      "grad_norm": 0.4091915190219879,
      "learning_rate": 0.000751,
      "loss": 0.515,
      "step": 250
    },
    {
      "epoch": 0.8537414965986394,
      "grad_norm": 0.36734217405319214,
      "learning_rate": 0.00075,
      "loss": 0.4631,
      "step": 251
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.3765786290168762,
      "learning_rate": 0.000749,
      "loss": 0.5575,
      "step": 252
    },
    {
      "epoch": 0.8605442176870748,
      "grad_norm": 0.4062848389148712,
      "learning_rate": 0.000748,
      "loss": 0.4503,
      "step": 253
    },
    {
      "epoch": 0.8639455782312925,
      "grad_norm": 0.46013638377189636,
      "learning_rate": 0.000747,
      "loss": 0.6329,
      "step": 254
    },
    {
      "epoch": 0.8673469387755102,
      "grad_norm": 0.4990274906158447,
      "learning_rate": 0.000746,
      "loss": 0.6026,
      "step": 255
    },
    {
      "epoch": 0.8707482993197279,
      "grad_norm": 0.4674326479434967,
      "learning_rate": 0.000745,
      "loss": 0.4812,
      "step": 256
    },
    {
      "epoch": 0.8741496598639455,
      "grad_norm": 0.4340956211090088,
      "learning_rate": 0.000744,
      "loss": 0.5023,
      "step": 257
    },
    {
      "epoch": 0.8775510204081632,
      "grad_norm": 0.44265565276145935,
      "learning_rate": 0.0007430000000000001,
      "loss": 0.5597,
      "step": 258
    },
    {
      "epoch": 0.8809523809523809,
      "grad_norm": 0.44809800386428833,
      "learning_rate": 0.000742,
      "loss": 0.4611,
      "step": 259
    },
    {
      "epoch": 0.8843537414965986,
      "grad_norm": 0.45430055260658264,
      "learning_rate": 0.000741,
      "loss": 0.5026,
      "step": 260
    },
    {
      "epoch": 0.8877551020408163,
      "grad_norm": 0.41408681869506836,
      "learning_rate": 0.00074,
      "loss": 0.5162,
      "step": 261
    },
    {
      "epoch": 0.891156462585034,
      "grad_norm": 0.5267934799194336,
      "learning_rate": 0.000739,
      "loss": 0.7291,
      "step": 262
    },
    {
      "epoch": 0.8945578231292517,
      "grad_norm": 1.4483948945999146,
      "learning_rate": 0.000738,
      "loss": 0.6138,
      "step": 263
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 0.6636711955070496,
      "learning_rate": 0.000737,
      "loss": 0.9078,
      "step": 264
    },
    {
      "epoch": 0.9013605442176871,
      "grad_norm": 0.8257571458816528,
      "learning_rate": 0.000736,
      "loss": 0.7708,
      "step": 265
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 0.5723528265953064,
      "learning_rate": 0.000735,
      "loss": 0.5001,
      "step": 266
    },
    {
      "epoch": 0.9081632653061225,
      "grad_norm": 0.5111401081085205,
      "learning_rate": 0.000734,
      "loss": 0.5116,
      "step": 267
    },
    {
      "epoch": 0.9115646258503401,
      "grad_norm": 0.47601211071014404,
      "learning_rate": 0.000733,
      "loss": 0.6358,
      "step": 268
    },
    {
      "epoch": 0.9149659863945578,
      "grad_norm": 0.4471440315246582,
      "learning_rate": 0.000732,
      "loss": 0.5893,
      "step": 269
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 0.3585986793041229,
      "learning_rate": 0.000731,
      "loss": 0.4535,
      "step": 270
    },
    {
      "epoch": 0.9217687074829932,
      "grad_norm": 0.7344409227371216,
      "learning_rate": 0.00073,
      "loss": 0.7063,
      "step": 271
    },
    {
      "epoch": 0.9251700680272109,
      "grad_norm": 0.3505713641643524,
      "learning_rate": 0.000729,
      "loss": 0.4559,
      "step": 272
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 0.41561752557754517,
      "learning_rate": 0.000728,
      "loss": 0.5809,
      "step": 273
    },
    {
      "epoch": 0.9319727891156463,
      "grad_norm": 0.39792853593826294,
      "learning_rate": 0.000727,
      "loss": 0.4849,
      "step": 274
    },
    {
      "epoch": 0.935374149659864,
      "grad_norm": 0.3718342185020447,
      "learning_rate": 0.000726,
      "loss": 0.5418,
      "step": 275
    },
    {
      "epoch": 0.9387755102040817,
      "grad_norm": 0.4352514147758484,
      "learning_rate": 0.000725,
      "loss": 0.715,
      "step": 276
    },
    {
      "epoch": 0.9421768707482994,
      "grad_norm": 1.0737218856811523,
      "learning_rate": 0.000724,
      "loss": 0.5637,
      "step": 277
    },
    {
      "epoch": 0.9455782312925171,
      "grad_norm": 0.3826991319656372,
      "learning_rate": 0.000723,
      "loss": 0.4447,
      "step": 278
    },
    {
      "epoch": 0.9489795918367347,
      "grad_norm": 0.4207819104194641,
      "learning_rate": 0.000722,
      "loss": 0.5815,
      "step": 279
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.8742081522941589,
      "learning_rate": 0.000721,
      "loss": 0.8412,
      "step": 280
    },
    {
      "epoch": 0.95578231292517,
      "grad_norm": 0.5892325639724731,
      "learning_rate": 0.0007199999999999999,
      "loss": 0.8083,
      "step": 281
    },
    {
      "epoch": 0.9591836734693877,
      "grad_norm": 0.45686209201812744,
      "learning_rate": 0.000719,
      "loss": 0.6895,
      "step": 282
    },
    {
      "epoch": 0.9625850340136054,
      "grad_norm": 0.6158970594406128,
      "learning_rate": 0.000718,
      "loss": 0.8027,
      "step": 283
    },
    {
      "epoch": 0.9659863945578231,
      "grad_norm": 0.4211978018283844,
      "learning_rate": 0.000717,
      "loss": 0.4966,
      "step": 284
    },
    {
      "epoch": 0.9693877551020408,
      "grad_norm": 0.4305450916290283,
      "learning_rate": 0.000716,
      "loss": 0.4991,
      "step": 285
    },
    {
      "epoch": 0.9727891156462585,
      "grad_norm": 0.41142258048057556,
      "learning_rate": 0.000715,
      "loss": 0.4919,
      "step": 286
    },
    {
      "epoch": 0.9761904761904762,
      "grad_norm": 0.38377895951271057,
      "learning_rate": 0.000714,
      "loss": 0.5137,
      "step": 287
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 0.4385194182395935,
      "learning_rate": 0.000713,
      "loss": 0.6075,
      "step": 288
    },
    {
      "epoch": 0.9829931972789115,
      "grad_norm": 0.7066640853881836,
      "learning_rate": 0.000712,
      "loss": 0.8451,
      "step": 289
    },
    {
      "epoch": 0.9863945578231292,
      "grad_norm": 0.3779961168766022,
      "learning_rate": 0.0007109999999999999,
      "loss": 0.4365,
      "step": 290
    },
    {
      "epoch": 0.9897959183673469,
      "grad_norm": 0.3855566382408142,
      "learning_rate": 0.00071,
      "loss": 0.5148,
      "step": 291
    },
    {
      "epoch": 0.9931972789115646,
      "grad_norm": 0.8779159188270569,
      "learning_rate": 0.000709,
      "loss": 0.592,
      "step": 292
    },
    {
      "epoch": 0.9965986394557823,
      "grad_norm": 0.7639044523239136,
      "learning_rate": 0.000708,
      "loss": 0.6993,
      "step": 293
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.6970953941345215,
      "learning_rate": 0.000707,
      "loss": 0.6918,
      "step": 294
    },
    {
      "epoch": 1.0034013605442176,
      "grad_norm": 0.39415499567985535,
      "learning_rate": 0.0007059999999999999,
      "loss": 0.4665,
      "step": 295
    },
    {
      "epoch": 1.0068027210884354,
      "grad_norm": 0.4623047411441803,
      "learning_rate": 0.000705,
      "loss": 0.5321,
      "step": 296
    },
    {
      "epoch": 1.010204081632653,
      "grad_norm": 0.3812301456928253,
      "learning_rate": 0.000704,
      "loss": 0.5207,
      "step": 297
    },
    {
      "epoch": 1.0136054421768708,
      "grad_norm": 0.4460451900959015,
      "learning_rate": 0.000703,
      "loss": 0.5711,
      "step": 298
    },
    {
      "epoch": 1.0170068027210883,
      "grad_norm": 0.45870140194892883,
      "learning_rate": 0.0007019999999999999,
      "loss": 0.649,
      "step": 299
    },
    {
      "epoch": 1.0204081632653061,
      "grad_norm": 0.4227885603904724,
      "learning_rate": 0.000701,
      "loss": 0.5032,
      "step": 300
    },
    {
      "epoch": 1.0238095238095237,
      "grad_norm": 0.4740886986255646,
      "learning_rate": 0.0007,
      "loss": 0.6807,
      "step": 301
    },
    {
      "epoch": 1.0272108843537415,
      "grad_norm": 0.4243636429309845,
      "learning_rate": 0.000699,
      "loss": 0.4707,
      "step": 302
    },
    {
      "epoch": 1.030612244897959,
      "grad_norm": 0.4391559064388275,
      "learning_rate": 0.0006979999999999999,
      "loss": 0.5656,
      "step": 303
    },
    {
      "epoch": 1.034013605442177,
      "grad_norm": 0.5225119590759277,
      "learning_rate": 0.0006969999999999999,
      "loss": 0.746,
      "step": 304
    },
    {
      "epoch": 1.0374149659863945,
      "grad_norm": 0.40480756759643555,
      "learning_rate": 0.000696,
      "loss": 0.5762,
      "step": 305
    },
    {
      "epoch": 1.0408163265306123,
      "grad_norm": 0.6292508840560913,
      "learning_rate": 0.000695,
      "loss": 0.6699,
      "step": 306
    },
    {
      "epoch": 1.0442176870748299,
      "grad_norm": 0.38368239998817444,
      "learning_rate": 0.000694,
      "loss": 0.5554,
      "step": 307
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.3814524710178375,
      "learning_rate": 0.0006929999999999999,
      "loss": 0.4818,
      "step": 308
    },
    {
      "epoch": 1.0510204081632653,
      "grad_norm": 0.38383424282073975,
      "learning_rate": 0.000692,
      "loss": 0.434,
      "step": 309
    },
    {
      "epoch": 1.054421768707483,
      "grad_norm": 0.6326311826705933,
      "learning_rate": 0.000691,
      "loss": 0.7988,
      "step": 310
    },
    {
      "epoch": 1.0578231292517006,
      "grad_norm": 0.4246196746826172,
      "learning_rate": 0.00069,
      "loss": 0.5344,
      "step": 311
    },
    {
      "epoch": 1.0612244897959184,
      "grad_norm": 0.6159356832504272,
      "learning_rate": 0.0006889999999999999,
      "loss": 0.7828,
      "step": 312
    },
    {
      "epoch": 1.064625850340136,
      "grad_norm": 0.4075046181678772,
      "learning_rate": 0.0006879999999999999,
      "loss": 0.499,
      "step": 313
    },
    {
      "epoch": 1.0680272108843538,
      "grad_norm": 0.411137193441391,
      "learning_rate": 0.0006870000000000001,
      "loss": 0.5123,
      "step": 314
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 0.42309868335723877,
      "learning_rate": 0.0006860000000000001,
      "loss": 0.5765,
      "step": 315
    },
    {
      "epoch": 1.0748299319727892,
      "grad_norm": 0.3838217854499817,
      "learning_rate": 0.0006850000000000001,
      "loss": 0.4837,
      "step": 316
    },
    {
      "epoch": 1.0782312925170068,
      "grad_norm": 0.3904767334461212,
      "learning_rate": 0.000684,
      "loss": 0.5079,
      "step": 317
    },
    {
      "epoch": 1.0816326530612246,
      "grad_norm": 0.6497023105621338,
      "learning_rate": 0.000683,
      "loss": 0.6579,
      "step": 318
    },
    {
      "epoch": 1.0850340136054422,
      "grad_norm": 0.39951130747795105,
      "learning_rate": 0.0006820000000000001,
      "loss": 0.5292,
      "step": 319
    },
    {
      "epoch": 1.08843537414966,
      "grad_norm": 0.396593302488327,
      "learning_rate": 0.0006810000000000001,
      "loss": 0.5064,
      "step": 320
    },
    {
      "epoch": 1.0918367346938775,
      "grad_norm": 0.42003583908081055,
      "learning_rate": 0.00068,
      "loss": 0.5744,
      "step": 321
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.4639812707901001,
      "learning_rate": 0.000679,
      "loss": 0.594,
      "step": 322
    },
    {
      "epoch": 1.098639455782313,
      "grad_norm": 0.42189326882362366,
      "learning_rate": 0.0006780000000000001,
      "loss": 0.5368,
      "step": 323
    },
    {
      "epoch": 1.1020408163265305,
      "grad_norm": 0.4145585596561432,
      "learning_rate": 0.0006770000000000001,
      "loss": 0.4448,
      "step": 324
    },
    {
      "epoch": 1.1054421768707483,
      "grad_norm": 1.4136836528778076,
      "learning_rate": 0.0006760000000000001,
      "loss": 0.5604,
      "step": 325
    },
    {
      "epoch": 1.1088435374149659,
      "grad_norm": 0.3974197208881378,
      "learning_rate": 0.000675,
      "loss": 0.5342,
      "step": 326
    },
    {
      "epoch": 1.1122448979591837,
      "grad_norm": 0.3810410797595978,
      "learning_rate": 0.000674,
      "loss": 0.4234,
      "step": 327
    },
    {
      "epoch": 1.1156462585034013,
      "grad_norm": 0.3943595886230469,
      "learning_rate": 0.0006730000000000001,
      "loss": 0.5082,
      "step": 328
    },
    {
      "epoch": 1.119047619047619,
      "grad_norm": 0.6499569416046143,
      "learning_rate": 0.0006720000000000001,
      "loss": 0.6608,
      "step": 329
    },
    {
      "epoch": 1.1224489795918366,
      "grad_norm": 0.3858071267604828,
      "learning_rate": 0.000671,
      "loss": 0.5512,
      "step": 330
    },
    {
      "epoch": 1.1258503401360545,
      "grad_norm": 0.4428236484527588,
      "learning_rate": 0.00067,
      "loss": 0.4808,
      "step": 331
    },
    {
      "epoch": 1.129251700680272,
      "grad_norm": 0.6348071694374084,
      "learning_rate": 0.0006690000000000001,
      "loss": 0.7437,
      "step": 332
    },
    {
      "epoch": 1.1326530612244898,
      "grad_norm": 0.6815812587738037,
      "learning_rate": 0.0006680000000000001,
      "loss": 0.8121,
      "step": 333
    },
    {
      "epoch": 1.1360544217687074,
      "grad_norm": 0.5159076452255249,
      "learning_rate": 0.0006670000000000001,
      "loss": 0.6534,
      "step": 334
    },
    {
      "epoch": 1.1394557823129252,
      "grad_norm": 0.42083004117012024,
      "learning_rate": 0.000666,
      "loss": 0.528,
      "step": 335
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.38229480385780334,
      "learning_rate": 0.000665,
      "loss": 0.5085,
      "step": 336
    },
    {
      "epoch": 1.1462585034013606,
      "grad_norm": 0.3769139051437378,
      "learning_rate": 0.0006640000000000001,
      "loss": 0.4626,
      "step": 337
    },
    {
      "epoch": 1.1496598639455782,
      "grad_norm": 0.45484742522239685,
      "learning_rate": 0.0006630000000000001,
      "loss": 0.6272,
      "step": 338
    },
    {
      "epoch": 1.153061224489796,
      "grad_norm": 0.7205960154533386,
      "learning_rate": 0.000662,
      "loss": 0.7908,
      "step": 339
    },
    {
      "epoch": 1.1564625850340136,
      "grad_norm": 0.44417285919189453,
      "learning_rate": 0.000661,
      "loss": 0.6179,
      "step": 340
    },
    {
      "epoch": 1.1598639455782314,
      "grad_norm": 0.7139511704444885,
      "learning_rate": 0.00066,
      "loss": 0.6087,
      "step": 341
    },
    {
      "epoch": 1.163265306122449,
      "grad_norm": 0.6828626990318298,
      "learning_rate": 0.0006590000000000001,
      "loss": 0.6416,
      "step": 342
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.6949402689933777,
      "learning_rate": 0.0006580000000000001,
      "loss": 0.7833,
      "step": 343
    },
    {
      "epoch": 1.1700680272108843,
      "grad_norm": 0.4188932478427887,
      "learning_rate": 0.000657,
      "loss": 0.5035,
      "step": 344
    },
    {
      "epoch": 1.1734693877551021,
      "grad_norm": 0.45252326130867004,
      "learning_rate": 0.000656,
      "loss": 0.6926,
      "step": 345
    },
    {
      "epoch": 1.1768707482993197,
      "grad_norm": 0.4010677933692932,
      "learning_rate": 0.0006550000000000001,
      "loss": 0.501,
      "step": 346
    },
    {
      "epoch": 1.1802721088435375,
      "grad_norm": 0.4400947093963623,
      "learning_rate": 0.0006540000000000001,
      "loss": 0.4985,
      "step": 347
    },
    {
      "epoch": 1.183673469387755,
      "grad_norm": 0.43119192123413086,
      "learning_rate": 0.000653,
      "loss": 0.6075,
      "step": 348
    },
    {
      "epoch": 1.1870748299319729,
      "grad_norm": 0.38243505358695984,
      "learning_rate": 0.000652,
      "loss": 0.5477,
      "step": 349
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.43849077820777893,
      "learning_rate": 0.000651,
      "loss": 0.5042,
      "step": 350
    },
    {
      "epoch": 1.193877551020408,
      "grad_norm": 0.43243658542633057,
      "learning_rate": 0.0006500000000000001,
      "loss": 0.5151,
      "step": 351
    },
    {
      "epoch": 1.1972789115646258,
      "grad_norm": 0.3912246823310852,
      "learning_rate": 0.0006490000000000001,
      "loss": 0.5054,
      "step": 352
    },
    {
      "epoch": 1.2006802721088436,
      "grad_norm": 0.510084867477417,
      "learning_rate": 0.000648,
      "loss": 0.6384,
      "step": 353
    },
    {
      "epoch": 1.2040816326530612,
      "grad_norm": 0.46981438994407654,
      "learning_rate": 0.000647,
      "loss": 0.539,
      "step": 354
    },
    {
      "epoch": 1.2074829931972788,
      "grad_norm": 0.39971786737442017,
      "learning_rate": 0.000646,
      "loss": 0.5345,
      "step": 355
    },
    {
      "epoch": 1.2108843537414966,
      "grad_norm": 0.39670005440711975,
      "learning_rate": 0.0006450000000000001,
      "loss": 0.5557,
      "step": 356
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 0.3672257661819458,
      "learning_rate": 0.000644,
      "loss": 0.4876,
      "step": 357
    },
    {
      "epoch": 1.217687074829932,
      "grad_norm": 0.39680659770965576,
      "learning_rate": 0.000643,
      "loss": 0.5363,
      "step": 358
    },
    {
      "epoch": 1.2210884353741496,
      "grad_norm": 0.4769033193588257,
      "learning_rate": 0.000642,
      "loss": 0.4903,
      "step": 359
    },
    {
      "epoch": 1.2244897959183674,
      "grad_norm": 0.4097796678543091,
      "learning_rate": 0.0006410000000000001,
      "loss": 0.5294,
      "step": 360
    },
    {
      "epoch": 1.227891156462585,
      "grad_norm": 0.410548597574234,
      "learning_rate": 0.00064,
      "loss": 0.514,
      "step": 361
    },
    {
      "epoch": 1.2312925170068028,
      "grad_norm": 0.38312405347824097,
      "learning_rate": 0.000639,
      "loss": 0.4653,
      "step": 362
    },
    {
      "epoch": 1.2346938775510203,
      "grad_norm": 0.353451669216156,
      "learning_rate": 0.000638,
      "loss": 0.4825,
      "step": 363
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 0.49675828218460083,
      "learning_rate": 0.000637,
      "loss": 0.5028,
      "step": 364
    },
    {
      "epoch": 1.2414965986394557,
      "grad_norm": 0.3763902187347412,
      "learning_rate": 0.0006360000000000001,
      "loss": 0.474,
      "step": 365
    },
    {
      "epoch": 1.2448979591836735,
      "grad_norm": 0.44630467891693115,
      "learning_rate": 0.000635,
      "loss": 0.6732,
      "step": 366
    },
    {
      "epoch": 1.248299319727891,
      "grad_norm": 0.3696989119052887,
      "learning_rate": 0.000634,
      "loss": 0.4906,
      "step": 367
    },
    {
      "epoch": 1.251700680272109,
      "grad_norm": 0.41737693548202515,
      "learning_rate": 0.000633,
      "loss": 0.5972,
      "step": 368
    },
    {
      "epoch": 1.2551020408163265,
      "grad_norm": 0.39241814613342285,
      "learning_rate": 0.000632,
      "loss": 0.5055,
      "step": 369
    },
    {
      "epoch": 1.2585034013605443,
      "grad_norm": 0.6248729228973389,
      "learning_rate": 0.000631,
      "loss": 0.7332,
      "step": 370
    },
    {
      "epoch": 1.2619047619047619,
      "grad_norm": 0.33347606658935547,
      "learning_rate": 0.00063,
      "loss": 0.4455,
      "step": 371
    },
    {
      "epoch": 1.2653061224489797,
      "grad_norm": 0.4503405690193176,
      "learning_rate": 0.000629,
      "loss": 0.5955,
      "step": 372
    },
    {
      "epoch": 1.2687074829931972,
      "grad_norm": 0.39587268233299255,
      "learning_rate": 0.000628,
      "loss": 0.5597,
      "step": 373
    },
    {
      "epoch": 1.272108843537415,
      "grad_norm": 0.41313162446022034,
      "learning_rate": 0.0006270000000000001,
      "loss": 0.586,
      "step": 374
    },
    {
      "epoch": 1.2755102040816326,
      "grad_norm": 0.4257844090461731,
      "learning_rate": 0.000626,
      "loss": 0.4338,
      "step": 375
    },
    {
      "epoch": 1.2789115646258504,
      "grad_norm": 0.3978210687637329,
      "learning_rate": 0.000625,
      "loss": 0.4611,
      "step": 376
    },
    {
      "epoch": 1.282312925170068,
      "grad_norm": 0.6994594931602478,
      "learning_rate": 0.000624,
      "loss": 0.7587,
      "step": 377
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.4304191470146179,
      "learning_rate": 0.000623,
      "loss": 0.6289,
      "step": 378
    },
    {
      "epoch": 1.2891156462585034,
      "grad_norm": 2.6271872520446777,
      "learning_rate": 0.000622,
      "loss": 0.6623,
      "step": 379
    },
    {
      "epoch": 1.2925170068027212,
      "grad_norm": 0.3901435434818268,
      "learning_rate": 0.000621,
      "loss": 0.6287,
      "step": 380
    },
    {
      "epoch": 1.2959183673469388,
      "grad_norm": 0.5672126412391663,
      "learning_rate": 0.00062,
      "loss": 0.758,
      "step": 381
    },
    {
      "epoch": 1.2993197278911564,
      "grad_norm": 0.36017489433288574,
      "learning_rate": 0.000619,
      "loss": 0.4362,
      "step": 382
    },
    {
      "epoch": 1.3027210884353742,
      "grad_norm": 0.7671438455581665,
      "learning_rate": 0.0006180000000000001,
      "loss": 0.9311,
      "step": 383
    },
    {
      "epoch": 1.306122448979592,
      "grad_norm": 0.4665575325489044,
      "learning_rate": 0.000617,
      "loss": 0.6395,
      "step": 384
    },
    {
      "epoch": 1.3095238095238095,
      "grad_norm": 0.3971516191959381,
      "learning_rate": 0.000616,
      "loss": 0.5489,
      "step": 385
    },
    {
      "epoch": 1.3129251700680271,
      "grad_norm": 0.4349454939365387,
      "learning_rate": 0.000615,
      "loss": 0.6266,
      "step": 386
    },
    {
      "epoch": 1.316326530612245,
      "grad_norm": 0.8766851425170898,
      "learning_rate": 0.000614,
      "loss": 0.6696,
      "step": 387
    },
    {
      "epoch": 1.3197278911564627,
      "grad_norm": 0.38813886046409607,
      "learning_rate": 0.000613,
      "loss": 0.4243,
      "step": 388
    },
    {
      "epoch": 1.3231292517006803,
      "grad_norm": 0.3988165557384491,
      "learning_rate": 0.000612,
      "loss": 0.5045,
      "step": 389
    },
    {
      "epoch": 1.3265306122448979,
      "grad_norm": 0.47079935669898987,
      "learning_rate": 0.000611,
      "loss": 0.6224,
      "step": 390
    },
    {
      "epoch": 1.3299319727891157,
      "grad_norm": 0.6837965250015259,
      "learning_rate": 0.00061,
      "loss": 0.8325,
      "step": 391
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.3575672209262848,
      "learning_rate": 0.000609,
      "loss": 0.4342,
      "step": 392
    },
    {
      "epoch": 1.336734693877551,
      "grad_norm": 0.8855772614479065,
      "learning_rate": 0.000608,
      "loss": 0.6354,
      "step": 393
    },
    {
      "epoch": 1.3401360544217686,
      "grad_norm": 0.7855793237686157,
      "learning_rate": 0.000607,
      "loss": 0.6032,
      "step": 394
    },
    {
      "epoch": 1.3435374149659864,
      "grad_norm": 0.6591776013374329,
      "learning_rate": 0.000606,
      "loss": 0.6723,
      "step": 395
    },
    {
      "epoch": 1.346938775510204,
      "grad_norm": 0.38477417826652527,
      "learning_rate": 0.000605,
      "loss": 0.5132,
      "step": 396
    },
    {
      "epoch": 1.3503401360544218,
      "grad_norm": 0.3768228590488434,
      "learning_rate": 0.000604,
      "loss": 0.4759,
      "step": 397
    },
    {
      "epoch": 1.3537414965986394,
      "grad_norm": 0.4540899395942688,
      "learning_rate": 0.000603,
      "loss": 0.6654,
      "step": 398
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 0.7233046293258667,
      "learning_rate": 0.000602,
      "loss": 0.6528,
      "step": 399
    },
    {
      "epoch": 1.3605442176870748,
      "grad_norm": 0.3996579647064209,
      "learning_rate": 0.000601,
      "loss": 0.6076,
      "step": 400
    },
    {
      "epoch": 1.3639455782312926,
      "grad_norm": 0.44890666007995605,
      "learning_rate": 0.0006,
      "loss": 0.6802,
      "step": 401
    },
    {
      "epoch": 1.3673469387755102,
      "grad_norm": 0.46492111682891846,
      "learning_rate": 0.000599,
      "loss": 0.6795,
      "step": 402
    },
    {
      "epoch": 1.370748299319728,
      "grad_norm": 0.44606173038482666,
      "learning_rate": 0.000598,
      "loss": 0.534,
      "step": 403
    },
    {
      "epoch": 1.3741496598639455,
      "grad_norm": 0.45179247856140137,
      "learning_rate": 0.000597,
      "loss": 0.5904,
      "step": 404
    },
    {
      "epoch": 1.3775510204081631,
      "grad_norm": 0.4471849203109741,
      "learning_rate": 0.000596,
      "loss": 0.4914,
      "step": 405
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.6087457537651062,
      "learning_rate": 0.0005949999999999999,
      "loss": 0.7508,
      "step": 406
    },
    {
      "epoch": 1.3843537414965987,
      "grad_norm": 0.43095460534095764,
      "learning_rate": 0.000594,
      "loss": 0.6786,
      "step": 407
    },
    {
      "epoch": 1.3877551020408163,
      "grad_norm": 0.6178235411643982,
      "learning_rate": 0.000593,
      "loss": 0.7914,
      "step": 408
    },
    {
      "epoch": 1.391156462585034,
      "grad_norm": 0.421979695558548,
      "learning_rate": 0.000592,
      "loss": 0.5692,
      "step": 409
    },
    {
      "epoch": 1.3945578231292517,
      "grad_norm": 0.35110875964164734,
      "learning_rate": 0.0005909999999999999,
      "loss": 0.4642,
      "step": 410
    },
    {
      "epoch": 1.3979591836734695,
      "grad_norm": 0.35671281814575195,
      "learning_rate": 0.00059,
      "loss": 0.506,
      "step": 411
    },
    {
      "epoch": 1.401360544217687,
      "grad_norm": 0.3883647322654724,
      "learning_rate": 0.000589,
      "loss": 0.4741,
      "step": 412
    },
    {
      "epoch": 1.4047619047619047,
      "grad_norm": 0.4396906793117523,
      "learning_rate": 0.000588,
      "loss": 0.573,
      "step": 413
    },
    {
      "epoch": 1.4081632653061225,
      "grad_norm": 0.3659057319164276,
      "learning_rate": 0.000587,
      "loss": 0.4857,
      "step": 414
    },
    {
      "epoch": 1.4115646258503403,
      "grad_norm": 0.43196985125541687,
      "learning_rate": 0.0005859999999999999,
      "loss": 0.6404,
      "step": 415
    },
    {
      "epoch": 1.4149659863945578,
      "grad_norm": 0.6275237798690796,
      "learning_rate": 0.000585,
      "loss": 0.612,
      "step": 416
    },
    {
      "epoch": 1.4183673469387754,
      "grad_norm": 0.40762296319007874,
      "learning_rate": 0.000584,
      "loss": 0.6394,
      "step": 417
    },
    {
      "epoch": 1.4217687074829932,
      "grad_norm": 0.3991619050502777,
      "learning_rate": 0.000583,
      "loss": 0.5098,
      "step": 418
    },
    {
      "epoch": 1.4251700680272108,
      "grad_norm": 1.728105068206787,
      "learning_rate": 0.0005819999999999999,
      "loss": 0.5762,
      "step": 419
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.4017624855041504,
      "learning_rate": 0.0005809999999999999,
      "loss": 0.5638,
      "step": 420
    },
    {
      "epoch": 1.4319727891156462,
      "grad_norm": 0.38483530282974243,
      "learning_rate": 0.00058,
      "loss": 0.5516,
      "step": 421
    },
    {
      "epoch": 1.435374149659864,
      "grad_norm": 0.44072121381759644,
      "learning_rate": 0.000579,
      "loss": 0.5545,
      "step": 422
    },
    {
      "epoch": 1.4387755102040816,
      "grad_norm": 0.7344143390655518,
      "learning_rate": 0.000578,
      "loss": 0.8016,
      "step": 423
    },
    {
      "epoch": 1.4421768707482994,
      "grad_norm": 0.4008866548538208,
      "learning_rate": 0.0005769999999999999,
      "loss": 0.4878,
      "step": 424
    },
    {
      "epoch": 1.445578231292517,
      "grad_norm": 0.46454671025276184,
      "learning_rate": 0.000576,
      "loss": 0.4659,
      "step": 425
    },
    {
      "epoch": 1.4489795918367347,
      "grad_norm": 0.4455871284008026,
      "learning_rate": 0.000575,
      "loss": 0.63,
      "step": 426
    },
    {
      "epoch": 1.4523809523809523,
      "grad_norm": 0.3611079752445221,
      "learning_rate": 0.000574,
      "loss": 0.5534,
      "step": 427
    },
    {
      "epoch": 1.4557823129251701,
      "grad_norm": 1.096718192100525,
      "learning_rate": 0.0005729999999999999,
      "loss": 0.4785,
      "step": 428
    },
    {
      "epoch": 1.4591836734693877,
      "grad_norm": 0.38688793778419495,
      "learning_rate": 0.0005719999999999999,
      "loss": 0.5322,
      "step": 429
    },
    {
      "epoch": 1.4625850340136055,
      "grad_norm": 0.3902435600757599,
      "learning_rate": 0.000571,
      "loss": 0.4965,
      "step": 430
    },
    {
      "epoch": 1.465986394557823,
      "grad_norm": 0.34456363320350647,
      "learning_rate": 0.00057,
      "loss": 0.473,
      "step": 431
    },
    {
      "epoch": 1.469387755102041,
      "grad_norm": 0.4854115843772888,
      "learning_rate": 0.000569,
      "loss": 0.6032,
      "step": 432
    },
    {
      "epoch": 1.4727891156462585,
      "grad_norm": 0.3829842507839203,
      "learning_rate": 0.0005679999999999999,
      "loss": 0.5455,
      "step": 433
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 0.468318909406662,
      "learning_rate": 0.000567,
      "loss": 0.6026,
      "step": 434
    },
    {
      "epoch": 1.4795918367346939,
      "grad_norm": 0.5083014965057373,
      "learning_rate": 0.000566,
      "loss": 0.4558,
      "step": 435
    },
    {
      "epoch": 1.4829931972789114,
      "grad_norm": 0.3654029667377472,
      "learning_rate": 0.000565,
      "loss": 0.4439,
      "step": 436
    },
    {
      "epoch": 1.4863945578231292,
      "grad_norm": 0.3958032727241516,
      "learning_rate": 0.0005639999999999999,
      "loss": 0.6068,
      "step": 437
    },
    {
      "epoch": 1.489795918367347,
      "grad_norm": 0.3945842981338501,
      "learning_rate": 0.0005629999999999999,
      "loss": 0.5938,
      "step": 438
    },
    {
      "epoch": 1.4931972789115646,
      "grad_norm": 0.3588222563266754,
      "learning_rate": 0.0005620000000000001,
      "loss": 0.4494,
      "step": 439
    },
    {
      "epoch": 1.4965986394557822,
      "grad_norm": 0.3557702600955963,
      "learning_rate": 0.0005610000000000001,
      "loss": 0.5245,
      "step": 440
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.3805009424686432,
      "learning_rate": 0.0005600000000000001,
      "loss": 0.5184,
      "step": 441
    },
    {
      "epoch": 1.5034013605442178,
      "grad_norm": 0.43106067180633545,
      "learning_rate": 0.000559,
      "loss": 0.6131,
      "step": 442
    },
    {
      "epoch": 1.5068027210884354,
      "grad_norm": 0.7293701767921448,
      "learning_rate": 0.000558,
      "loss": 0.8652,
      "step": 443
    },
    {
      "epoch": 1.510204081632653,
      "grad_norm": 0.7642205953598022,
      "learning_rate": 0.0005570000000000001,
      "loss": 0.6634,
      "step": 444
    },
    {
      "epoch": 1.5136054421768708,
      "grad_norm": 0.7345632910728455,
      "learning_rate": 0.0005560000000000001,
      "loss": 0.6366,
      "step": 445
    },
    {
      "epoch": 1.5170068027210886,
      "grad_norm": 0.37771350145339966,
      "learning_rate": 0.000555,
      "loss": 0.4945,
      "step": 446
    },
    {
      "epoch": 1.5204081632653061,
      "grad_norm": 0.3571699857711792,
      "learning_rate": 0.000554,
      "loss": 0.4599,
      "step": 447
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.569014847278595,
      "learning_rate": 0.0005530000000000001,
      "loss": 0.6569,
      "step": 448
    },
    {
      "epoch": 1.5272108843537415,
      "grad_norm": 0.44904908537864685,
      "learning_rate": 0.0005520000000000001,
      "loss": 0.4641,
      "step": 449
    },
    {
      "epoch": 1.5306122448979593,
      "grad_norm": 0.33931100368499756,
      "learning_rate": 0.0005510000000000001,
      "loss": 0.4219,
      "step": 450
    },
    {
      "epoch": 1.534013605442177,
      "grad_norm": 0.4107019305229187,
      "learning_rate": 0.00055,
      "loss": 0.475,
      "step": 451
    },
    {
      "epoch": 1.5374149659863945,
      "grad_norm": 0.4067656397819519,
      "learning_rate": 0.000549,
      "loss": 0.4212,
      "step": 452
    },
    {
      "epoch": 1.5408163265306123,
      "grad_norm": 0.46120625734329224,
      "learning_rate": 0.0005480000000000001,
      "loss": 0.7276,
      "step": 453
    },
    {
      "epoch": 1.54421768707483,
      "grad_norm": 0.7941103577613831,
      "learning_rate": 0.0005470000000000001,
      "loss": 0.8516,
      "step": 454
    },
    {
      "epoch": 1.5476190476190477,
      "grad_norm": 0.48437371850013733,
      "learning_rate": 0.000546,
      "loss": 0.6397,
      "step": 455
    },
    {
      "epoch": 1.5510204081632653,
      "grad_norm": 0.6146100759506226,
      "learning_rate": 0.000545,
      "loss": 0.5704,
      "step": 456
    },
    {
      "epoch": 1.554421768707483,
      "grad_norm": 0.43065279722213745,
      "learning_rate": 0.0005440000000000001,
      "loss": 0.5249,
      "step": 457
    },
    {
      "epoch": 1.5578231292517006,
      "grad_norm": 0.7874581813812256,
      "learning_rate": 0.0005430000000000001,
      "loss": 0.511,
      "step": 458
    },
    {
      "epoch": 1.5612244897959182,
      "grad_norm": 0.32095015048980713,
      "learning_rate": 0.0005420000000000001,
      "loss": 0.4211,
      "step": 459
    },
    {
      "epoch": 1.564625850340136,
      "grad_norm": 0.4780237078666687,
      "learning_rate": 0.000541,
      "loss": 0.608,
      "step": 460
    },
    {
      "epoch": 1.5680272108843538,
      "grad_norm": 0.4476306140422821,
      "learning_rate": 0.00054,
      "loss": 0.6778,
      "step": 461
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.5810877680778503,
      "learning_rate": 0.0005390000000000001,
      "loss": 0.813,
      "step": 462
    },
    {
      "epoch": 1.574829931972789,
      "grad_norm": 0.41699591279029846,
      "learning_rate": 0.0005380000000000001,
      "loss": 0.4998,
      "step": 463
    },
    {
      "epoch": 1.5782312925170068,
      "grad_norm": 0.44852492213249207,
      "learning_rate": 0.000537,
      "loss": 0.7002,
      "step": 464
    },
    {
      "epoch": 1.5816326530612246,
      "grad_norm": 0.3897212743759155,
      "learning_rate": 0.000536,
      "loss": 0.5501,
      "step": 465
    },
    {
      "epoch": 1.5850340136054422,
      "grad_norm": 0.4685978889465332,
      "learning_rate": 0.000535,
      "loss": 0.602,
      "step": 466
    },
    {
      "epoch": 1.5884353741496597,
      "grad_norm": 0.38145965337753296,
      "learning_rate": 0.0005340000000000001,
      "loss": 0.5374,
      "step": 467
    },
    {
      "epoch": 1.5918367346938775,
      "grad_norm": 0.45153185725212097,
      "learning_rate": 0.000533,
      "loss": 0.658,
      "step": 468
    },
    {
      "epoch": 1.5952380952380953,
      "grad_norm": 0.3400948941707611,
      "learning_rate": 0.000532,
      "loss": 0.5156,
      "step": 469
    },
    {
      "epoch": 1.598639455782313,
      "grad_norm": 0.4111236035823822,
      "learning_rate": 0.000531,
      "loss": 0.5813,
      "step": 470
    },
    {
      "epoch": 1.6020408163265305,
      "grad_norm": 0.8302266597747803,
      "learning_rate": 0.0005300000000000001,
      "loss": 0.7704,
      "step": 471
    },
    {
      "epoch": 1.6054421768707483,
      "grad_norm": 0.3966231346130371,
      "learning_rate": 0.0005290000000000001,
      "loss": 0.515,
      "step": 472
    },
    {
      "epoch": 1.608843537414966,
      "grad_norm": 0.35397034883499146,
      "learning_rate": 0.000528,
      "loss": 0.4783,
      "step": 473
    },
    {
      "epoch": 1.6122448979591837,
      "grad_norm": 0.9343937635421753,
      "learning_rate": 0.000527,
      "loss": 0.4504,
      "step": 474
    },
    {
      "epoch": 1.6156462585034013,
      "grad_norm": 0.4249943494796753,
      "learning_rate": 0.000526,
      "loss": 0.5128,
      "step": 475
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 0.32877108454704285,
      "learning_rate": 0.0005250000000000001,
      "loss": 0.4317,
      "step": 476
    },
    {
      "epoch": 1.6224489795918369,
      "grad_norm": 0.4309558868408203,
      "learning_rate": 0.000524,
      "loss": 0.5042,
      "step": 477
    },
    {
      "epoch": 1.6258503401360545,
      "grad_norm": 0.4168809652328491,
      "learning_rate": 0.000523,
      "loss": 0.5626,
      "step": 478
    },
    {
      "epoch": 1.629251700680272,
      "grad_norm": 0.34742042422294617,
      "learning_rate": 0.000522,
      "loss": 0.5138,
      "step": 479
    },
    {
      "epoch": 1.6326530612244898,
      "grad_norm": 0.36099058389663696,
      "learning_rate": 0.000521,
      "loss": 0.4561,
      "step": 480
    },
    {
      "epoch": 1.6360544217687076,
      "grad_norm": 0.40878817439079285,
      "learning_rate": 0.0005200000000000001,
      "loss": 0.5345,
      "step": 481
    },
    {
      "epoch": 1.6394557823129252,
      "grad_norm": 0.3205166757106781,
      "learning_rate": 0.000519,
      "loss": 0.3915,
      "step": 482
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 0.35774844884872437,
      "learning_rate": 0.000518,
      "loss": 0.41,
      "step": 483
    },
    {
      "epoch": 1.6462585034013606,
      "grad_norm": 0.5919167399406433,
      "learning_rate": 0.000517,
      "loss": 0.7813,
      "step": 484
    },
    {
      "epoch": 1.6496598639455784,
      "grad_norm": 0.42828917503356934,
      "learning_rate": 0.0005160000000000001,
      "loss": 0.5051,
      "step": 485
    },
    {
      "epoch": 1.6530612244897958,
      "grad_norm": 0.8246292471885681,
      "learning_rate": 0.000515,
      "loss": 0.7809,
      "step": 486
    },
    {
      "epoch": 1.6564625850340136,
      "grad_norm": 0.6117207407951355,
      "learning_rate": 0.000514,
      "loss": 0.8184,
      "step": 487
    },
    {
      "epoch": 1.6598639455782314,
      "grad_norm": 0.4125910699367523,
      "learning_rate": 0.000513,
      "loss": 0.5103,
      "step": 488
    },
    {
      "epoch": 1.663265306122449,
      "grad_norm": 0.34731778502464294,
      "learning_rate": 0.000512,
      "loss": 0.3676,
      "step": 489
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.5622156262397766,
      "learning_rate": 0.0005110000000000001,
      "loss": 0.7706,
      "step": 490
    },
    {
      "epoch": 1.6700680272108843,
      "grad_norm": 0.40113747119903564,
      "learning_rate": 0.00051,
      "loss": 0.4542,
      "step": 491
    },
    {
      "epoch": 1.6734693877551021,
      "grad_norm": 0.4263211488723755,
      "learning_rate": 0.000509,
      "loss": 0.5913,
      "step": 492
    },
    {
      "epoch": 1.6768707482993197,
      "grad_norm": 0.4428161382675171,
      "learning_rate": 0.000508,
      "loss": 0.6479,
      "step": 493
    },
    {
      "epoch": 1.6802721088435373,
      "grad_norm": 0.461599200963974,
      "learning_rate": 0.000507,
      "loss": 0.6927,
      "step": 494
    },
    {
      "epoch": 1.683673469387755,
      "grad_norm": 0.4318101704120636,
      "learning_rate": 0.000506,
      "loss": 0.5432,
      "step": 495
    },
    {
      "epoch": 1.6870748299319729,
      "grad_norm": 0.35210496187210083,
      "learning_rate": 0.000505,
      "loss": 0.4812,
      "step": 496
    },
    {
      "epoch": 1.6904761904761905,
      "grad_norm": 0.6439231038093567,
      "learning_rate": 0.000504,
      "loss": 0.7088,
      "step": 497
    },
    {
      "epoch": 1.693877551020408,
      "grad_norm": 0.35662737488746643,
      "learning_rate": 0.000503,
      "loss": 0.5238,
      "step": 498
    },
    {
      "epoch": 1.6972789115646258,
      "grad_norm": 0.3877478241920471,
      "learning_rate": 0.0005020000000000001,
      "loss": 0.4891,
      "step": 499
    },
    {
      "epoch": 1.7006802721088436,
      "grad_norm": 0.39178746938705444,
      "learning_rate": 0.000501,
      "loss": 0.5304,
      "step": 500
    },
    {
      "epoch": 1.7040816326530612,
      "grad_norm": 0.3719993233680725,
      "learning_rate": 0.0005,
      "loss": 0.4683,
      "step": 501
    },
    {
      "epoch": 1.7074829931972788,
      "grad_norm": 0.5093944072723389,
      "learning_rate": 0.000499,
      "loss": 0.7225,
      "step": 502
    },
    {
      "epoch": 1.7108843537414966,
      "grad_norm": 0.37583351135253906,
      "learning_rate": 0.000498,
      "loss": 0.5056,
      "step": 503
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 0.6115846037864685,
      "learning_rate": 0.000497,
      "loss": 0.6735,
      "step": 504
    },
    {
      "epoch": 1.717687074829932,
      "grad_norm": 0.3653305172920227,
      "learning_rate": 0.000496,
      "loss": 0.4839,
      "step": 505
    },
    {
      "epoch": 1.7210884353741496,
      "grad_norm": 0.6876494884490967,
      "learning_rate": 0.000495,
      "loss": 0.8807,
      "step": 506
    },
    {
      "epoch": 1.7244897959183674,
      "grad_norm": 0.6734439730644226,
      "learning_rate": 0.000494,
      "loss": 0.6317,
      "step": 507
    },
    {
      "epoch": 1.7278911564625852,
      "grad_norm": 0.35688236355781555,
      "learning_rate": 0.0004930000000000001,
      "loss": 0.5341,
      "step": 508
    },
    {
      "epoch": 1.7312925170068028,
      "grad_norm": 0.985443115234375,
      "learning_rate": 0.000492,
      "loss": 0.7588,
      "step": 509
    },
    {
      "epoch": 1.7346938775510203,
      "grad_norm": 0.6128401756286621,
      "learning_rate": 0.000491,
      "loss": 0.6552,
      "step": 510
    },
    {
      "epoch": 1.7380952380952381,
      "grad_norm": 0.38985729217529297,
      "learning_rate": 0.00049,
      "loss": 0.5632,
      "step": 511
    },
    {
      "epoch": 1.741496598639456,
      "grad_norm": 0.534986674785614,
      "learning_rate": 0.000489,
      "loss": 0.8182,
      "step": 512
    },
    {
      "epoch": 1.7448979591836735,
      "grad_norm": 0.42058655619621277,
      "learning_rate": 0.000488,
      "loss": 0.4852,
      "step": 513
    },
    {
      "epoch": 1.748299319727891,
      "grad_norm": 0.4304945468902588,
      "learning_rate": 0.000487,
      "loss": 0.4434,
      "step": 514
    },
    {
      "epoch": 1.751700680272109,
      "grad_norm": 0.5245054960250854,
      "learning_rate": 0.000486,
      "loss": 0.6643,
      "step": 515
    },
    {
      "epoch": 1.7551020408163265,
      "grad_norm": 0.6611493229866028,
      "learning_rate": 0.00048499999999999997,
      "loss": 0.823,
      "step": 516
    },
    {
      "epoch": 1.758503401360544,
      "grad_norm": 0.44613176584243774,
      "learning_rate": 0.000484,
      "loss": 0.583,
      "step": 517
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 0.4641825258731842,
      "learning_rate": 0.000483,
      "loss": 0.5658,
      "step": 518
    },
    {
      "epoch": 1.7653061224489797,
      "grad_norm": 0.41564565896987915,
      "learning_rate": 0.000482,
      "loss": 0.5195,
      "step": 519
    },
    {
      "epoch": 1.7687074829931972,
      "grad_norm": 0.43910887837409973,
      "learning_rate": 0.000481,
      "loss": 0.5521,
      "step": 520
    },
    {
      "epoch": 1.7721088435374148,
      "grad_norm": 0.4210354685783386,
      "learning_rate": 0.00048,
      "loss": 0.4641,
      "step": 521
    },
    {
      "epoch": 1.7755102040816326,
      "grad_norm": 0.4469608664512634,
      "learning_rate": 0.000479,
      "loss": 0.5687,
      "step": 522
    },
    {
      "epoch": 1.7789115646258504,
      "grad_norm": 0.4538816511631012,
      "learning_rate": 0.00047799999999999996,
      "loss": 0.6208,
      "step": 523
    },
    {
      "epoch": 1.782312925170068,
      "grad_norm": 0.46719926595687866,
      "learning_rate": 0.000477,
      "loss": 0.6549,
      "step": 524
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 0.43229275941848755,
      "learning_rate": 0.00047599999999999997,
      "loss": 0.6883,
      "step": 525
    },
    {
      "epoch": 1.7891156462585034,
      "grad_norm": 0.7137728929519653,
      "learning_rate": 0.000475,
      "loss": 0.7708,
      "step": 526
    },
    {
      "epoch": 1.7925170068027212,
      "grad_norm": 0.5746672749519348,
      "learning_rate": 0.000474,
      "loss": 0.6906,
      "step": 527
    },
    {
      "epoch": 1.7959183673469388,
      "grad_norm": 0.5125105977058411,
      "learning_rate": 0.000473,
      "loss": 0.7903,
      "step": 528
    },
    {
      "epoch": 1.7993197278911564,
      "grad_norm": 0.35792022943496704,
      "learning_rate": 0.000472,
      "loss": 0.497,
      "step": 529
    },
    {
      "epoch": 1.8027210884353742,
      "grad_norm": 0.37235304713249207,
      "learning_rate": 0.000471,
      "loss": 0.4693,
      "step": 530
    },
    {
      "epoch": 1.806122448979592,
      "grad_norm": 0.3818470537662506,
      "learning_rate": 0.00047,
      "loss": 0.5022,
      "step": 531
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 0.6684969067573547,
      "learning_rate": 0.00046899999999999996,
      "loss": 0.6836,
      "step": 532
    },
    {
      "epoch": 1.8129251700680271,
      "grad_norm": 0.568112850189209,
      "learning_rate": 0.00046800000000000005,
      "loss": 0.6945,
      "step": 533
    },
    {
      "epoch": 1.816326530612245,
      "grad_norm": 0.4245455861091614,
      "learning_rate": 0.000467,
      "loss": 0.571,
      "step": 534
    },
    {
      "epoch": 1.8197278911564627,
      "grad_norm": 0.40540406107902527,
      "learning_rate": 0.00046600000000000005,
      "loss": 0.4957,
      "step": 535
    },
    {
      "epoch": 1.8231292517006803,
      "grad_norm": 0.3676404058933258,
      "learning_rate": 0.000465,
      "loss": 0.5622,
      "step": 536
    },
    {
      "epoch": 1.8265306122448979,
      "grad_norm": 0.39079537987709045,
      "learning_rate": 0.00046400000000000006,
      "loss": 0.4898,
      "step": 537
    },
    {
      "epoch": 1.8299319727891157,
      "grad_norm": 0.3884134292602539,
      "learning_rate": 0.00046300000000000003,
      "loss": 0.5625,
      "step": 538
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.4070723056793213,
      "learning_rate": 0.000462,
      "loss": 0.6164,
      "step": 539
    },
    {
      "epoch": 1.836734693877551,
      "grad_norm": 0.36045512557029724,
      "learning_rate": 0.00046100000000000004,
      "loss": 0.504,
      "step": 540
    },
    {
      "epoch": 1.8401360544217686,
      "grad_norm": 0.3652549386024475,
      "learning_rate": 0.00046,
      "loss": 0.5008,
      "step": 541
    },
    {
      "epoch": 1.8435374149659864,
      "grad_norm": 0.6444146633148193,
      "learning_rate": 0.00045900000000000004,
      "loss": 0.7038,
      "step": 542
    },
    {
      "epoch": 1.8469387755102042,
      "grad_norm": 0.4096437394618988,
      "learning_rate": 0.000458,
      "loss": 0.5799,
      "step": 543
    },
    {
      "epoch": 1.8503401360544216,
      "grad_norm": 0.39295494556427,
      "learning_rate": 0.00045700000000000005,
      "loss": 0.686,
      "step": 544
    },
    {
      "epoch": 1.8537414965986394,
      "grad_norm": 0.4388463497161865,
      "learning_rate": 0.000456,
      "loss": 0.686,
      "step": 545
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.4389379918575287,
      "learning_rate": 0.000455,
      "loss": 0.5634,
      "step": 546
    },
    {
      "epoch": 1.8605442176870748,
      "grad_norm": 0.4681272506713867,
      "learning_rate": 0.00045400000000000003,
      "loss": 0.6509,
      "step": 547
    },
    {
      "epoch": 1.8639455782312924,
      "grad_norm": 0.3469623625278473,
      "learning_rate": 0.000453,
      "loss": 0.533,
      "step": 548
    },
    {
      "epoch": 1.8673469387755102,
      "grad_norm": 0.3931783437728882,
      "learning_rate": 0.00045200000000000004,
      "loss": 0.4772,
      "step": 549
    },
    {
      "epoch": 1.870748299319728,
      "grad_norm": 0.37508171796798706,
      "learning_rate": 0.000451,
      "loss": 0.4729,
      "step": 550
    },
    {
      "epoch": 1.8741496598639455,
      "grad_norm": 0.3813720941543579,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.5116,
      "step": 551
    },
    {
      "epoch": 1.8775510204081631,
      "grad_norm": 0.3779955804347992,
      "learning_rate": 0.000449,
      "loss": 0.5643,
      "step": 552
    },
    {
      "epoch": 1.880952380952381,
      "grad_norm": 0.40049436688423157,
      "learning_rate": 0.000448,
      "loss": 0.5894,
      "step": 553
    },
    {
      "epoch": 1.8843537414965987,
      "grad_norm": 0.7092003226280212,
      "learning_rate": 0.000447,
      "loss": 0.7935,
      "step": 554
    },
    {
      "epoch": 1.8877551020408163,
      "grad_norm": 0.36103567481040955,
      "learning_rate": 0.000446,
      "loss": 0.5156,
      "step": 555
    },
    {
      "epoch": 1.891156462585034,
      "grad_norm": 0.3923347294330597,
      "learning_rate": 0.00044500000000000003,
      "loss": 0.5238,
      "step": 556
    },
    {
      "epoch": 1.8945578231292517,
      "grad_norm": 0.3526490330696106,
      "learning_rate": 0.000444,
      "loss": 0.5047,
      "step": 557
    },
    {
      "epoch": 1.8979591836734695,
      "grad_norm": 0.38785985112190247,
      "learning_rate": 0.00044300000000000003,
      "loss": 0.5691,
      "step": 558
    },
    {
      "epoch": 1.901360544217687,
      "grad_norm": 0.7209619283676147,
      "learning_rate": 0.000442,
      "loss": 0.745,
      "step": 559
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 0.35844188928604126,
      "learning_rate": 0.000441,
      "loss": 0.4915,
      "step": 560
    },
    {
      "epoch": 1.9081632653061225,
      "grad_norm": 0.34132495522499084,
      "learning_rate": 0.00044,
      "loss": 0.4607,
      "step": 561
    },
    {
      "epoch": 1.9115646258503403,
      "grad_norm": 0.35286420583724976,
      "learning_rate": 0.000439,
      "loss": 0.5094,
      "step": 562
    },
    {
      "epoch": 1.9149659863945578,
      "grad_norm": 0.38216036558151245,
      "learning_rate": 0.000438,
      "loss": 0.5776,
      "step": 563
    },
    {
      "epoch": 1.9183673469387754,
      "grad_norm": 0.32275280356407166,
      "learning_rate": 0.000437,
      "loss": 0.486,
      "step": 564
    },
    {
      "epoch": 1.9217687074829932,
      "grad_norm": 0.3159349858760834,
      "learning_rate": 0.000436,
      "loss": 0.4585,
      "step": 565
    },
    {
      "epoch": 1.925170068027211,
      "grad_norm": 0.42560383677482605,
      "learning_rate": 0.000435,
      "loss": 0.6263,
      "step": 566
    },
    {
      "epoch": 1.9285714285714286,
      "grad_norm": 0.33788904547691345,
      "learning_rate": 0.00043400000000000003,
      "loss": 0.4818,
      "step": 567
    },
    {
      "epoch": 1.9319727891156462,
      "grad_norm": 0.36977216601371765,
      "learning_rate": 0.000433,
      "loss": 0.4407,
      "step": 568
    },
    {
      "epoch": 1.935374149659864,
      "grad_norm": 0.37746959924697876,
      "learning_rate": 0.000432,
      "loss": 0.6248,
      "step": 569
    },
    {
      "epoch": 1.9387755102040818,
      "grad_norm": 0.39033836126327515,
      "learning_rate": 0.000431,
      "loss": 0.4956,
      "step": 570
    },
    {
      "epoch": 1.9421768707482994,
      "grad_norm": 0.6738771200180054,
      "learning_rate": 0.00043,
      "loss": 0.7172,
      "step": 571
    },
    {
      "epoch": 1.945578231292517,
      "grad_norm": 0.6186913847923279,
      "learning_rate": 0.000429,
      "loss": 0.7038,
      "step": 572
    },
    {
      "epoch": 1.9489795918367347,
      "grad_norm": 0.39395225048065186,
      "learning_rate": 0.000428,
      "loss": 0.6096,
      "step": 573
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 0.6769891977310181,
      "learning_rate": 0.000427,
      "loss": 0.753,
      "step": 574
    },
    {
      "epoch": 1.95578231292517,
      "grad_norm": 0.42648714780807495,
      "learning_rate": 0.000426,
      "loss": 0.6385,
      "step": 575
    },
    {
      "epoch": 1.9591836734693877,
      "grad_norm": 0.33787205815315247,
      "learning_rate": 0.000425,
      "loss": 0.436,
      "step": 576
    },
    {
      "epoch": 1.9625850340136055,
      "grad_norm": 0.33183857798576355,
      "learning_rate": 0.000424,
      "loss": 0.4405,
      "step": 577
    },
    {
      "epoch": 1.965986394557823,
      "grad_norm": 0.3928757905960083,
      "learning_rate": 0.000423,
      "loss": 0.5799,
      "step": 578
    },
    {
      "epoch": 1.9693877551020407,
      "grad_norm": 0.4072439968585968,
      "learning_rate": 0.000422,
      "loss": 0.4873,
      "step": 579
    },
    {
      "epoch": 1.9727891156462585,
      "grad_norm": 0.361632764339447,
      "learning_rate": 0.000421,
      "loss": 0.5166,
      "step": 580
    },
    {
      "epoch": 1.9761904761904763,
      "grad_norm": 0.3585166335105896,
      "learning_rate": 0.00042,
      "loss": 0.4015,
      "step": 581
    },
    {
      "epoch": 1.9795918367346939,
      "grad_norm": 0.4438639283180237,
      "learning_rate": 0.000419,
      "loss": 0.4959,
      "step": 582
    },
    {
      "epoch": 1.9829931972789114,
      "grad_norm": 0.5382132530212402,
      "learning_rate": 0.00041799999999999997,
      "loss": 0.6522,
      "step": 583
    },
    {
      "epoch": 1.9863945578231292,
      "grad_norm": 0.651134729385376,
      "learning_rate": 0.000417,
      "loss": 0.7136,
      "step": 584
    },
    {
      "epoch": 1.989795918367347,
      "grad_norm": 0.34365522861480713,
      "learning_rate": 0.000416,
      "loss": 0.3993,
      "step": 585
    },
    {
      "epoch": 1.9931972789115646,
      "grad_norm": 0.36001700162887573,
      "learning_rate": 0.000415,
      "loss": 0.4885,
      "step": 586
    },
    {
      "epoch": 1.9965986394557822,
      "grad_norm": 0.4122874438762665,
      "learning_rate": 0.000414,
      "loss": 0.5761,
      "step": 587
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.3598294258117676,
      "learning_rate": 0.000413,
      "loss": 0.4745,
      "step": 588
    },
    {
      "epoch": 2.003401360544218,
      "grad_norm": 0.39688992500305176,
      "learning_rate": 0.000412,
      "loss": 0.4981,
      "step": 589
    },
    {
      "epoch": 2.006802721088435,
      "grad_norm": 0.37934356927871704,
      "learning_rate": 0.00041099999999999996,
      "loss": 0.4295,
      "step": 590
    },
    {
      "epoch": 2.010204081632653,
      "grad_norm": 0.37139344215393066,
      "learning_rate": 0.00041,
      "loss": 0.5486,
      "step": 591
    },
    {
      "epoch": 2.0136054421768708,
      "grad_norm": 0.37134015560150146,
      "learning_rate": 0.00040899999999999997,
      "loss": 0.4521,
      "step": 592
    },
    {
      "epoch": 2.0170068027210886,
      "grad_norm": 0.3971015512943268,
      "learning_rate": 0.000408,
      "loss": 0.4771,
      "step": 593
    },
    {
      "epoch": 2.020408163265306,
      "grad_norm": 0.40161725878715515,
      "learning_rate": 0.00040699999999999997,
      "loss": 0.5481,
      "step": 594
    },
    {
      "epoch": 2.0238095238095237,
      "grad_norm": 0.43973255157470703,
      "learning_rate": 0.00040600000000000006,
      "loss": 0.6396,
      "step": 595
    },
    {
      "epoch": 2.0272108843537415,
      "grad_norm": 0.36821117997169495,
      "learning_rate": 0.00040500000000000003,
      "loss": 0.5582,
      "step": 596
    },
    {
      "epoch": 2.0306122448979593,
      "grad_norm": 0.3321211636066437,
      "learning_rate": 0.000404,
      "loss": 0.3697,
      "step": 597
    },
    {
      "epoch": 2.0340136054421767,
      "grad_norm": 0.396685391664505,
      "learning_rate": 0.00040300000000000004,
      "loss": 0.5827,
      "step": 598
    },
    {
      "epoch": 2.0374149659863945,
      "grad_norm": 0.4362660348415375,
      "learning_rate": 0.000402,
      "loss": 0.5956,
      "step": 599
    },
    {
      "epoch": 2.0408163265306123,
      "grad_norm": 0.4020931124687195,
      "learning_rate": 0.00040100000000000004,
      "loss": 0.5169,
      "step": 600
    },
    {
      "epoch": 2.04421768707483,
      "grad_norm": 0.414559930562973,
      "learning_rate": 0.0004,
      "loss": 0.5338,
      "step": 601
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 0.32974162697792053,
      "learning_rate": 0.00039900000000000005,
      "loss": 0.4471,
      "step": 602
    },
    {
      "epoch": 2.0510204081632653,
      "grad_norm": 0.9827978610992432,
      "learning_rate": 0.000398,
      "loss": 0.5313,
      "step": 603
    },
    {
      "epoch": 2.054421768707483,
      "grad_norm": 0.7275838255882263,
      "learning_rate": 0.00039700000000000005,
      "loss": 0.8425,
      "step": 604
    },
    {
      "epoch": 2.057823129251701,
      "grad_norm": 0.3939170241355896,
      "learning_rate": 0.00039600000000000003,
      "loss": 0.5023,
      "step": 605
    },
    {
      "epoch": 2.061224489795918,
      "grad_norm": 0.3692057728767395,
      "learning_rate": 0.000395,
      "loss": 0.487,
      "step": 606
    },
    {
      "epoch": 2.064625850340136,
      "grad_norm": 0.3793211281299591,
      "learning_rate": 0.00039400000000000004,
      "loss": 0.5192,
      "step": 607
    },
    {
      "epoch": 2.068027210884354,
      "grad_norm": 0.4142305254936218,
      "learning_rate": 0.000393,
      "loss": 0.6335,
      "step": 608
    },
    {
      "epoch": 2.0714285714285716,
      "grad_norm": 0.43514153361320496,
      "learning_rate": 0.00039200000000000004,
      "loss": 0.64,
      "step": 609
    },
    {
      "epoch": 2.074829931972789,
      "grad_norm": 0.40945932269096375,
      "learning_rate": 0.000391,
      "loss": 0.4843,
      "step": 610
    },
    {
      "epoch": 2.078231292517007,
      "grad_norm": 0.5972234010696411,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.6671,
      "step": 611
    },
    {
      "epoch": 2.0816326530612246,
      "grad_norm": 0.41260677576065063,
      "learning_rate": 0.000389,
      "loss": 0.5615,
      "step": 612
    },
    {
      "epoch": 2.0850340136054424,
      "grad_norm": 0.3505583703517914,
      "learning_rate": 0.000388,
      "loss": 0.4906,
      "step": 613
    },
    {
      "epoch": 2.0884353741496597,
      "grad_norm": 0.5744602680206299,
      "learning_rate": 0.00038700000000000003,
      "loss": 0.6153,
      "step": 614
    },
    {
      "epoch": 2.0918367346938775,
      "grad_norm": 0.6039581894874573,
      "learning_rate": 0.000386,
      "loss": 0.8021,
      "step": 615
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.4210602641105652,
      "learning_rate": 0.00038500000000000003,
      "loss": 0.5893,
      "step": 616
    },
    {
      "epoch": 2.0986394557823127,
      "grad_norm": 0.8429011702537537,
      "learning_rate": 0.000384,
      "loss": 0.6371,
      "step": 617
    },
    {
      "epoch": 2.1020408163265305,
      "grad_norm": 0.45123761892318726,
      "learning_rate": 0.00038300000000000004,
      "loss": 0.6786,
      "step": 618
    },
    {
      "epoch": 2.1054421768707483,
      "grad_norm": 0.36806848645210266,
      "learning_rate": 0.000382,
      "loss": 0.4823,
      "step": 619
    },
    {
      "epoch": 2.108843537414966,
      "grad_norm": 0.4335032105445862,
      "learning_rate": 0.000381,
      "loss": 0.5712,
      "step": 620
    },
    {
      "epoch": 2.1122448979591835,
      "grad_norm": 0.3921753764152527,
      "learning_rate": 0.00038,
      "loss": 0.4159,
      "step": 621
    },
    {
      "epoch": 2.1156462585034013,
      "grad_norm": 0.376659095287323,
      "learning_rate": 0.000379,
      "loss": 0.5076,
      "step": 622
    },
    {
      "epoch": 2.119047619047619,
      "grad_norm": 0.7083644270896912,
      "learning_rate": 0.000378,
      "loss": 0.7311,
      "step": 623
    },
    {
      "epoch": 2.122448979591837,
      "grad_norm": 0.38646769523620605,
      "learning_rate": 0.000377,
      "loss": 0.4537,
      "step": 624
    },
    {
      "epoch": 2.1258503401360542,
      "grad_norm": 0.47482335567474365,
      "learning_rate": 0.00037600000000000003,
      "loss": 0.6501,
      "step": 625
    },
    {
      "epoch": 2.129251700680272,
      "grad_norm": 0.3902798891067505,
      "learning_rate": 0.000375,
      "loss": 0.4814,
      "step": 626
    },
    {
      "epoch": 2.13265306122449,
      "grad_norm": 0.39121755957603455,
      "learning_rate": 0.000374,
      "loss": 0.53,
      "step": 627
    },
    {
      "epoch": 2.1360544217687076,
      "grad_norm": 0.48380500078201294,
      "learning_rate": 0.000373,
      "loss": 0.6336,
      "step": 628
    },
    {
      "epoch": 2.139455782312925,
      "grad_norm": 0.3899043798446655,
      "learning_rate": 0.000372,
      "loss": 0.4898,
      "step": 629
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.5179905295372009,
      "learning_rate": 0.000371,
      "loss": 0.7825,
      "step": 630
    },
    {
      "epoch": 2.1462585034013606,
      "grad_norm": 0.40362849831581116,
      "learning_rate": 0.00037,
      "loss": 0.5995,
      "step": 631
    },
    {
      "epoch": 2.1496598639455784,
      "grad_norm": 0.37550392746925354,
      "learning_rate": 0.000369,
      "loss": 0.5335,
      "step": 632
    },
    {
      "epoch": 2.1530612244897958,
      "grad_norm": 0.7034806609153748,
      "learning_rate": 0.000368,
      "loss": 0.7832,
      "step": 633
    },
    {
      "epoch": 2.1564625850340136,
      "grad_norm": 0.3575666546821594,
      "learning_rate": 0.000367,
      "loss": 0.5184,
      "step": 634
    },
    {
      "epoch": 2.1598639455782314,
      "grad_norm": 0.39553460478782654,
      "learning_rate": 0.000366,
      "loss": 0.4906,
      "step": 635
    },
    {
      "epoch": 2.163265306122449,
      "grad_norm": 0.4301643967628479,
      "learning_rate": 0.000365,
      "loss": 0.6107,
      "step": 636
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.6951864361763,
      "learning_rate": 0.000364,
      "loss": 0.8383,
      "step": 637
    },
    {
      "epoch": 2.1700680272108843,
      "grad_norm": 0.36719951033592224,
      "learning_rate": 0.000363,
      "loss": 0.474,
      "step": 638
    },
    {
      "epoch": 2.173469387755102,
      "grad_norm": 0.3952556848526001,
      "learning_rate": 0.000362,
      "loss": 0.6573,
      "step": 639
    },
    {
      "epoch": 2.17687074829932,
      "grad_norm": 0.3935798704624176,
      "learning_rate": 0.000361,
      "loss": 0.5065,
      "step": 640
    },
    {
      "epoch": 2.1802721088435373,
      "grad_norm": 0.39111000299453735,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.4898,
      "step": 641
    },
    {
      "epoch": 2.183673469387755,
      "grad_norm": 0.3840170204639435,
      "learning_rate": 0.000359,
      "loss": 0.5489,
      "step": 642
    },
    {
      "epoch": 2.187074829931973,
      "grad_norm": 0.693291962146759,
      "learning_rate": 0.000358,
      "loss": 0.6552,
      "step": 643
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.3811284005641937,
      "learning_rate": 0.000357,
      "loss": 0.441,
      "step": 644
    },
    {
      "epoch": 2.193877551020408,
      "grad_norm": 0.3751178979873657,
      "learning_rate": 0.000356,
      "loss": 0.479,
      "step": 645
    },
    {
      "epoch": 2.197278911564626,
      "grad_norm": 0.4217274785041809,
      "learning_rate": 0.000355,
      "loss": 0.6647,
      "step": 646
    },
    {
      "epoch": 2.2006802721088436,
      "grad_norm": 0.5591371059417725,
      "learning_rate": 0.000354,
      "loss": 0.6481,
      "step": 647
    },
    {
      "epoch": 2.204081632653061,
      "grad_norm": 0.7040664553642273,
      "learning_rate": 0.00035299999999999996,
      "loss": 0.8034,
      "step": 648
    },
    {
      "epoch": 2.207482993197279,
      "grad_norm": 0.42109960317611694,
      "learning_rate": 0.000352,
      "loss": 0.6445,
      "step": 649
    },
    {
      "epoch": 2.2108843537414966,
      "grad_norm": 0.5885796546936035,
      "learning_rate": 0.00035099999999999997,
      "loss": 0.8,
      "step": 650
    },
    {
      "epoch": 2.2142857142857144,
      "grad_norm": 0.36794421076774597,
      "learning_rate": 0.00035,
      "loss": 0.5111,
      "step": 651
    },
    {
      "epoch": 2.2176870748299318,
      "grad_norm": 0.3714912235736847,
      "learning_rate": 0.00034899999999999997,
      "loss": 0.4568,
      "step": 652
    },
    {
      "epoch": 2.2210884353741496,
      "grad_norm": 0.43077099323272705,
      "learning_rate": 0.000348,
      "loss": 0.5902,
      "step": 653
    },
    {
      "epoch": 2.2244897959183674,
      "grad_norm": 0.42144879698753357,
      "learning_rate": 0.000347,
      "loss": 0.6266,
      "step": 654
    },
    {
      "epoch": 2.227891156462585,
      "grad_norm": 0.3522490859031677,
      "learning_rate": 0.000346,
      "loss": 0.5043,
      "step": 655
    },
    {
      "epoch": 2.2312925170068025,
      "grad_norm": 0.4163723587989807,
      "learning_rate": 0.000345,
      "loss": 0.5297,
      "step": 656
    },
    {
      "epoch": 2.2346938775510203,
      "grad_norm": 0.4124273657798767,
      "learning_rate": 0.00034399999999999996,
      "loss": 0.4718,
      "step": 657
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 0.5259599089622498,
      "learning_rate": 0.00034300000000000004,
      "loss": 0.6263,
      "step": 658
    },
    {
      "epoch": 2.241496598639456,
      "grad_norm": 0.41805601119995117,
      "learning_rate": 0.000342,
      "loss": 0.5026,
      "step": 659
    },
    {
      "epoch": 2.2448979591836733,
      "grad_norm": 0.38287317752838135,
      "learning_rate": 0.00034100000000000005,
      "loss": 0.4731,
      "step": 660
    },
    {
      "epoch": 2.248299319727891,
      "grad_norm": 0.6723731756210327,
      "learning_rate": 0.00034,
      "loss": 0.6214,
      "step": 661
    },
    {
      "epoch": 2.251700680272109,
      "grad_norm": 0.3853261172771454,
      "learning_rate": 0.00033900000000000005,
      "loss": 0.4361,
      "step": 662
    },
    {
      "epoch": 2.2551020408163267,
      "grad_norm": 0.6805813312530518,
      "learning_rate": 0.00033800000000000003,
      "loss": 0.5919,
      "step": 663
    },
    {
      "epoch": 2.258503401360544,
      "grad_norm": 0.35191264748573303,
      "learning_rate": 0.000337,
      "loss": 0.466,
      "step": 664
    },
    {
      "epoch": 2.261904761904762,
      "grad_norm": 0.41655680537223816,
      "learning_rate": 0.00033600000000000004,
      "loss": 0.4522,
      "step": 665
    },
    {
      "epoch": 2.2653061224489797,
      "grad_norm": 0.45600903034210205,
      "learning_rate": 0.000335,
      "loss": 0.6061,
      "step": 666
    },
    {
      "epoch": 2.2687074829931975,
      "grad_norm": 0.44396573305130005,
      "learning_rate": 0.00033400000000000004,
      "loss": 0.5028,
      "step": 667
    },
    {
      "epoch": 2.272108843537415,
      "grad_norm": 0.4208870232105255,
      "learning_rate": 0.000333,
      "loss": 0.4922,
      "step": 668
    },
    {
      "epoch": 2.2755102040816326,
      "grad_norm": 0.39182108640670776,
      "learning_rate": 0.00033200000000000005,
      "loss": 0.5838,
      "step": 669
    },
    {
      "epoch": 2.2789115646258504,
      "grad_norm": 0.8116123676300049,
      "learning_rate": 0.000331,
      "loss": 0.4943,
      "step": 670
    },
    {
      "epoch": 2.282312925170068,
      "grad_norm": 0.40923547744750977,
      "learning_rate": 0.00033,
      "loss": 0.673,
      "step": 671
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.44411158561706543,
      "learning_rate": 0.00032900000000000003,
      "loss": 0.5635,
      "step": 672
    },
    {
      "epoch": 2.2891156462585034,
      "grad_norm": 0.3492506444454193,
      "learning_rate": 0.000328,
      "loss": 0.4594,
      "step": 673
    },
    {
      "epoch": 2.292517006802721,
      "grad_norm": 0.7141737937927246,
      "learning_rate": 0.00032700000000000003,
      "loss": 0.8657,
      "step": 674
    },
    {
      "epoch": 2.295918367346939,
      "grad_norm": 0.397809773683548,
      "learning_rate": 0.000326,
      "loss": 0.5081,
      "step": 675
    },
    {
      "epoch": 2.2993197278911564,
      "grad_norm": 0.39328306913375854,
      "learning_rate": 0.00032500000000000004,
      "loss": 0.5558,
      "step": 676
    },
    {
      "epoch": 2.302721088435374,
      "grad_norm": 0.43574070930480957,
      "learning_rate": 0.000324,
      "loss": 0.5166,
      "step": 677
    },
    {
      "epoch": 2.306122448979592,
      "grad_norm": 1.2908644676208496,
      "learning_rate": 0.000323,
      "loss": 0.5302,
      "step": 678
    },
    {
      "epoch": 2.3095238095238093,
      "grad_norm": 0.3762350082397461,
      "learning_rate": 0.000322,
      "loss": 0.4128,
      "step": 679
    },
    {
      "epoch": 2.312925170068027,
      "grad_norm": 0.40452274680137634,
      "learning_rate": 0.000321,
      "loss": 0.553,
      "step": 680
    },
    {
      "epoch": 2.316326530612245,
      "grad_norm": 0.3845154047012329,
      "learning_rate": 0.00032,
      "loss": 0.5287,
      "step": 681
    },
    {
      "epoch": 2.3197278911564627,
      "grad_norm": 0.617985725402832,
      "learning_rate": 0.000319,
      "loss": 0.7589,
      "step": 682
    },
    {
      "epoch": 2.3231292517006805,
      "grad_norm": 0.6960259079933167,
      "learning_rate": 0.00031800000000000003,
      "loss": 0.6998,
      "step": 683
    },
    {
      "epoch": 2.326530612244898,
      "grad_norm": 0.4442165195941925,
      "learning_rate": 0.000317,
      "loss": 0.5086,
      "step": 684
    },
    {
      "epoch": 2.3299319727891157,
      "grad_norm": 0.3570914566516876,
      "learning_rate": 0.000316,
      "loss": 0.3834,
      "step": 685
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.47377103567123413,
      "learning_rate": 0.000315,
      "loss": 0.6519,
      "step": 686
    },
    {
      "epoch": 2.336734693877551,
      "grad_norm": 0.3967217206954956,
      "learning_rate": 0.000314,
      "loss": 0.5426,
      "step": 687
    },
    {
      "epoch": 2.3401360544217686,
      "grad_norm": 0.633086621761322,
      "learning_rate": 0.000313,
      "loss": 0.6846,
      "step": 688
    },
    {
      "epoch": 2.3435374149659864,
      "grad_norm": 0.43270978331565857,
      "learning_rate": 0.000312,
      "loss": 0.471,
      "step": 689
    },
    {
      "epoch": 2.3469387755102042,
      "grad_norm": 0.48686516284942627,
      "learning_rate": 0.000311,
      "loss": 0.6813,
      "step": 690
    },
    {
      "epoch": 2.3503401360544216,
      "grad_norm": 0.8393629789352417,
      "learning_rate": 0.00031,
      "loss": 0.5655,
      "step": 691
    },
    {
      "epoch": 2.3537414965986394,
      "grad_norm": 0.38937118649482727,
      "learning_rate": 0.00030900000000000003,
      "loss": 0.5192,
      "step": 692
    },
    {
      "epoch": 2.357142857142857,
      "grad_norm": 0.4002501666545868,
      "learning_rate": 0.000308,
      "loss": 0.5712,
      "step": 693
    },
    {
      "epoch": 2.360544217687075,
      "grad_norm": 0.39577528834342957,
      "learning_rate": 0.000307,
      "loss": 0.4975,
      "step": 694
    },
    {
      "epoch": 2.3639455782312924,
      "grad_norm": 0.4102546274662018,
      "learning_rate": 0.000306,
      "loss": 0.5589,
      "step": 695
    },
    {
      "epoch": 2.36734693877551,
      "grad_norm": 0.44054898619651794,
      "learning_rate": 0.000305,
      "loss": 0.5923,
      "step": 696
    },
    {
      "epoch": 2.370748299319728,
      "grad_norm": 0.4283811151981354,
      "learning_rate": 0.000304,
      "loss": 0.6614,
      "step": 697
    },
    {
      "epoch": 2.3741496598639458,
      "grad_norm": 0.3753543496131897,
      "learning_rate": 0.000303,
      "loss": 0.4787,
      "step": 698
    },
    {
      "epoch": 2.377551020408163,
      "grad_norm": 0.35979586839675903,
      "learning_rate": 0.000302,
      "loss": 0.4624,
      "step": 699
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.41616126894950867,
      "learning_rate": 0.000301,
      "loss": 0.5697,
      "step": 700
    },
    {
      "epoch": 2.3843537414965987,
      "grad_norm": 0.5210995078086853,
      "learning_rate": 0.0003,
      "loss": 0.7486,
      "step": 701
    },
    {
      "epoch": 2.387755102040816,
      "grad_norm": 0.3626402020454407,
      "learning_rate": 0.000299,
      "loss": 0.5177,
      "step": 702
    },
    {
      "epoch": 2.391156462585034,
      "grad_norm": 0.38475972414016724,
      "learning_rate": 0.000298,
      "loss": 0.5297,
      "step": 703
    },
    {
      "epoch": 2.3945578231292517,
      "grad_norm": 0.44089916348457336,
      "learning_rate": 0.000297,
      "loss": 0.6117,
      "step": 704
    },
    {
      "epoch": 2.3979591836734695,
      "grad_norm": 0.36218389868736267,
      "learning_rate": 0.000296,
      "loss": 0.4814,
      "step": 705
    },
    {
      "epoch": 2.4013605442176873,
      "grad_norm": 0.3598928451538086,
      "learning_rate": 0.000295,
      "loss": 0.4795,
      "step": 706
    },
    {
      "epoch": 2.4047619047619047,
      "grad_norm": 0.42712584137916565,
      "learning_rate": 0.000294,
      "loss": 0.6001,
      "step": 707
    },
    {
      "epoch": 2.4081632653061225,
      "grad_norm": 0.5962764024734497,
      "learning_rate": 0.00029299999999999997,
      "loss": 0.7093,
      "step": 708
    },
    {
      "epoch": 2.4115646258503403,
      "grad_norm": 0.3708910048007965,
      "learning_rate": 0.000292,
      "loss": 0.4717,
      "step": 709
    },
    {
      "epoch": 2.4149659863945576,
      "grad_norm": 0.38641801476478577,
      "learning_rate": 0.00029099999999999997,
      "loss": 0.4964,
      "step": 710
    },
    {
      "epoch": 2.4183673469387754,
      "grad_norm": 0.355581134557724,
      "learning_rate": 0.00029,
      "loss": 0.5132,
      "step": 711
    },
    {
      "epoch": 2.421768707482993,
      "grad_norm": 0.39337387681007385,
      "learning_rate": 0.000289,
      "loss": 0.65,
      "step": 712
    },
    {
      "epoch": 2.425170068027211,
      "grad_norm": 0.6317988634109497,
      "learning_rate": 0.000288,
      "loss": 0.7247,
      "step": 713
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.39516690373420715,
      "learning_rate": 0.000287,
      "loss": 0.5847,
      "step": 714
    },
    {
      "epoch": 2.431972789115646,
      "grad_norm": 0.3767973482608795,
      "learning_rate": 0.00028599999999999996,
      "loss": 0.498,
      "step": 715
    },
    {
      "epoch": 2.435374149659864,
      "grad_norm": 0.7261524796485901,
      "learning_rate": 0.000285,
      "loss": 0.7881,
      "step": 716
    },
    {
      "epoch": 2.438775510204082,
      "grad_norm": 0.4082394242286682,
      "learning_rate": 0.00028399999999999996,
      "loss": 0.645,
      "step": 717
    },
    {
      "epoch": 2.442176870748299,
      "grad_norm": 0.7002867460250854,
      "learning_rate": 0.000283,
      "loss": 0.8081,
      "step": 718
    },
    {
      "epoch": 2.445578231292517,
      "grad_norm": 0.3856392502784729,
      "learning_rate": 0.00028199999999999997,
      "loss": 0.5064,
      "step": 719
    },
    {
      "epoch": 2.4489795918367347,
      "grad_norm": 0.38951045274734497,
      "learning_rate": 0.00028100000000000005,
      "loss": 0.4562,
      "step": 720
    },
    {
      "epoch": 2.4523809523809526,
      "grad_norm": 0.39976632595062256,
      "learning_rate": 0.00028000000000000003,
      "loss": 0.5589,
      "step": 721
    },
    {
      "epoch": 2.45578231292517,
      "grad_norm": 0.7458876967430115,
      "learning_rate": 0.000279,
      "loss": 0.7,
      "step": 722
    },
    {
      "epoch": 2.4591836734693877,
      "grad_norm": 0.34082821011543274,
      "learning_rate": 0.00027800000000000004,
      "loss": 0.4371,
      "step": 723
    },
    {
      "epoch": 2.4625850340136055,
      "grad_norm": 0.34948965907096863,
      "learning_rate": 0.000277,
      "loss": 0.4171,
      "step": 724
    },
    {
      "epoch": 2.4659863945578233,
      "grad_norm": 0.356751412153244,
      "learning_rate": 0.00027600000000000004,
      "loss": 0.5278,
      "step": 725
    },
    {
      "epoch": 2.4693877551020407,
      "grad_norm": 0.40971869230270386,
      "learning_rate": 0.000275,
      "loss": 0.5239,
      "step": 726
    },
    {
      "epoch": 2.4727891156462585,
      "grad_norm": 0.4064975380897522,
      "learning_rate": 0.00027400000000000005,
      "loss": 0.6287,
      "step": 727
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.42178162932395935,
      "learning_rate": 0.000273,
      "loss": 0.5848,
      "step": 728
    },
    {
      "epoch": 2.479591836734694,
      "grad_norm": 0.7783451676368713,
      "learning_rate": 0.00027200000000000005,
      "loss": 0.837,
      "step": 729
    },
    {
      "epoch": 2.4829931972789114,
      "grad_norm": 1.0055718421936035,
      "learning_rate": 0.00027100000000000003,
      "loss": 0.7376,
      "step": 730
    },
    {
      "epoch": 2.4863945578231292,
      "grad_norm": 0.6609615087509155,
      "learning_rate": 0.00027,
      "loss": 0.7168,
      "step": 731
    },
    {
      "epoch": 2.489795918367347,
      "grad_norm": 0.35979384183883667,
      "learning_rate": 0.00026900000000000003,
      "loss": 0.5044,
      "step": 732
    },
    {
      "epoch": 2.4931972789115644,
      "grad_norm": 0.4453805983066559,
      "learning_rate": 0.000268,
      "loss": 0.639,
      "step": 733
    },
    {
      "epoch": 2.496598639455782,
      "grad_norm": 0.3519146740436554,
      "learning_rate": 0.00026700000000000004,
      "loss": 0.5854,
      "step": 734
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.4899059236049652,
      "learning_rate": 0.000266,
      "loss": 0.6442,
      "step": 735
    },
    {
      "epoch": 2.503401360544218,
      "grad_norm": 0.42709940671920776,
      "learning_rate": 0.00026500000000000004,
      "loss": 0.6145,
      "step": 736
    },
    {
      "epoch": 2.5068027210884356,
      "grad_norm": 0.7159500122070312,
      "learning_rate": 0.000264,
      "loss": 0.5727,
      "step": 737
    },
    {
      "epoch": 2.510204081632653,
      "grad_norm": 0.3868669867515564,
      "learning_rate": 0.000263,
      "loss": 0.5986,
      "step": 738
    },
    {
      "epoch": 2.5136054421768708,
      "grad_norm": 0.4182274341583252,
      "learning_rate": 0.000262,
      "loss": 0.5172,
      "step": 739
    },
    {
      "epoch": 2.5170068027210886,
      "grad_norm": 0.41902828216552734,
      "learning_rate": 0.000261,
      "loss": 0.5022,
      "step": 740
    },
    {
      "epoch": 2.520408163265306,
      "grad_norm": 0.439058393239975,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.5094,
      "step": 741
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 0.3581922948360443,
      "learning_rate": 0.000259,
      "loss": 0.5088,
      "step": 742
    },
    {
      "epoch": 2.5272108843537415,
      "grad_norm": 0.3897128403186798,
      "learning_rate": 0.00025800000000000004,
      "loss": 0.4405,
      "step": 743
    },
    {
      "epoch": 2.5306122448979593,
      "grad_norm": 0.4149033725261688,
      "learning_rate": 0.000257,
      "loss": 0.5103,
      "step": 744
    },
    {
      "epoch": 2.534013605442177,
      "grad_norm": 0.4428374171257019,
      "learning_rate": 0.000256,
      "loss": 0.5047,
      "step": 745
    },
    {
      "epoch": 2.5374149659863945,
      "grad_norm": 0.41483360528945923,
      "learning_rate": 0.000255,
      "loss": 0.5343,
      "step": 746
    },
    {
      "epoch": 2.5408163265306123,
      "grad_norm": 0.37219154834747314,
      "learning_rate": 0.000254,
      "loss": 0.479,
      "step": 747
    },
    {
      "epoch": 2.54421768707483,
      "grad_norm": 0.3928647041320801,
      "learning_rate": 0.000253,
      "loss": 0.4523,
      "step": 748
    },
    {
      "epoch": 2.5476190476190474,
      "grad_norm": 0.3553835153579712,
      "learning_rate": 0.000252,
      "loss": 0.4363,
      "step": 749
    },
    {
      "epoch": 2.5510204081632653,
      "grad_norm": 0.6131541132926941,
      "learning_rate": 0.00025100000000000003,
      "loss": 0.7904,
      "step": 750
    },
    {
      "epoch": 2.554421768707483,
      "grad_norm": 0.38916683197021484,
      "learning_rate": 0.00025,
      "loss": 0.5184,
      "step": 751
    },
    {
      "epoch": 2.557823129251701,
      "grad_norm": 0.41995131969451904,
      "learning_rate": 0.000249,
      "loss": 0.5464,
      "step": 752
    },
    {
      "epoch": 2.561224489795918,
      "grad_norm": 0.38813653588294983,
      "learning_rate": 0.000248,
      "loss": 0.4907,
      "step": 753
    },
    {
      "epoch": 2.564625850340136,
      "grad_norm": 0.4682192802429199,
      "learning_rate": 0.000247,
      "loss": 0.6134,
      "step": 754
    },
    {
      "epoch": 2.568027210884354,
      "grad_norm": 0.5601564049720764,
      "learning_rate": 0.000246,
      "loss": 0.7406,
      "step": 755
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.42164507508277893,
      "learning_rate": 0.000245,
      "loss": 0.6207,
      "step": 756
    },
    {
      "epoch": 2.574829931972789,
      "grad_norm": 0.5771769285202026,
      "learning_rate": 0.000244,
      "loss": 0.7714,
      "step": 757
    },
    {
      "epoch": 2.578231292517007,
      "grad_norm": 0.37895509600639343,
      "learning_rate": 0.000243,
      "loss": 0.5182,
      "step": 758
    },
    {
      "epoch": 2.5816326530612246,
      "grad_norm": 0.40190061926841736,
      "learning_rate": 0.000242,
      "loss": 0.4892,
      "step": 759
    },
    {
      "epoch": 2.5850340136054424,
      "grad_norm": 0.3745453953742981,
      "learning_rate": 0.000241,
      "loss": 0.4538,
      "step": 760
    },
    {
      "epoch": 2.5884353741496597,
      "grad_norm": 0.4254196882247925,
      "learning_rate": 0.00024,
      "loss": 0.654,
      "step": 761
    },
    {
      "epoch": 2.5918367346938775,
      "grad_norm": 0.3894517719745636,
      "learning_rate": 0.00023899999999999998,
      "loss": 0.5081,
      "step": 762
    },
    {
      "epoch": 2.5952380952380953,
      "grad_norm": 0.7760348916053772,
      "learning_rate": 0.00023799999999999998,
      "loss": 0.6561,
      "step": 763
    },
    {
      "epoch": 2.5986394557823127,
      "grad_norm": 0.3532026708126068,
      "learning_rate": 0.000237,
      "loss": 0.4303,
      "step": 764
    },
    {
      "epoch": 2.6020408163265305,
      "grad_norm": 0.41843658685684204,
      "learning_rate": 0.000236,
      "loss": 0.4839,
      "step": 765
    },
    {
      "epoch": 2.6054421768707483,
      "grad_norm": 0.6760868430137634,
      "learning_rate": 0.000235,
      "loss": 0.6859,
      "step": 766
    },
    {
      "epoch": 2.608843537414966,
      "grad_norm": 0.3464645445346832,
      "learning_rate": 0.00023400000000000002,
      "loss": 0.4452,
      "step": 767
    },
    {
      "epoch": 2.612244897959184,
      "grad_norm": 0.3842136561870575,
      "learning_rate": 0.00023300000000000003,
      "loss": 0.4316,
      "step": 768
    },
    {
      "epoch": 2.6156462585034013,
      "grad_norm": 0.45375651121139526,
      "learning_rate": 0.00023200000000000003,
      "loss": 0.4731,
      "step": 769
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 0.7944602370262146,
      "learning_rate": 0.000231,
      "loss": 0.5734,
      "step": 770
    },
    {
      "epoch": 2.622448979591837,
      "grad_norm": 0.588817834854126,
      "learning_rate": 0.00023,
      "loss": 0.7517,
      "step": 771
    },
    {
      "epoch": 2.6258503401360542,
      "grad_norm": 0.37380799651145935,
      "learning_rate": 0.000229,
      "loss": 0.5095,
      "step": 772
    },
    {
      "epoch": 2.629251700680272,
      "grad_norm": 0.6055300235748291,
      "learning_rate": 0.000228,
      "loss": 0.6476,
      "step": 773
    },
    {
      "epoch": 2.63265306122449,
      "grad_norm": 0.33363136649131775,
      "learning_rate": 0.00022700000000000002,
      "loss": 0.3946,
      "step": 774
    },
    {
      "epoch": 2.6360544217687076,
      "grad_norm": 0.4196399450302124,
      "learning_rate": 0.00022600000000000002,
      "loss": 0.5786,
      "step": 775
    },
    {
      "epoch": 2.6394557823129254,
      "grad_norm": 0.6389960050582886,
      "learning_rate": 0.00022500000000000002,
      "loss": 0.6309,
      "step": 776
    },
    {
      "epoch": 2.642857142857143,
      "grad_norm": 0.40182608366012573,
      "learning_rate": 0.000224,
      "loss": 0.5289,
      "step": 777
    },
    {
      "epoch": 2.6462585034013606,
      "grad_norm": 0.36187979578971863,
      "learning_rate": 0.000223,
      "loss": 0.4636,
      "step": 778
    },
    {
      "epoch": 2.6496598639455784,
      "grad_norm": 0.3720533847808838,
      "learning_rate": 0.000222,
      "loss": 0.5155,
      "step": 779
    },
    {
      "epoch": 2.6530612244897958,
      "grad_norm": 0.6671199202537537,
      "learning_rate": 0.000221,
      "loss": 0.5844,
      "step": 780
    },
    {
      "epoch": 2.6564625850340136,
      "grad_norm": 0.4167207181453705,
      "learning_rate": 0.00022,
      "loss": 0.5195,
      "step": 781
    },
    {
      "epoch": 2.6598639455782314,
      "grad_norm": 0.6649337410926819,
      "learning_rate": 0.000219,
      "loss": 0.6474,
      "step": 782
    },
    {
      "epoch": 2.663265306122449,
      "grad_norm": 0.3983607590198517,
      "learning_rate": 0.000218,
      "loss": 0.4681,
      "step": 783
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.38711225986480713,
      "learning_rate": 0.00021700000000000002,
      "loss": 0.4531,
      "step": 784
    },
    {
      "epoch": 2.6700680272108843,
      "grad_norm": 0.40419772267341614,
      "learning_rate": 0.000216,
      "loss": 0.4524,
      "step": 785
    },
    {
      "epoch": 2.673469387755102,
      "grad_norm": 0.44477587938308716,
      "learning_rate": 0.000215,
      "loss": 0.6926,
      "step": 786
    },
    {
      "epoch": 2.6768707482993195,
      "grad_norm": 0.4208657145500183,
      "learning_rate": 0.000214,
      "loss": 0.5696,
      "step": 787
    },
    {
      "epoch": 2.6802721088435373,
      "grad_norm": 1.1864272356033325,
      "learning_rate": 0.000213,
      "loss": 0.4916,
      "step": 788
    },
    {
      "epoch": 2.683673469387755,
      "grad_norm": 0.39152470231056213,
      "learning_rate": 0.000212,
      "loss": 0.4771,
      "step": 789
    },
    {
      "epoch": 2.687074829931973,
      "grad_norm": 0.3816203474998474,
      "learning_rate": 0.000211,
      "loss": 0.509,
      "step": 790
    },
    {
      "epoch": 2.6904761904761907,
      "grad_norm": 0.4256383776664734,
      "learning_rate": 0.00021,
      "loss": 0.4795,
      "step": 791
    },
    {
      "epoch": 2.693877551020408,
      "grad_norm": 0.42477110028266907,
      "learning_rate": 0.00020899999999999998,
      "loss": 0.4502,
      "step": 792
    },
    {
      "epoch": 2.697278911564626,
      "grad_norm": 0.43037715554237366,
      "learning_rate": 0.000208,
      "loss": 0.5094,
      "step": 793
    },
    {
      "epoch": 2.7006802721088436,
      "grad_norm": 0.42870965600013733,
      "learning_rate": 0.000207,
      "loss": 0.5022,
      "step": 794
    },
    {
      "epoch": 2.704081632653061,
      "grad_norm": 0.3401421010494232,
      "learning_rate": 0.000206,
      "loss": 0.4175,
      "step": 795
    },
    {
      "epoch": 2.707482993197279,
      "grad_norm": 0.362373411655426,
      "learning_rate": 0.000205,
      "loss": 0.4453,
      "step": 796
    },
    {
      "epoch": 2.7108843537414966,
      "grad_norm": 0.4534027576446533,
      "learning_rate": 0.000204,
      "loss": 0.4627,
      "step": 797
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 0.39704325795173645,
      "learning_rate": 0.00020300000000000003,
      "loss": 0.4624,
      "step": 798
    },
    {
      "epoch": 2.717687074829932,
      "grad_norm": 0.46737492084503174,
      "learning_rate": 0.000202,
      "loss": 0.6265,
      "step": 799
    },
    {
      "epoch": 2.7210884353741496,
      "grad_norm": 0.6442068219184875,
      "learning_rate": 0.000201,
      "loss": 0.6576,
      "step": 800
    },
    {
      "epoch": 2.7244897959183674,
      "grad_norm": 0.5906636714935303,
      "learning_rate": 0.0002,
      "loss": 0.6909,
      "step": 801
    },
    {
      "epoch": 2.727891156462585,
      "grad_norm": 0.86101895570755,
      "learning_rate": 0.000199,
      "loss": 0.7727,
      "step": 802
    },
    {
      "epoch": 2.7312925170068025,
      "grad_norm": 0.3296317160129547,
      "learning_rate": 0.00019800000000000002,
      "loss": 0.4479,
      "step": 803
    },
    {
      "epoch": 2.7346938775510203,
      "grad_norm": 0.4255754053592682,
      "learning_rate": 0.00019700000000000002,
      "loss": 0.5717,
      "step": 804
    },
    {
      "epoch": 2.738095238095238,
      "grad_norm": 0.4456673562526703,
      "learning_rate": 0.00019600000000000002,
      "loss": 0.6792,
      "step": 805
    },
    {
      "epoch": 2.741496598639456,
      "grad_norm": 0.3801611065864563,
      "learning_rate": 0.00019500000000000002,
      "loss": 0.4226,
      "step": 806
    },
    {
      "epoch": 2.7448979591836737,
      "grad_norm": 0.38878414034843445,
      "learning_rate": 0.000194,
      "loss": 0.4991,
      "step": 807
    },
    {
      "epoch": 2.748299319727891,
      "grad_norm": 0.6260315179824829,
      "learning_rate": 0.000193,
      "loss": 0.6624,
      "step": 808
    },
    {
      "epoch": 2.751700680272109,
      "grad_norm": 0.4356565475463867,
      "learning_rate": 0.000192,
      "loss": 0.5965,
      "step": 809
    },
    {
      "epoch": 2.7551020408163263,
      "grad_norm": 0.4060423672199249,
      "learning_rate": 0.000191,
      "loss": 0.6202,
      "step": 810
    },
    {
      "epoch": 2.758503401360544,
      "grad_norm": 0.40590646862983704,
      "learning_rate": 0.00019,
      "loss": 0.6419,
      "step": 811
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.3971695601940155,
      "learning_rate": 0.000189,
      "loss": 0.4628,
      "step": 812
    },
    {
      "epoch": 2.7653061224489797,
      "grad_norm": 0.46516087651252747,
      "learning_rate": 0.00018800000000000002,
      "loss": 0.5701,
      "step": 813
    },
    {
      "epoch": 2.7687074829931975,
      "grad_norm": 0.3396034836769104,
      "learning_rate": 0.000187,
      "loss": 0.3534,
      "step": 814
    },
    {
      "epoch": 2.772108843537415,
      "grad_norm": 0.39315900206565857,
      "learning_rate": 0.000186,
      "loss": 0.5085,
      "step": 815
    },
    {
      "epoch": 2.7755102040816326,
      "grad_norm": 0.3952585458755493,
      "learning_rate": 0.000185,
      "loss": 0.5357,
      "step": 816
    },
    {
      "epoch": 2.7789115646258504,
      "grad_norm": 0.44497475028038025,
      "learning_rate": 0.000184,
      "loss": 0.7111,
      "step": 817
    },
    {
      "epoch": 2.782312925170068,
      "grad_norm": 0.40308481454849243,
      "learning_rate": 0.000183,
      "loss": 0.5081,
      "step": 818
    },
    {
      "epoch": 2.7857142857142856,
      "grad_norm": 0.42781901359558105,
      "learning_rate": 0.000182,
      "loss": 0.5897,
      "step": 819
    },
    {
      "epoch": 2.7891156462585034,
      "grad_norm": 0.7099624276161194,
      "learning_rate": 0.000181,
      "loss": 0.7992,
      "step": 820
    },
    {
      "epoch": 2.792517006802721,
      "grad_norm": 0.6484186053276062,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.6314,
      "step": 821
    },
    {
      "epoch": 2.795918367346939,
      "grad_norm": 0.6199230551719666,
      "learning_rate": 0.000179,
      "loss": 0.7173,
      "step": 822
    },
    {
      "epoch": 2.7993197278911564,
      "grad_norm": 0.4174414277076721,
      "learning_rate": 0.000178,
      "loss": 0.45,
      "step": 823
    },
    {
      "epoch": 2.802721088435374,
      "grad_norm": 0.37301620841026306,
      "learning_rate": 0.000177,
      "loss": 0.4547,
      "step": 824
    },
    {
      "epoch": 2.806122448979592,
      "grad_norm": 0.6564555168151855,
      "learning_rate": 0.000176,
      "loss": 0.7854,
      "step": 825
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 0.36428701877593994,
      "learning_rate": 0.000175,
      "loss": 0.4824,
      "step": 826
    },
    {
      "epoch": 2.812925170068027,
      "grad_norm": 0.7837996482849121,
      "learning_rate": 0.000174,
      "loss": 0.7998,
      "step": 827
    },
    {
      "epoch": 2.816326530612245,
      "grad_norm": 0.506959855556488,
      "learning_rate": 0.000173,
      "loss": 0.7044,
      "step": 828
    },
    {
      "epoch": 2.8197278911564627,
      "grad_norm": 0.3958054482936859,
      "learning_rate": 0.00017199999999999998,
      "loss": 0.4956,
      "step": 829
    },
    {
      "epoch": 2.8231292517006805,
      "grad_norm": 0.3448379337787628,
      "learning_rate": 0.000171,
      "loss": 0.3829,
      "step": 830
    },
    {
      "epoch": 2.826530612244898,
      "grad_norm": 0.38803380727767944,
      "learning_rate": 0.00017,
      "loss": 0.4642,
      "step": 831
    },
    {
      "epoch": 2.8299319727891157,
      "grad_norm": 0.4326297342777252,
      "learning_rate": 0.00016900000000000002,
      "loss": 0.4455,
      "step": 832
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.39125362038612366,
      "learning_rate": 0.00016800000000000002,
      "loss": 0.4823,
      "step": 833
    },
    {
      "epoch": 2.836734693877551,
      "grad_norm": 0.42994725704193115,
      "learning_rate": 0.00016700000000000002,
      "loss": 0.5432,
      "step": 834
    },
    {
      "epoch": 2.8401360544217686,
      "grad_norm": 0.6909547448158264,
      "learning_rate": 0.00016600000000000002,
      "loss": 0.6674,
      "step": 835
    },
    {
      "epoch": 2.8435374149659864,
      "grad_norm": 0.40791866183280945,
      "learning_rate": 0.000165,
      "loss": 0.5238,
      "step": 836
    },
    {
      "epoch": 2.8469387755102042,
      "grad_norm": 0.43496474623680115,
      "learning_rate": 0.000164,
      "loss": 0.5892,
      "step": 837
    },
    {
      "epoch": 2.8503401360544216,
      "grad_norm": 0.37177684903144836,
      "learning_rate": 0.000163,
      "loss": 0.4857,
      "step": 838
    },
    {
      "epoch": 2.8537414965986394,
      "grad_norm": 0.38169583678245544,
      "learning_rate": 0.000162,
      "loss": 0.4696,
      "step": 839
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.4439150094985962,
      "learning_rate": 0.000161,
      "loss": 0.5911,
      "step": 840
    },
    {
      "epoch": 2.8605442176870746,
      "grad_norm": 0.6753532290458679,
      "learning_rate": 0.00016,
      "loss": 0.7021,
      "step": 841
    },
    {
      "epoch": 2.8639455782312924,
      "grad_norm": 0.3719159662723541,
      "learning_rate": 0.00015900000000000002,
      "loss": 0.4184,
      "step": 842
    },
    {
      "epoch": 2.86734693877551,
      "grad_norm": 0.4130648374557495,
      "learning_rate": 0.000158,
      "loss": 0.5064,
      "step": 843
    },
    {
      "epoch": 2.870748299319728,
      "grad_norm": 0.4762708246707916,
      "learning_rate": 0.000157,
      "loss": 0.5917,
      "step": 844
    },
    {
      "epoch": 2.8741496598639458,
      "grad_norm": 0.4349571168422699,
      "learning_rate": 0.000156,
      "loss": 0.5135,
      "step": 845
    },
    {
      "epoch": 2.877551020408163,
      "grad_norm": 0.43190622329711914,
      "learning_rate": 0.000155,
      "loss": 0.5917,
      "step": 846
    },
    {
      "epoch": 2.880952380952381,
      "grad_norm": 0.3870050311088562,
      "learning_rate": 0.000154,
      "loss": 0.5514,
      "step": 847
    },
    {
      "epoch": 2.8843537414965987,
      "grad_norm": 0.34296703338623047,
      "learning_rate": 0.000153,
      "loss": 0.3743,
      "step": 848
    },
    {
      "epoch": 2.887755102040816,
      "grad_norm": 0.36620640754699707,
      "learning_rate": 0.000152,
      "loss": 0.3992,
      "step": 849
    },
    {
      "epoch": 2.891156462585034,
      "grad_norm": 0.4622700810432434,
      "learning_rate": 0.000151,
      "loss": 0.69,
      "step": 850
    },
    {
      "epoch": 2.8945578231292517,
      "grad_norm": 0.38513505458831787,
      "learning_rate": 0.00015,
      "loss": 0.5586,
      "step": 851
    },
    {
      "epoch": 2.8979591836734695,
      "grad_norm": 1.6284005641937256,
      "learning_rate": 0.000149,
      "loss": 0.5691,
      "step": 852
    },
    {
      "epoch": 2.9013605442176873,
      "grad_norm": 0.40658655762672424,
      "learning_rate": 0.000148,
      "loss": 0.6292,
      "step": 853
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 0.6298061609268188,
      "learning_rate": 0.000147,
      "loss": 0.8069,
      "step": 854
    },
    {
      "epoch": 2.9081632653061225,
      "grad_norm": 0.39562949538230896,
      "learning_rate": 0.000146,
      "loss": 0.4817,
      "step": 855
    },
    {
      "epoch": 2.9115646258503403,
      "grad_norm": 0.4038800001144409,
      "learning_rate": 0.000145,
      "loss": 0.4534,
      "step": 856
    },
    {
      "epoch": 2.9149659863945576,
      "grad_norm": 0.37314268946647644,
      "learning_rate": 0.000144,
      "loss": 0.4762,
      "step": 857
    },
    {
      "epoch": 2.9183673469387754,
      "grad_norm": 0.38348114490509033,
      "learning_rate": 0.00014299999999999998,
      "loss": 0.5064,
      "step": 858
    },
    {
      "epoch": 2.921768707482993,
      "grad_norm": 0.353762686252594,
      "learning_rate": 0.00014199999999999998,
      "loss": 0.4503,
      "step": 859
    },
    {
      "epoch": 2.925170068027211,
      "grad_norm": 0.34472793340682983,
      "learning_rate": 0.00014099999999999998,
      "loss": 0.4344,
      "step": 860
    },
    {
      "epoch": 2.928571428571429,
      "grad_norm": 0.4246794879436493,
      "learning_rate": 0.00014000000000000001,
      "loss": 0.6203,
      "step": 861
    },
    {
      "epoch": 2.931972789115646,
      "grad_norm": 0.4309011697769165,
      "learning_rate": 0.00013900000000000002,
      "loss": 0.5839,
      "step": 862
    },
    {
      "epoch": 2.935374149659864,
      "grad_norm": 0.6623035073280334,
      "learning_rate": 0.00013800000000000002,
      "loss": 0.684,
      "step": 863
    },
    {
      "epoch": 2.938775510204082,
      "grad_norm": 0.34745919704437256,
      "learning_rate": 0.00013700000000000002,
      "loss": 0.4509,
      "step": 864
    },
    {
      "epoch": 2.942176870748299,
      "grad_norm": 0.6245547533035278,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.5674,
      "step": 865
    },
    {
      "epoch": 2.945578231292517,
      "grad_norm": 0.563115656375885,
      "learning_rate": 0.000135,
      "loss": 0.7971,
      "step": 866
    },
    {
      "epoch": 2.9489795918367347,
      "grad_norm": 0.37184372544288635,
      "learning_rate": 0.000134,
      "loss": 0.4502,
      "step": 867
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.5580228567123413,
      "learning_rate": 0.000133,
      "loss": 0.6529,
      "step": 868
    },
    {
      "epoch": 2.95578231292517,
      "grad_norm": 0.362395703792572,
      "learning_rate": 0.000132,
      "loss": 0.4593,
      "step": 869
    },
    {
      "epoch": 2.9591836734693877,
      "grad_norm": 0.3606446385383606,
      "learning_rate": 0.000131,
      "loss": 0.5149,
      "step": 870
    },
    {
      "epoch": 2.9625850340136055,
      "grad_norm": 2.2122514247894287,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.519,
      "step": 871
    },
    {
      "epoch": 2.965986394557823,
      "grad_norm": 0.4102756381034851,
      "learning_rate": 0.00012900000000000002,
      "loss": 0.5612,
      "step": 872
    },
    {
      "epoch": 2.9693877551020407,
      "grad_norm": 0.36688607931137085,
      "learning_rate": 0.000128,
      "loss": 0.413,
      "step": 873
    },
    {
      "epoch": 2.9727891156462585,
      "grad_norm": 0.35682427883148193,
      "learning_rate": 0.000127,
      "loss": 0.465,
      "step": 874
    },
    {
      "epoch": 2.9761904761904763,
      "grad_norm": 0.5997934341430664,
      "learning_rate": 0.000126,
      "loss": 0.7531,
      "step": 875
    },
    {
      "epoch": 2.979591836734694,
      "grad_norm": 0.43683210015296936,
      "learning_rate": 0.000125,
      "loss": 0.6226,
      "step": 876
    },
    {
      "epoch": 2.9829931972789114,
      "grad_norm": 0.3648676872253418,
      "learning_rate": 0.000124,
      "loss": 0.4302,
      "step": 877
    },
    {
      "epoch": 2.9863945578231292,
      "grad_norm": 0.37567198276519775,
      "learning_rate": 0.000123,
      "loss": 0.428,
      "step": 878
    },
    {
      "epoch": 2.989795918367347,
      "grad_norm": 0.37323054671287537,
      "learning_rate": 0.000122,
      "loss": 0.484,
      "step": 879
    },
    {
      "epoch": 2.9931972789115644,
      "grad_norm": 0.4976719319820404,
      "learning_rate": 0.000121,
      "loss": 0.6145,
      "step": 880
    },
    {
      "epoch": 2.996598639455782,
      "grad_norm": 0.38233768939971924,
      "learning_rate": 0.00012,
      "loss": 0.535,
      "step": 881
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.4368060231208801,
      "learning_rate": 0.00011899999999999999,
      "loss": 0.6298,
      "step": 882
    },
    {
      "epoch": 3.003401360544218,
      "grad_norm": 0.369798481464386,
      "learning_rate": 0.000118,
      "loss": 0.4486,
      "step": 883
    },
    {
      "epoch": 3.006802721088435,
      "grad_norm": 0.4717889130115509,
      "learning_rate": 0.00011700000000000001,
      "loss": 0.6865,
      "step": 884
    },
    {
      "epoch": 3.010204081632653,
      "grad_norm": 0.42597252130508423,
      "learning_rate": 0.00011600000000000001,
      "loss": 0.6606,
      "step": 885
    },
    {
      "epoch": 3.0136054421768708,
      "grad_norm": 0.36988726258277893,
      "learning_rate": 0.000115,
      "loss": 0.4889,
      "step": 886
    },
    {
      "epoch": 3.0170068027210886,
      "grad_norm": 0.5959520936012268,
      "learning_rate": 0.000114,
      "loss": 0.6831,
      "step": 887
    },
    {
      "epoch": 3.020408163265306,
      "grad_norm": 1.5118732452392578,
      "learning_rate": 0.00011300000000000001,
      "loss": 0.4922,
      "step": 888
    },
    {
      "epoch": 3.0238095238095237,
      "grad_norm": 0.4593767523765564,
      "learning_rate": 0.000112,
      "loss": 0.5712,
      "step": 889
    },
    {
      "epoch": 3.0272108843537415,
      "grad_norm": 0.35051754117012024,
      "learning_rate": 0.000111,
      "loss": 0.4626,
      "step": 890
    },
    {
      "epoch": 3.0306122448979593,
      "grad_norm": 0.6345756649971008,
      "learning_rate": 0.00011,
      "loss": 0.6644,
      "step": 891
    },
    {
      "epoch": 3.0340136054421767,
      "grad_norm": 0.4288594126701355,
      "learning_rate": 0.000109,
      "loss": 0.5794,
      "step": 892
    },
    {
      "epoch": 3.0374149659863945,
      "grad_norm": 0.40927666425704956,
      "learning_rate": 0.000108,
      "loss": 0.506,
      "step": 893
    },
    {
      "epoch": 3.0408163265306123,
      "grad_norm": 0.41134461760520935,
      "learning_rate": 0.000107,
      "loss": 0.4402,
      "step": 894
    },
    {
      "epoch": 3.04421768707483,
      "grad_norm": 0.407203733921051,
      "learning_rate": 0.000106,
      "loss": 0.5527,
      "step": 895
    },
    {
      "epoch": 3.0476190476190474,
      "grad_norm": 0.37703076004981995,
      "learning_rate": 0.000105,
      "loss": 0.5096,
      "step": 896
    },
    {
      "epoch": 3.0510204081632653,
      "grad_norm": 0.4092012643814087,
      "learning_rate": 0.000104,
      "loss": 0.5637,
      "step": 897
    },
    {
      "epoch": 3.054421768707483,
      "grad_norm": 0.35747525095939636,
      "learning_rate": 0.000103,
      "loss": 0.4508,
      "step": 898
    },
    {
      "epoch": 3.057823129251701,
      "grad_norm": 0.41348424553871155,
      "learning_rate": 0.000102,
      "loss": 0.5066,
      "step": 899
    },
    {
      "epoch": 3.061224489795918,
      "grad_norm": 1.0263168811798096,
      "learning_rate": 0.000101,
      "loss": 0.7627,
      "step": 900
    },
    {
      "epoch": 3.064625850340136,
      "grad_norm": 0.3593388795852661,
      "learning_rate": 0.0001,
      "loss": 0.4721,
      "step": 901
    },
    {
      "epoch": 3.068027210884354,
      "grad_norm": 0.41154611110687256,
      "learning_rate": 9.900000000000001e-05,
      "loss": 0.5612,
      "step": 902
    },
    {
      "epoch": 3.0714285714285716,
      "grad_norm": 0.423309862613678,
      "learning_rate": 9.800000000000001e-05,
      "loss": 0.4769,
      "step": 903
    },
    {
      "epoch": 3.074829931972789,
      "grad_norm": 0.45288360118865967,
      "learning_rate": 9.7e-05,
      "loss": 0.5915,
      "step": 904
    },
    {
      "epoch": 3.078231292517007,
      "grad_norm": 0.6150966286659241,
      "learning_rate": 9.6e-05,
      "loss": 0.6707,
      "step": 905
    },
    {
      "epoch": 3.0816326530612246,
      "grad_norm": 0.43714773654937744,
      "learning_rate": 9.5e-05,
      "loss": 0.488,
      "step": 906
    },
    {
      "epoch": 3.0850340136054424,
      "grad_norm": 0.37792715430259705,
      "learning_rate": 9.400000000000001e-05,
      "loss": 0.5347,
      "step": 907
    },
    {
      "epoch": 3.0884353741496597,
      "grad_norm": 0.34651657938957214,
      "learning_rate": 9.3e-05,
      "loss": 0.3957,
      "step": 908
    },
    {
      "epoch": 3.0918367346938775,
      "grad_norm": 0.7030893564224243,
      "learning_rate": 9.2e-05,
      "loss": 0.8404,
      "step": 909
    },
    {
      "epoch": 3.0952380952380953,
      "grad_norm": 0.33867862820625305,
      "learning_rate": 9.1e-05,
      "loss": 0.4145,
      "step": 910
    },
    {
      "epoch": 3.0986394557823127,
      "grad_norm": 0.5940285325050354,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.5904,
      "step": 911
    },
    {
      "epoch": 3.1020408163265305,
      "grad_norm": 0.6179566979408264,
      "learning_rate": 8.9e-05,
      "loss": 0.7434,
      "step": 912
    },
    {
      "epoch": 3.1054421768707483,
      "grad_norm": 0.39269953966140747,
      "learning_rate": 8.8e-05,
      "loss": 0.4585,
      "step": 913
    },
    {
      "epoch": 3.108843537414966,
      "grad_norm": 0.7112455368041992,
      "learning_rate": 8.7e-05,
      "loss": 0.8218,
      "step": 914
    },
    {
      "epoch": 3.1122448979591835,
      "grad_norm": 0.4130833148956299,
      "learning_rate": 8.599999999999999e-05,
      "loss": 0.5668,
      "step": 915
    },
    {
      "epoch": 3.1156462585034013,
      "grad_norm": 0.720389187335968,
      "learning_rate": 8.5e-05,
      "loss": 0.7699,
      "step": 916
    },
    {
      "epoch": 3.119047619047619,
      "grad_norm": 0.4704832434654236,
      "learning_rate": 8.400000000000001e-05,
      "loss": 0.5989,
      "step": 917
    },
    {
      "epoch": 3.122448979591837,
      "grad_norm": 0.4455391466617584,
      "learning_rate": 8.300000000000001e-05,
      "loss": 0.6215,
      "step": 918
    },
    {
      "epoch": 3.1258503401360542,
      "grad_norm": 0.3315236270427704,
      "learning_rate": 8.2e-05,
      "loss": 0.374,
      "step": 919
    },
    {
      "epoch": 3.129251700680272,
      "grad_norm": 0.3946831226348877,
      "learning_rate": 8.1e-05,
      "loss": 0.5241,
      "step": 920
    },
    {
      "epoch": 3.13265306122449,
      "grad_norm": 0.39276519417762756,
      "learning_rate": 8e-05,
      "loss": 0.5304,
      "step": 921
    },
    {
      "epoch": 3.1360544217687076,
      "grad_norm": 0.357221782207489,
      "learning_rate": 7.9e-05,
      "loss": 0.3833,
      "step": 922
    },
    {
      "epoch": 3.139455782312925,
      "grad_norm": 0.3521365225315094,
      "learning_rate": 7.8e-05,
      "loss": 0.4587,
      "step": 923
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 0.6960167288780212,
      "learning_rate": 7.7e-05,
      "loss": 0.7647,
      "step": 924
    },
    {
      "epoch": 3.1462585034013606,
      "grad_norm": 0.4060671031475067,
      "learning_rate": 7.6e-05,
      "loss": 0.4678,
      "step": 925
    },
    {
      "epoch": 3.1496598639455784,
      "grad_norm": 0.4451645016670227,
      "learning_rate": 7.5e-05,
      "loss": 0.5589,
      "step": 926
    },
    {
      "epoch": 3.1530612244897958,
      "grad_norm": 0.3581123650074005,
      "learning_rate": 7.4e-05,
      "loss": 0.4293,
      "step": 927
    },
    {
      "epoch": 3.1564625850340136,
      "grad_norm": 0.3939483165740967,
      "learning_rate": 7.3e-05,
      "loss": 0.4839,
      "step": 928
    },
    {
      "epoch": 3.1598639455782314,
      "grad_norm": 0.43617454171180725,
      "learning_rate": 7.2e-05,
      "loss": 0.6688,
      "step": 929
    },
    {
      "epoch": 3.163265306122449,
      "grad_norm": 0.3915238380432129,
      "learning_rate": 7.099999999999999e-05,
      "loss": 0.5692,
      "step": 930
    },
    {
      "epoch": 3.1666666666666665,
      "grad_norm": 0.3800797462463379,
      "learning_rate": 7.000000000000001e-05,
      "loss": 0.472,
      "step": 931
    },
    {
      "epoch": 3.1700680272108843,
      "grad_norm": 0.4260604977607727,
      "learning_rate": 6.900000000000001e-05,
      "loss": 0.532,
      "step": 932
    },
    {
      "epoch": 3.173469387755102,
      "grad_norm": 0.3744601607322693,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.4605,
      "step": 933
    },
    {
      "epoch": 3.17687074829932,
      "grad_norm": 0.33481743931770325,
      "learning_rate": 6.7e-05,
      "loss": 0.3997,
      "step": 934
    },
    {
      "epoch": 3.1802721088435373,
      "grad_norm": 0.4179706275463104,
      "learning_rate": 6.6e-05,
      "loss": 0.5599,
      "step": 935
    },
    {
      "epoch": 3.183673469387755,
      "grad_norm": 0.38930392265319824,
      "learning_rate": 6.500000000000001e-05,
      "loss": 0.4858,
      "step": 936
    },
    {
      "epoch": 3.187074829931973,
      "grad_norm": 0.384133905172348,
      "learning_rate": 6.4e-05,
      "loss": 0.4244,
      "step": 937
    },
    {
      "epoch": 3.1904761904761907,
      "grad_norm": 0.34524646401405334,
      "learning_rate": 6.3e-05,
      "loss": 0.4426,
      "step": 938
    },
    {
      "epoch": 3.193877551020408,
      "grad_norm": 0.40634071826934814,
      "learning_rate": 6.2e-05,
      "loss": 0.5446,
      "step": 939
    },
    {
      "epoch": 3.197278911564626,
      "grad_norm": 0.3572140336036682,
      "learning_rate": 6.1e-05,
      "loss": 0.4307,
      "step": 940
    },
    {
      "epoch": 3.2006802721088436,
      "grad_norm": 0.4106769561767578,
      "learning_rate": 6e-05,
      "loss": 0.5628,
      "step": 941
    },
    {
      "epoch": 3.204081632653061,
      "grad_norm": 0.40839824080467224,
      "learning_rate": 5.9e-05,
      "loss": 0.5529,
      "step": 942
    },
    {
      "epoch": 3.207482993197279,
      "grad_norm": 0.4285517930984497,
      "learning_rate": 5.800000000000001e-05,
      "loss": 0.5788,
      "step": 943
    },
    {
      "epoch": 3.2108843537414966,
      "grad_norm": 0.34517839550971985,
      "learning_rate": 5.7e-05,
      "loss": 0.4065,
      "step": 944
    },
    {
      "epoch": 3.2142857142857144,
      "grad_norm": 0.4067913591861725,
      "learning_rate": 5.6e-05,
      "loss": 0.4419,
      "step": 945
    },
    {
      "epoch": 3.2176870748299318,
      "grad_norm": 0.39793485403060913,
      "learning_rate": 5.5e-05,
      "loss": 0.4374,
      "step": 946
    },
    {
      "epoch": 3.2210884353741496,
      "grad_norm": 0.4153159558773041,
      "learning_rate": 5.4e-05,
      "loss": 0.4535,
      "step": 947
    },
    {
      "epoch": 3.2244897959183674,
      "grad_norm": 0.40793877840042114,
      "learning_rate": 5.3e-05,
      "loss": 0.4856,
      "step": 948
    },
    {
      "epoch": 3.227891156462585,
      "grad_norm": 0.3890369236469269,
      "learning_rate": 5.2e-05,
      "loss": 0.4939,
      "step": 949
    },
    {
      "epoch": 3.2312925170068025,
      "grad_norm": 0.3937864303588867,
      "learning_rate": 5.1e-05,
      "loss": 0.5315,
      "step": 950
    },
    {
      "epoch": 3.2346938775510203,
      "grad_norm": 0.36934107542037964,
      "learning_rate": 5e-05,
      "loss": 0.4552,
      "step": 951
    },
    {
      "epoch": 3.238095238095238,
      "grad_norm": 0.4064260721206665,
      "learning_rate": 4.9000000000000005e-05,
      "loss": 0.4565,
      "step": 952
    },
    {
      "epoch": 3.241496598639456,
      "grad_norm": 0.40304774045944214,
      "learning_rate": 4.8e-05,
      "loss": 0.4871,
      "step": 953
    },
    {
      "epoch": 3.2448979591836733,
      "grad_norm": 0.3523615896701813,
      "learning_rate": 4.7000000000000004e-05,
      "loss": 0.4547,
      "step": 954
    },
    {
      "epoch": 3.248299319727891,
      "grad_norm": 0.4863641858100891,
      "learning_rate": 4.6e-05,
      "loss": 0.6919,
      "step": 955
    },
    {
      "epoch": 3.251700680272109,
      "grad_norm": 0.42258718609809875,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 0.6866,
      "step": 956
    },
    {
      "epoch": 3.2551020408163267,
      "grad_norm": 0.36335569620132446,
      "learning_rate": 4.4e-05,
      "loss": 0.4339,
      "step": 957
    },
    {
      "epoch": 3.258503401360544,
      "grad_norm": 0.688447892665863,
      "learning_rate": 4.2999999999999995e-05,
      "loss": 0.7267,
      "step": 958
    },
    {
      "epoch": 3.261904761904762,
      "grad_norm": 0.3731117844581604,
      "learning_rate": 4.2000000000000004e-05,
      "loss": 0.3712,
      "step": 959
    },
    {
      "epoch": 3.2653061224489797,
      "grad_norm": 0.40919482707977295,
      "learning_rate": 4.1e-05,
      "loss": 0.5083,
      "step": 960
    },
    {
      "epoch": 3.2687074829931975,
      "grad_norm": 0.36879023909568787,
      "learning_rate": 4e-05,
      "loss": 0.4804,
      "step": 961
    },
    {
      "epoch": 3.272108843537415,
      "grad_norm": 0.6476359963417053,
      "learning_rate": 3.9e-05,
      "loss": 0.6463,
      "step": 962
    },
    {
      "epoch": 3.2755102040816326,
      "grad_norm": 0.406659334897995,
      "learning_rate": 3.8e-05,
      "loss": 0.5482,
      "step": 963
    },
    {
      "epoch": 3.2789115646258504,
      "grad_norm": 0.41298288106918335,
      "learning_rate": 3.7e-05,
      "loss": 0.5638,
      "step": 964
    },
    {
      "epoch": 3.282312925170068,
      "grad_norm": 0.38989371061325073,
      "learning_rate": 3.6e-05,
      "loss": 0.4351,
      "step": 965
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.7106528282165527,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 0.7881,
      "step": 966
    },
    {
      "epoch": 3.2891156462585034,
      "grad_norm": 0.346261590719223,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.4379,
      "step": 967
    },
    {
      "epoch": 3.292517006802721,
      "grad_norm": 0.3791273534297943,
      "learning_rate": 3.3e-05,
      "loss": 0.4732,
      "step": 968
    },
    {
      "epoch": 3.295918367346939,
      "grad_norm": 0.436840683221817,
      "learning_rate": 3.2e-05,
      "loss": 0.6731,
      "step": 969
    },
    {
      "epoch": 3.2993197278911564,
      "grad_norm": 0.43579113483428955,
      "learning_rate": 3.1e-05,
      "loss": 0.6022,
      "step": 970
    },
    {
      "epoch": 3.302721088435374,
      "grad_norm": 0.36250993609428406,
      "learning_rate": 3e-05,
      "loss": 0.3959,
      "step": 971
    },
    {
      "epoch": 3.306122448979592,
      "grad_norm": 0.4250190556049347,
      "learning_rate": 2.9000000000000004e-05,
      "loss": 0.4754,
      "step": 972
    },
    {
      "epoch": 3.3095238095238093,
      "grad_norm": 0.3923872113227844,
      "learning_rate": 2.8e-05,
      "loss": 0.4629,
      "step": 973
    },
    {
      "epoch": 3.312925170068027,
      "grad_norm": 0.42232006788253784,
      "learning_rate": 2.7e-05,
      "loss": 0.5713,
      "step": 974
    },
    {
      "epoch": 3.316326530612245,
      "grad_norm": 0.380370557308197,
      "learning_rate": 2.6e-05,
      "loss": 0.474,
      "step": 975
    },
    {
      "epoch": 3.3197278911564627,
      "grad_norm": 0.3608500361442566,
      "learning_rate": 2.5e-05,
      "loss": 0.4171,
      "step": 976
    },
    {
      "epoch": 3.3231292517006805,
      "grad_norm": 0.36512142419815063,
      "learning_rate": 2.4e-05,
      "loss": 0.4116,
      "step": 977
    },
    {
      "epoch": 3.326530612244898,
      "grad_norm": 0.3436587154865265,
      "learning_rate": 2.3e-05,
      "loss": 0.3975,
      "step": 978
    },
    {
      "epoch": 3.3299319727891157,
      "grad_norm": 0.38314422965049744,
      "learning_rate": 2.2e-05,
      "loss": 0.4729,
      "step": 979
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.382527232170105,
      "learning_rate": 2.1000000000000002e-05,
      "loss": 0.5947,
      "step": 980
    },
    {
      "epoch": 3.336734693877551,
      "grad_norm": 0.5262083411216736,
      "learning_rate": 2e-05,
      "loss": 0.707,
      "step": 981
    },
    {
      "epoch": 3.3401360544217686,
      "grad_norm": 0.34569334983825684,
      "learning_rate": 1.9e-05,
      "loss": 0.495,
      "step": 982
    },
    {
      "epoch": 3.3435374149659864,
      "grad_norm": 0.4672069251537323,
      "learning_rate": 1.8e-05,
      "loss": 0.6156,
      "step": 983
    },
    {
      "epoch": 3.3469387755102042,
      "grad_norm": 0.6617316007614136,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.6884,
      "step": 984
    },
    {
      "epoch": 3.3503401360544216,
      "grad_norm": 0.38614344596862793,
      "learning_rate": 1.6e-05,
      "loss": 0.488,
      "step": 985
    },
    {
      "epoch": 3.3537414965986394,
      "grad_norm": 0.38310152292251587,
      "learning_rate": 1.5e-05,
      "loss": 0.5519,
      "step": 986
    },
    {
      "epoch": 3.357142857142857,
      "grad_norm": 0.3936814069747925,
      "learning_rate": 1.4e-05,
      "loss": 0.5133,
      "step": 987
    },
    {
      "epoch": 3.360544217687075,
      "grad_norm": 0.3795023560523987,
      "learning_rate": 1.3e-05,
      "loss": 0.4404,
      "step": 988
    },
    {
      "epoch": 3.3639455782312924,
      "grad_norm": 0.4151543974876404,
      "learning_rate": 1.2e-05,
      "loss": 0.4912,
      "step": 989
    },
    {
      "epoch": 3.36734693877551,
      "grad_norm": 0.7175803184509277,
      "learning_rate": 1.1e-05,
      "loss": 0.6814,
      "step": 990
    },
    {
      "epoch": 3.370748299319728,
      "grad_norm": 0.36451032757759094,
      "learning_rate": 1e-05,
      "loss": 0.4651,
      "step": 991
    },
    {
      "epoch": 3.3741496598639458,
      "grad_norm": 0.6233773827552795,
      "learning_rate": 9e-06,
      "loss": 0.6414,
      "step": 992
    },
    {
      "epoch": 3.377551020408163,
      "grad_norm": 0.5620681047439575,
      "learning_rate": 8e-06,
      "loss": 0.6377,
      "step": 993
    },
    {
      "epoch": 3.380952380952381,
      "grad_norm": 0.38636916875839233,
      "learning_rate": 7e-06,
      "loss": 0.4724,
      "step": 994
    },
    {
      "epoch": 3.3843537414965987,
      "grad_norm": 0.6542948484420776,
      "learning_rate": 6e-06,
      "loss": 0.5696,
      "step": 995
    },
    {
      "epoch": 3.387755102040816,
      "grad_norm": 0.36901357769966125,
      "learning_rate": 5e-06,
      "loss": 0.4669,
      "step": 996
    },
    {
      "epoch": 3.391156462585034,
      "grad_norm": 0.4467936158180237,
      "learning_rate": 4e-06,
      "loss": 0.6595,
      "step": 997
    },
    {
      "epoch": 3.3945578231292517,
      "grad_norm": 0.4168374538421631,
      "learning_rate": 3e-06,
      "loss": 0.5221,
      "step": 998
    },
    {
      "epoch": 3.3979591836734695,
      "grad_norm": 0.7062934637069702,
      "learning_rate": 2e-06,
      "loss": 0.6537,
      "step": 999
    },
    {
      "epoch": 3.4013605442176873,
      "grad_norm": 0.43468204140663147,
      "learning_rate": 1e-06,
      "loss": 0.4753,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2993430724608000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
