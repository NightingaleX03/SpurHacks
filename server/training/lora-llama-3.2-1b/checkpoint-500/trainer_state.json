{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.7006802721088436,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003401360544217687,
      "grad_norm": 0.43904975056648254,
      "learning_rate": 0.001,
      "loss": 0.6628,
      "step": 1
    },
    {
      "epoch": 0.006802721088435374,
      "grad_norm": 0.34338799118995667,
      "learning_rate": 0.000999,
      "loss": 0.4078,
      "step": 2
    },
    {
      "epoch": 0.01020408163265306,
      "grad_norm": 0.3496520519256592,
      "learning_rate": 0.000998,
      "loss": 0.446,
      "step": 3
    },
    {
      "epoch": 0.013605442176870748,
      "grad_norm": 0.7586523294448853,
      "learning_rate": 0.000997,
      "loss": 0.785,
      "step": 4
    },
    {
      "epoch": 0.017006802721088437,
      "grad_norm": 0.49323564767837524,
      "learning_rate": 0.000996,
      "loss": 0.6371,
      "step": 5
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 0.353442519903183,
      "learning_rate": 0.000995,
      "loss": 0.3601,
      "step": 6
    },
    {
      "epoch": 0.023809523809523808,
      "grad_norm": 0.4444120526313782,
      "learning_rate": 0.000994,
      "loss": 0.5081,
      "step": 7
    },
    {
      "epoch": 0.027210884353741496,
      "grad_norm": 0.3631472885608673,
      "learning_rate": 0.000993,
      "loss": 0.459,
      "step": 8
    },
    {
      "epoch": 0.030612244897959183,
      "grad_norm": 0.412321537733078,
      "learning_rate": 0.000992,
      "loss": 0.5179,
      "step": 9
    },
    {
      "epoch": 0.034013605442176874,
      "grad_norm": 0.4057539999485016,
      "learning_rate": 0.000991,
      "loss": 0.5577,
      "step": 10
    },
    {
      "epoch": 0.03741496598639456,
      "grad_norm": 0.4252506494522095,
      "learning_rate": 0.00099,
      "loss": 0.4941,
      "step": 11
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 0.4808809459209442,
      "learning_rate": 0.000989,
      "loss": 0.5394,
      "step": 12
    },
    {
      "epoch": 0.04421768707482993,
      "grad_norm": 0.4597184360027313,
      "learning_rate": 0.000988,
      "loss": 0.583,
      "step": 13
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 0.6446241140365601,
      "learning_rate": 0.000987,
      "loss": 0.6863,
      "step": 14
    },
    {
      "epoch": 0.05102040816326531,
      "grad_norm": 0.4800374507904053,
      "learning_rate": 0.0009860000000000001,
      "loss": 0.6604,
      "step": 15
    },
    {
      "epoch": 0.05442176870748299,
      "grad_norm": 0.4625658094882965,
      "learning_rate": 0.000985,
      "loss": 0.5289,
      "step": 16
    },
    {
      "epoch": 0.05782312925170068,
      "grad_norm": 0.7231526374816895,
      "learning_rate": 0.000984,
      "loss": 0.6706,
      "step": 17
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 0.42780622839927673,
      "learning_rate": 0.000983,
      "loss": 0.4902,
      "step": 18
    },
    {
      "epoch": 0.06462585034013606,
      "grad_norm": 0.47992438077926636,
      "learning_rate": 0.000982,
      "loss": 0.604,
      "step": 19
    },
    {
      "epoch": 0.06802721088435375,
      "grad_norm": 0.4256789982318878,
      "learning_rate": 0.000981,
      "loss": 0.4513,
      "step": 20
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 0.43105411529541016,
      "learning_rate": 0.00098,
      "loss": 0.5057,
      "step": 21
    },
    {
      "epoch": 0.07482993197278912,
      "grad_norm": 0.45649662613868713,
      "learning_rate": 0.000979,
      "loss": 0.6078,
      "step": 22
    },
    {
      "epoch": 0.0782312925170068,
      "grad_norm": 0.4225643277168274,
      "learning_rate": 0.000978,
      "loss": 0.5001,
      "step": 23
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 0.7035265564918518,
      "learning_rate": 0.000977,
      "loss": 0.7213,
      "step": 24
    },
    {
      "epoch": 0.08503401360544217,
      "grad_norm": 0.4117766320705414,
      "learning_rate": 0.000976,
      "loss": 0.4845,
      "step": 25
    },
    {
      "epoch": 0.08843537414965986,
      "grad_norm": 0.9819594025611877,
      "learning_rate": 0.000975,
      "loss": 0.5432,
      "step": 26
    },
    {
      "epoch": 0.09183673469387756,
      "grad_norm": 0.4424475431442261,
      "learning_rate": 0.000974,
      "loss": 0.5319,
      "step": 27
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 0.4152551591396332,
      "learning_rate": 0.000973,
      "loss": 0.4538,
      "step": 28
    },
    {
      "epoch": 0.09863945578231292,
      "grad_norm": 0.427023321390152,
      "learning_rate": 0.000972,
      "loss": 0.4718,
      "step": 29
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 0.40389102697372437,
      "learning_rate": 0.000971,
      "loss": 0.4991,
      "step": 30
    },
    {
      "epoch": 0.1054421768707483,
      "grad_norm": 0.4629152715206146,
      "learning_rate": 0.0009699999999999999,
      "loss": 0.6724,
      "step": 31
    },
    {
      "epoch": 0.10884353741496598,
      "grad_norm": 0.4302474856376648,
      "learning_rate": 0.000969,
      "loss": 0.5556,
      "step": 32
    },
    {
      "epoch": 0.11224489795918367,
      "grad_norm": 0.38674086332321167,
      "learning_rate": 0.000968,
      "loss": 0.4879,
      "step": 33
    },
    {
      "epoch": 0.11564625850340136,
      "grad_norm": 0.48143380880355835,
      "learning_rate": 0.000967,
      "loss": 0.5961,
      "step": 34
    },
    {
      "epoch": 0.11904761904761904,
      "grad_norm": 0.49737176299095154,
      "learning_rate": 0.000966,
      "loss": 0.5835,
      "step": 35
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 0.616187572479248,
      "learning_rate": 0.000965,
      "loss": 0.7033,
      "step": 36
    },
    {
      "epoch": 0.12585034013605442,
      "grad_norm": 0.8163517713546753,
      "learning_rate": 0.000964,
      "loss": 0.6043,
      "step": 37
    },
    {
      "epoch": 0.1292517006802721,
      "grad_norm": 0.75090092420578,
      "learning_rate": 0.000963,
      "loss": 0.8775,
      "step": 38
    },
    {
      "epoch": 0.1326530612244898,
      "grad_norm": 0.4003351032733917,
      "learning_rate": 0.000962,
      "loss": 0.5275,
      "step": 39
    },
    {
      "epoch": 0.1360544217687075,
      "grad_norm": 0.4579761028289795,
      "learning_rate": 0.0009609999999999999,
      "loss": 0.461,
      "step": 40
    },
    {
      "epoch": 0.13945578231292516,
      "grad_norm": 0.42943528294563293,
      "learning_rate": 0.00096,
      "loss": 0.5412,
      "step": 41
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.45541584491729736,
      "learning_rate": 0.000959,
      "loss": 0.6362,
      "step": 42
    },
    {
      "epoch": 0.14625850340136054,
      "grad_norm": 0.4088207483291626,
      "learning_rate": 0.000958,
      "loss": 0.5062,
      "step": 43
    },
    {
      "epoch": 0.14965986394557823,
      "grad_norm": 0.40928715467453003,
      "learning_rate": 0.000957,
      "loss": 0.6413,
      "step": 44
    },
    {
      "epoch": 0.15306122448979592,
      "grad_norm": 0.6453690528869629,
      "learning_rate": 0.0009559999999999999,
      "loss": 0.7,
      "step": 45
    },
    {
      "epoch": 0.1564625850340136,
      "grad_norm": 0.38388240337371826,
      "learning_rate": 0.000955,
      "loss": 0.515,
      "step": 46
    },
    {
      "epoch": 0.1598639455782313,
      "grad_norm": 0.38415101170539856,
      "learning_rate": 0.000954,
      "loss": 0.4829,
      "step": 47
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 0.383630633354187,
      "learning_rate": 0.000953,
      "loss": 0.518,
      "step": 48
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.5679585933685303,
      "learning_rate": 0.0009519999999999999,
      "loss": 0.7639,
      "step": 49
    },
    {
      "epoch": 0.17006802721088435,
      "grad_norm": 0.3628492057323456,
      "learning_rate": 0.000951,
      "loss": 0.4747,
      "step": 50
    },
    {
      "epoch": 0.17346938775510204,
      "grad_norm": 0.3659270405769348,
      "learning_rate": 0.00095,
      "loss": 0.4722,
      "step": 51
    },
    {
      "epoch": 0.17687074829931973,
      "grad_norm": 0.657664954662323,
      "learning_rate": 0.000949,
      "loss": 0.697,
      "step": 52
    },
    {
      "epoch": 0.18027210884353742,
      "grad_norm": 0.3902646601200104,
      "learning_rate": 0.000948,
      "loss": 0.5537,
      "step": 53
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 0.7268112897872925,
      "learning_rate": 0.0009469999999999999,
      "loss": 0.7982,
      "step": 54
    },
    {
      "epoch": 0.1870748299319728,
      "grad_norm": 0.44308140873908997,
      "learning_rate": 0.000946,
      "loss": 0.5191,
      "step": 55
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 0.619560182094574,
      "learning_rate": 0.000945,
      "loss": 0.5531,
      "step": 56
    },
    {
      "epoch": 0.19387755102040816,
      "grad_norm": 0.4778512418270111,
      "learning_rate": 0.000944,
      "loss": 0.6011,
      "step": 57
    },
    {
      "epoch": 0.19727891156462585,
      "grad_norm": 0.5032291412353516,
      "learning_rate": 0.0009429999999999999,
      "loss": 0.6517,
      "step": 58
    },
    {
      "epoch": 0.20068027210884354,
      "grad_norm": 0.4066300392150879,
      "learning_rate": 0.000942,
      "loss": 0.5284,
      "step": 59
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 0.45637619495391846,
      "learning_rate": 0.000941,
      "loss": 0.4577,
      "step": 60
    },
    {
      "epoch": 0.20748299319727892,
      "grad_norm": 0.4239285886287689,
      "learning_rate": 0.00094,
      "loss": 0.5089,
      "step": 61
    },
    {
      "epoch": 0.2108843537414966,
      "grad_norm": 0.46293678879737854,
      "learning_rate": 0.000939,
      "loss": 0.6767,
      "step": 62
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 0.41105180978775024,
      "learning_rate": 0.0009379999999999999,
      "loss": 0.4918,
      "step": 63
    },
    {
      "epoch": 0.21768707482993196,
      "grad_norm": 0.3781757652759552,
      "learning_rate": 0.0009370000000000001,
      "loss": 0.4496,
      "step": 64
    },
    {
      "epoch": 0.22108843537414966,
      "grad_norm": 0.4076799154281616,
      "learning_rate": 0.0009360000000000001,
      "loss": 0.4682,
      "step": 65
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 0.4034968912601471,
      "learning_rate": 0.0009350000000000001,
      "loss": 0.5238,
      "step": 66
    },
    {
      "epoch": 0.22789115646258504,
      "grad_norm": 0.33748582005500793,
      "learning_rate": 0.000934,
      "loss": 0.4093,
      "step": 67
    },
    {
      "epoch": 0.23129251700680273,
      "grad_norm": 0.4484717845916748,
      "learning_rate": 0.000933,
      "loss": 0.6232,
      "step": 68
    },
    {
      "epoch": 0.23469387755102042,
      "grad_norm": 0.4544461965560913,
      "learning_rate": 0.0009320000000000001,
      "loss": 0.521,
      "step": 69
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 0.3191428780555725,
      "learning_rate": 0.0009310000000000001,
      "loss": 0.3945,
      "step": 70
    },
    {
      "epoch": 0.24149659863945577,
      "grad_norm": 0.4655865430831909,
      "learning_rate": 0.00093,
      "loss": 0.6157,
      "step": 71
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 0.7077147960662842,
      "learning_rate": 0.000929,
      "loss": 0.8228,
      "step": 72
    },
    {
      "epoch": 0.24829931972789115,
      "grad_norm": 0.4681417942047119,
      "learning_rate": 0.0009280000000000001,
      "loss": 0.5182,
      "step": 73
    },
    {
      "epoch": 0.25170068027210885,
      "grad_norm": 0.4665752053260803,
      "learning_rate": 0.0009270000000000001,
      "loss": 0.6592,
      "step": 74
    },
    {
      "epoch": 0.25510204081632654,
      "grad_norm": 0.3582819104194641,
      "learning_rate": 0.0009260000000000001,
      "loss": 0.444,
      "step": 75
    },
    {
      "epoch": 0.2585034013605442,
      "grad_norm": 0.5536210536956787,
      "learning_rate": 0.000925,
      "loss": 0.6418,
      "step": 76
    },
    {
      "epoch": 0.2619047619047619,
      "grad_norm": 0.4808898866176605,
      "learning_rate": 0.000924,
      "loss": 0.6588,
      "step": 77
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 0.8391821384429932,
      "learning_rate": 0.0009230000000000001,
      "loss": 0.9355,
      "step": 78
    },
    {
      "epoch": 0.2687074829931973,
      "grad_norm": 0.8346506953239441,
      "learning_rate": 0.0009220000000000001,
      "loss": 0.461,
      "step": 79
    },
    {
      "epoch": 0.272108843537415,
      "grad_norm": 0.41494882106781006,
      "learning_rate": 0.000921,
      "loss": 0.5509,
      "step": 80
    },
    {
      "epoch": 0.2755102040816326,
      "grad_norm": 0.39272966980934143,
      "learning_rate": 0.00092,
      "loss": 0.5483,
      "step": 81
    },
    {
      "epoch": 0.2789115646258503,
      "grad_norm": 0.46857863664627075,
      "learning_rate": 0.0009190000000000001,
      "loss": 0.6117,
      "step": 82
    },
    {
      "epoch": 0.282312925170068,
      "grad_norm": 0.4624940752983093,
      "learning_rate": 0.0009180000000000001,
      "loss": 0.665,
      "step": 83
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.47268638014793396,
      "learning_rate": 0.0009170000000000001,
      "loss": 0.6185,
      "step": 84
    },
    {
      "epoch": 0.2891156462585034,
      "grad_norm": 0.8000028729438782,
      "learning_rate": 0.000916,
      "loss": 0.8192,
      "step": 85
    },
    {
      "epoch": 0.2925170068027211,
      "grad_norm": 1.062780737876892,
      "learning_rate": 0.000915,
      "loss": 0.7773,
      "step": 86
    },
    {
      "epoch": 0.29591836734693877,
      "grad_norm": 2.517961263656616,
      "learning_rate": 0.0009140000000000001,
      "loss": 0.5421,
      "step": 87
    },
    {
      "epoch": 0.29931972789115646,
      "grad_norm": 0.4983011484146118,
      "learning_rate": 0.0009130000000000001,
      "loss": 0.5034,
      "step": 88
    },
    {
      "epoch": 0.30272108843537415,
      "grad_norm": 0.4985005259513855,
      "learning_rate": 0.000912,
      "loss": 0.7319,
      "step": 89
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 0.5695905685424805,
      "learning_rate": 0.000911,
      "loss": 0.5597,
      "step": 90
    },
    {
      "epoch": 0.30952380952380953,
      "grad_norm": 0.569358229637146,
      "learning_rate": 0.00091,
      "loss": 0.6189,
      "step": 91
    },
    {
      "epoch": 0.3129251700680272,
      "grad_norm": 0.49868011474609375,
      "learning_rate": 0.0009090000000000001,
      "loss": 0.5304,
      "step": 92
    },
    {
      "epoch": 0.3163265306122449,
      "grad_norm": 0.5510866641998291,
      "learning_rate": 0.0009080000000000001,
      "loss": 0.544,
      "step": 93
    },
    {
      "epoch": 0.3197278911564626,
      "grad_norm": 0.9388875365257263,
      "learning_rate": 0.000907,
      "loss": 0.7614,
      "step": 94
    },
    {
      "epoch": 0.3231292517006803,
      "grad_norm": 0.5690369009971619,
      "learning_rate": 0.000906,
      "loss": 0.739,
      "step": 95
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 0.4840584993362427,
      "learning_rate": 0.0009050000000000001,
      "loss": 0.6339,
      "step": 96
    },
    {
      "epoch": 0.3299319727891156,
      "grad_norm": 0.6260333061218262,
      "learning_rate": 0.0009040000000000001,
      "loss": 0.6578,
      "step": 97
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.4350126087665558,
      "learning_rate": 0.000903,
      "loss": 0.4878,
      "step": 98
    },
    {
      "epoch": 0.336734693877551,
      "grad_norm": 0.44257140159606934,
      "learning_rate": 0.000902,
      "loss": 0.6567,
      "step": 99
    },
    {
      "epoch": 0.3401360544217687,
      "grad_norm": 0.47258812189102173,
      "learning_rate": 0.000901,
      "loss": 0.5074,
      "step": 100
    },
    {
      "epoch": 0.3435374149659864,
      "grad_norm": 0.43536731600761414,
      "learning_rate": 0.0009000000000000001,
      "loss": 0.4923,
      "step": 101
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 0.37104126811027527,
      "learning_rate": 0.0008990000000000001,
      "loss": 0.4747,
      "step": 102
    },
    {
      "epoch": 0.35034013605442177,
      "grad_norm": 0.5936031937599182,
      "learning_rate": 0.000898,
      "loss": 0.6882,
      "step": 103
    },
    {
      "epoch": 0.35374149659863946,
      "grad_norm": 0.47464168071746826,
      "learning_rate": 0.000897,
      "loss": 0.5165,
      "step": 104
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 0.4576664865016937,
      "learning_rate": 0.000896,
      "loss": 0.5461,
      "step": 105
    },
    {
      "epoch": 0.36054421768707484,
      "grad_norm": 0.400833398103714,
      "learning_rate": 0.0008950000000000001,
      "loss": 0.5841,
      "step": 106
    },
    {
      "epoch": 0.36394557823129253,
      "grad_norm": 0.37464404106140137,
      "learning_rate": 0.000894,
      "loss": 0.4444,
      "step": 107
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 0.39482617378234863,
      "learning_rate": 0.000893,
      "loss": 0.4466,
      "step": 108
    },
    {
      "epoch": 0.3707482993197279,
      "grad_norm": 0.38426467776298523,
      "learning_rate": 0.000892,
      "loss": 0.4487,
      "step": 109
    },
    {
      "epoch": 0.3741496598639456,
      "grad_norm": 0.4058493375778198,
      "learning_rate": 0.0008910000000000001,
      "loss": 0.5521,
      "step": 110
    },
    {
      "epoch": 0.37755102040816324,
      "grad_norm": 0.445745587348938,
      "learning_rate": 0.0008900000000000001,
      "loss": 0.5798,
      "step": 111
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 0.7277312278747559,
      "learning_rate": 0.000889,
      "loss": 0.8218,
      "step": 112
    },
    {
      "epoch": 0.3843537414965986,
      "grad_norm": 0.4721895754337311,
      "learning_rate": 0.000888,
      "loss": 0.6182,
      "step": 113
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 0.5992775559425354,
      "learning_rate": 0.000887,
      "loss": 0.7944,
      "step": 114
    },
    {
      "epoch": 0.391156462585034,
      "grad_norm": 0.3790716528892517,
      "learning_rate": 0.0008860000000000001,
      "loss": 0.4337,
      "step": 115
    },
    {
      "epoch": 0.3945578231292517,
      "grad_norm": 0.4485473036766052,
      "learning_rate": 0.000885,
      "loss": 0.6711,
      "step": 116
    },
    {
      "epoch": 0.3979591836734694,
      "grad_norm": 0.4593411684036255,
      "learning_rate": 0.000884,
      "loss": 0.4729,
      "step": 117
    },
    {
      "epoch": 0.4013605442176871,
      "grad_norm": 0.42524856328964233,
      "learning_rate": 0.000883,
      "loss": 0.5204,
      "step": 118
    },
    {
      "epoch": 0.40476190476190477,
      "grad_norm": 0.5161793231964111,
      "learning_rate": 0.000882,
      "loss": 0.5659,
      "step": 119
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 0.40441811084747314,
      "learning_rate": 0.0008810000000000001,
      "loss": 0.4818,
      "step": 120
    },
    {
      "epoch": 0.41156462585034015,
      "grad_norm": 0.45445331931114197,
      "learning_rate": 0.00088,
      "loss": 0.6232,
      "step": 121
    },
    {
      "epoch": 0.41496598639455784,
      "grad_norm": 0.5509088635444641,
      "learning_rate": 0.000879,
      "loss": 0.7611,
      "step": 122
    },
    {
      "epoch": 0.41836734693877553,
      "grad_norm": 0.5153271555900574,
      "learning_rate": 0.000878,
      "loss": 0.6321,
      "step": 123
    },
    {
      "epoch": 0.4217687074829932,
      "grad_norm": 0.759239137172699,
      "learning_rate": 0.0008770000000000001,
      "loss": 0.6202,
      "step": 124
    },
    {
      "epoch": 0.42517006802721086,
      "grad_norm": 0.38372543454170227,
      "learning_rate": 0.000876,
      "loss": 0.4605,
      "step": 125
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.6664329171180725,
      "learning_rate": 0.000875,
      "loss": 0.7487,
      "step": 126
    },
    {
      "epoch": 0.43197278911564624,
      "grad_norm": 0.39703547954559326,
      "learning_rate": 0.000874,
      "loss": 0.5229,
      "step": 127
    },
    {
      "epoch": 0.43537414965986393,
      "grad_norm": 0.46749013662338257,
      "learning_rate": 0.000873,
      "loss": 0.5426,
      "step": 128
    },
    {
      "epoch": 0.4387755102040816,
      "grad_norm": 0.4086017310619354,
      "learning_rate": 0.000872,
      "loss": 0.5119,
      "step": 129
    },
    {
      "epoch": 0.4421768707482993,
      "grad_norm": 0.40723717212677,
      "learning_rate": 0.000871,
      "loss": 0.5642,
      "step": 130
    },
    {
      "epoch": 0.445578231292517,
      "grad_norm": 0.42833051085472107,
      "learning_rate": 0.00087,
      "loss": 0.4631,
      "step": 131
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 0.4198424816131592,
      "learning_rate": 0.000869,
      "loss": 0.5712,
      "step": 132
    },
    {
      "epoch": 0.4523809523809524,
      "grad_norm": 0.4579571783542633,
      "learning_rate": 0.0008680000000000001,
      "loss": 0.4185,
      "step": 133
    },
    {
      "epoch": 0.4557823129251701,
      "grad_norm": 0.3898964524269104,
      "learning_rate": 0.000867,
      "loss": 0.4789,
      "step": 134
    },
    {
      "epoch": 0.45918367346938777,
      "grad_norm": 0.4107678234577179,
      "learning_rate": 0.000866,
      "loss": 0.5347,
      "step": 135
    },
    {
      "epoch": 0.46258503401360546,
      "grad_norm": 0.41668638586997986,
      "learning_rate": 0.000865,
      "loss": 0.5782,
      "step": 136
    },
    {
      "epoch": 0.46598639455782315,
      "grad_norm": 0.37064945697784424,
      "learning_rate": 0.000864,
      "loss": 0.5323,
      "step": 137
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 0.8612028360366821,
      "learning_rate": 0.000863,
      "loss": 0.676,
      "step": 138
    },
    {
      "epoch": 0.47278911564625853,
      "grad_norm": 0.658547580242157,
      "learning_rate": 0.000862,
      "loss": 0.6909,
      "step": 139
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.35427430272102356,
      "learning_rate": 0.000861,
      "loss": 0.4517,
      "step": 140
    },
    {
      "epoch": 0.47959183673469385,
      "grad_norm": 0.4198349714279175,
      "learning_rate": 0.00086,
      "loss": 0.494,
      "step": 141
    },
    {
      "epoch": 0.48299319727891155,
      "grad_norm": 0.4046105444431305,
      "learning_rate": 0.000859,
      "loss": 0.4314,
      "step": 142
    },
    {
      "epoch": 0.48639455782312924,
      "grad_norm": 0.7313613891601562,
      "learning_rate": 0.000858,
      "loss": 0.6825,
      "step": 143
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 0.6087557673454285,
      "learning_rate": 0.000857,
      "loss": 0.6661,
      "step": 144
    },
    {
      "epoch": 0.4931972789115646,
      "grad_norm": 0.46740448474884033,
      "learning_rate": 0.000856,
      "loss": 0.5892,
      "step": 145
    },
    {
      "epoch": 0.4965986394557823,
      "grad_norm": 0.7027713656425476,
      "learning_rate": 0.000855,
      "loss": 0.8245,
      "step": 146
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.772977352142334,
      "learning_rate": 0.000854,
      "loss": 0.549,
      "step": 147
    },
    {
      "epoch": 0.5034013605442177,
      "grad_norm": 0.5903976559638977,
      "learning_rate": 0.000853,
      "loss": 0.704,
      "step": 148
    },
    {
      "epoch": 0.5068027210884354,
      "grad_norm": 0.5079483985900879,
      "learning_rate": 0.000852,
      "loss": 0.589,
      "step": 149
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 0.7311748266220093,
      "learning_rate": 0.000851,
      "loss": 0.7171,
      "step": 150
    },
    {
      "epoch": 0.5136054421768708,
      "grad_norm": 0.7821089625358582,
      "learning_rate": 0.00085,
      "loss": 0.5305,
      "step": 151
    },
    {
      "epoch": 0.5170068027210885,
      "grad_norm": 0.4937252700328827,
      "learning_rate": 0.000849,
      "loss": 0.4785,
      "step": 152
    },
    {
      "epoch": 0.5204081632653061,
      "grad_norm": 0.4567559063434601,
      "learning_rate": 0.000848,
      "loss": 0.638,
      "step": 153
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 0.4812706410884857,
      "learning_rate": 0.000847,
      "loss": 0.5908,
      "step": 154
    },
    {
      "epoch": 0.5272108843537415,
      "grad_norm": 0.49904027581214905,
      "learning_rate": 0.000846,
      "loss": 0.5353,
      "step": 155
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 0.5085261464118958,
      "learning_rate": 0.0008449999999999999,
      "loss": 0.6445,
      "step": 156
    },
    {
      "epoch": 0.5340136054421769,
      "grad_norm": 0.4896882176399231,
      "learning_rate": 0.000844,
      "loss": 0.5633,
      "step": 157
    },
    {
      "epoch": 0.5374149659863946,
      "grad_norm": 0.3975076973438263,
      "learning_rate": 0.000843,
      "loss": 0.5517,
      "step": 158
    },
    {
      "epoch": 0.5408163265306123,
      "grad_norm": 0.42066484689712524,
      "learning_rate": 0.000842,
      "loss": 0.642,
      "step": 159
    },
    {
      "epoch": 0.54421768707483,
      "grad_norm": 0.4417033791542053,
      "learning_rate": 0.000841,
      "loss": 0.5879,
      "step": 160
    },
    {
      "epoch": 0.5476190476190477,
      "grad_norm": 0.3946283459663391,
      "learning_rate": 0.00084,
      "loss": 0.6037,
      "step": 161
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 0.44213202595710754,
      "learning_rate": 0.000839,
      "loss": 0.5717,
      "step": 162
    },
    {
      "epoch": 0.5544217687074829,
      "grad_norm": 0.4454842209815979,
      "learning_rate": 0.000838,
      "loss": 0.4813,
      "step": 163
    },
    {
      "epoch": 0.5578231292517006,
      "grad_norm": 0.4127839207649231,
      "learning_rate": 0.000837,
      "loss": 0.5321,
      "step": 164
    },
    {
      "epoch": 0.5612244897959183,
      "grad_norm": 0.39013123512268066,
      "learning_rate": 0.0008359999999999999,
      "loss": 0.5924,
      "step": 165
    },
    {
      "epoch": 0.564625850340136,
      "grad_norm": 0.7014365196228027,
      "learning_rate": 0.000835,
      "loss": 0.7484,
      "step": 166
    },
    {
      "epoch": 0.5680272108843537,
      "grad_norm": 0.5378953218460083,
      "learning_rate": 0.000834,
      "loss": 0.8059,
      "step": 167
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.4366937577724457,
      "learning_rate": 0.000833,
      "loss": 0.5231,
      "step": 168
    },
    {
      "epoch": 0.5748299319727891,
      "grad_norm": 0.5417550802230835,
      "learning_rate": 0.000832,
      "loss": 0.5354,
      "step": 169
    },
    {
      "epoch": 0.5782312925170068,
      "grad_norm": 0.5081700682640076,
      "learning_rate": 0.0008309999999999999,
      "loss": 0.4768,
      "step": 170
    },
    {
      "epoch": 0.5816326530612245,
      "grad_norm": 0.45062127709388733,
      "learning_rate": 0.00083,
      "loss": 0.5515,
      "step": 171
    },
    {
      "epoch": 0.5850340136054422,
      "grad_norm": 0.43168529868125916,
      "learning_rate": 0.000829,
      "loss": 0.5555,
      "step": 172
    },
    {
      "epoch": 0.5884353741496599,
      "grad_norm": 0.4140106737613678,
      "learning_rate": 0.000828,
      "loss": 0.532,
      "step": 173
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 0.5312669277191162,
      "learning_rate": 0.0008269999999999999,
      "loss": 0.6322,
      "step": 174
    },
    {
      "epoch": 0.5952380952380952,
      "grad_norm": 0.416483610868454,
      "learning_rate": 0.000826,
      "loss": 0.591,
      "step": 175
    },
    {
      "epoch": 0.5986394557823129,
      "grad_norm": 0.39083367586135864,
      "learning_rate": 0.000825,
      "loss": 0.4439,
      "step": 176
    },
    {
      "epoch": 0.6020408163265306,
      "grad_norm": 0.40139806270599365,
      "learning_rate": 0.000824,
      "loss": 0.4879,
      "step": 177
    },
    {
      "epoch": 0.6054421768707483,
      "grad_norm": 0.47462719678878784,
      "learning_rate": 0.000823,
      "loss": 0.5154,
      "step": 178
    },
    {
      "epoch": 0.608843537414966,
      "grad_norm": 0.4715237319469452,
      "learning_rate": 0.0008219999999999999,
      "loss": 0.5338,
      "step": 179
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 0.44422081112861633,
      "learning_rate": 0.000821,
      "loss": 0.5768,
      "step": 180
    },
    {
      "epoch": 0.6156462585034014,
      "grad_norm": 0.3922727406024933,
      "learning_rate": 0.00082,
      "loss": 0.458,
      "step": 181
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 0.4326289892196655,
      "learning_rate": 0.000819,
      "loss": 0.4811,
      "step": 182
    },
    {
      "epoch": 0.6224489795918368,
      "grad_norm": 0.8473650217056274,
      "learning_rate": 0.0008179999999999999,
      "loss": 0.8198,
      "step": 183
    },
    {
      "epoch": 0.6258503401360545,
      "grad_norm": 0.441036581993103,
      "learning_rate": 0.000817,
      "loss": 0.6804,
      "step": 184
    },
    {
      "epoch": 0.6292517006802721,
      "grad_norm": 0.4259941577911377,
      "learning_rate": 0.000816,
      "loss": 0.603,
      "step": 185
    },
    {
      "epoch": 0.6326530612244898,
      "grad_norm": 0.4232258200645447,
      "learning_rate": 0.000815,
      "loss": 0.6915,
      "step": 186
    },
    {
      "epoch": 0.6360544217687075,
      "grad_norm": 0.46016475558280945,
      "learning_rate": 0.0008139999999999999,
      "loss": 0.494,
      "step": 187
    },
    {
      "epoch": 0.6394557823129252,
      "grad_norm": 0.3785788416862488,
      "learning_rate": 0.0008129999999999999,
      "loss": 0.408,
      "step": 188
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 0.9034178853034973,
      "learning_rate": 0.0008120000000000001,
      "loss": 0.7019,
      "step": 189
    },
    {
      "epoch": 0.6462585034013606,
      "grad_norm": 0.6808021664619446,
      "learning_rate": 0.0008110000000000001,
      "loss": 0.5447,
      "step": 190
    },
    {
      "epoch": 0.6496598639455783,
      "grad_norm": 0.6191615462303162,
      "learning_rate": 0.0008100000000000001,
      "loss": 0.6912,
      "step": 191
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 0.3495190143585205,
      "learning_rate": 0.000809,
      "loss": 0.4035,
      "step": 192
    },
    {
      "epoch": 0.6564625850340136,
      "grad_norm": 0.44765862822532654,
      "learning_rate": 0.000808,
      "loss": 0.4828,
      "step": 193
    },
    {
      "epoch": 0.6598639455782312,
      "grad_norm": 0.468095988035202,
      "learning_rate": 0.0008070000000000001,
      "loss": 0.6117,
      "step": 194
    },
    {
      "epoch": 0.6632653061224489,
      "grad_norm": 0.4128992259502411,
      "learning_rate": 0.0008060000000000001,
      "loss": 0.4962,
      "step": 195
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.4935455918312073,
      "learning_rate": 0.000805,
      "loss": 0.5027,
      "step": 196
    },
    {
      "epoch": 0.6700680272108843,
      "grad_norm": 0.39268064498901367,
      "learning_rate": 0.000804,
      "loss": 0.4402,
      "step": 197
    },
    {
      "epoch": 0.673469387755102,
      "grad_norm": 0.35192549228668213,
      "learning_rate": 0.0008030000000000001,
      "loss": 0.442,
      "step": 198
    },
    {
      "epoch": 0.6768707482993197,
      "grad_norm": 0.4670467674732208,
      "learning_rate": 0.0008020000000000001,
      "loss": 0.6632,
      "step": 199
    },
    {
      "epoch": 0.6802721088435374,
      "grad_norm": 0.4128923714160919,
      "learning_rate": 0.0008010000000000001,
      "loss": 0.6232,
      "step": 200
    },
    {
      "epoch": 0.6836734693877551,
      "grad_norm": 0.7119241952896118,
      "learning_rate": 0.0008,
      "loss": 0.7337,
      "step": 201
    },
    {
      "epoch": 0.6870748299319728,
      "grad_norm": 0.43189719319343567,
      "learning_rate": 0.000799,
      "loss": 0.5938,
      "step": 202
    },
    {
      "epoch": 0.6904761904761905,
      "grad_norm": 0.40869569778442383,
      "learning_rate": 0.0007980000000000001,
      "loss": 0.513,
      "step": 203
    },
    {
      "epoch": 0.6938775510204082,
      "grad_norm": 0.3976488411426544,
      "learning_rate": 0.0007970000000000001,
      "loss": 0.4843,
      "step": 204
    },
    {
      "epoch": 0.6972789115646258,
      "grad_norm": 0.3737803101539612,
      "learning_rate": 0.000796,
      "loss": 0.3984,
      "step": 205
    },
    {
      "epoch": 0.7006802721088435,
      "grad_norm": 0.42684710025787354,
      "learning_rate": 0.000795,
      "loss": 0.5667,
      "step": 206
    },
    {
      "epoch": 0.7040816326530612,
      "grad_norm": 0.40371909737586975,
      "learning_rate": 0.0007940000000000001,
      "loss": 0.4369,
      "step": 207
    },
    {
      "epoch": 0.7074829931972789,
      "grad_norm": 0.6709085702896118,
      "learning_rate": 0.0007930000000000001,
      "loss": 0.6808,
      "step": 208
    },
    {
      "epoch": 0.7108843537414966,
      "grad_norm": 0.7418233752250671,
      "learning_rate": 0.0007920000000000001,
      "loss": 0.8354,
      "step": 209
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.41893333196640015,
      "learning_rate": 0.000791,
      "loss": 0.4834,
      "step": 210
    },
    {
      "epoch": 0.717687074829932,
      "grad_norm": 0.44767916202545166,
      "learning_rate": 0.00079,
      "loss": 0.6938,
      "step": 211
    },
    {
      "epoch": 0.7210884353741497,
      "grad_norm": 0.5040704011917114,
      "learning_rate": 0.0007890000000000001,
      "loss": 0.6202,
      "step": 212
    },
    {
      "epoch": 0.7244897959183674,
      "grad_norm": 0.4425831437110901,
      "learning_rate": 0.0007880000000000001,
      "loss": 0.4947,
      "step": 213
    },
    {
      "epoch": 0.7278911564625851,
      "grad_norm": 0.4075814485549927,
      "learning_rate": 0.000787,
      "loss": 0.5149,
      "step": 214
    },
    {
      "epoch": 0.7312925170068028,
      "grad_norm": 0.6825386881828308,
      "learning_rate": 0.000786,
      "loss": 0.8285,
      "step": 215
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 0.47472092509269714,
      "learning_rate": 0.000785,
      "loss": 0.5559,
      "step": 216
    },
    {
      "epoch": 0.7380952380952381,
      "grad_norm": 0.5985365509986877,
      "learning_rate": 0.0007840000000000001,
      "loss": 0.816,
      "step": 217
    },
    {
      "epoch": 0.7414965986394558,
      "grad_norm": 0.4287125766277313,
      "learning_rate": 0.0007830000000000001,
      "loss": 0.5643,
      "step": 218
    },
    {
      "epoch": 0.7448979591836735,
      "grad_norm": 0.422611802816391,
      "learning_rate": 0.000782,
      "loss": 0.4732,
      "step": 219
    },
    {
      "epoch": 0.7482993197278912,
      "grad_norm": 1.3669970035552979,
      "learning_rate": 0.000781,
      "loss": 0.5621,
      "step": 220
    },
    {
      "epoch": 0.7517006802721088,
      "grad_norm": 0.6698706746101379,
      "learning_rate": 0.0007800000000000001,
      "loss": 0.7714,
      "step": 221
    },
    {
      "epoch": 0.7551020408163265,
      "grad_norm": 0.48513227701187134,
      "learning_rate": 0.0007790000000000001,
      "loss": 0.6001,
      "step": 222
    },
    {
      "epoch": 0.7585034013605442,
      "grad_norm": 0.42329972982406616,
      "learning_rate": 0.000778,
      "loss": 0.5151,
      "step": 223
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 0.49934351444244385,
      "learning_rate": 0.000777,
      "loss": 0.5713,
      "step": 224
    },
    {
      "epoch": 0.7653061224489796,
      "grad_norm": 0.5149576663970947,
      "learning_rate": 0.000776,
      "loss": 0.5462,
      "step": 225
    },
    {
      "epoch": 0.7687074829931972,
      "grad_norm": 0.41891855001449585,
      "learning_rate": 0.0007750000000000001,
      "loss": 0.4211,
      "step": 226
    },
    {
      "epoch": 0.7721088435374149,
      "grad_norm": 0.6899072527885437,
      "learning_rate": 0.0007740000000000001,
      "loss": 0.6354,
      "step": 227
    },
    {
      "epoch": 0.7755102040816326,
      "grad_norm": 0.6013339757919312,
      "learning_rate": 0.000773,
      "loss": 0.7491,
      "step": 228
    },
    {
      "epoch": 0.7789115646258503,
      "grad_norm": 0.41804665327072144,
      "learning_rate": 0.000772,
      "loss": 0.5337,
      "step": 229
    },
    {
      "epoch": 0.782312925170068,
      "grad_norm": 0.41951555013656616,
      "learning_rate": 0.000771,
      "loss": 0.5364,
      "step": 230
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 0.44086146354675293,
      "learning_rate": 0.0007700000000000001,
      "loss": 0.5927,
      "step": 231
    },
    {
      "epoch": 0.7891156462585034,
      "grad_norm": 1.473420262336731,
      "learning_rate": 0.000769,
      "loss": 0.4801,
      "step": 232
    },
    {
      "epoch": 0.7925170068027211,
      "grad_norm": 0.5183613300323486,
      "learning_rate": 0.000768,
      "loss": 0.7,
      "step": 233
    },
    {
      "epoch": 0.7959183673469388,
      "grad_norm": 0.35542935132980347,
      "learning_rate": 0.000767,
      "loss": 0.4254,
      "step": 234
    },
    {
      "epoch": 0.7993197278911565,
      "grad_norm": 0.3784641623497009,
      "learning_rate": 0.0007660000000000001,
      "loss": 0.5011,
      "step": 235
    },
    {
      "epoch": 0.8027210884353742,
      "grad_norm": 0.3735581040382385,
      "learning_rate": 0.0007650000000000001,
      "loss": 0.4588,
      "step": 236
    },
    {
      "epoch": 0.8061224489795918,
      "grad_norm": 0.5500457286834717,
      "learning_rate": 0.000764,
      "loss": 0.5967,
      "step": 237
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 0.5763965845108032,
      "learning_rate": 0.000763,
      "loss": 0.6823,
      "step": 238
    },
    {
      "epoch": 0.8129251700680272,
      "grad_norm": 0.7331445217132568,
      "learning_rate": 0.000762,
      "loss": 0.8004,
      "step": 239
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 0.4994535744190216,
      "learning_rate": 0.0007610000000000001,
      "loss": 0.4823,
      "step": 240
    },
    {
      "epoch": 0.8197278911564626,
      "grad_norm": 0.5277029275894165,
      "learning_rate": 0.00076,
      "loss": 0.5714,
      "step": 241
    },
    {
      "epoch": 0.8231292517006803,
      "grad_norm": 0.4425184428691864,
      "learning_rate": 0.000759,
      "loss": 0.5468,
      "step": 242
    },
    {
      "epoch": 0.826530612244898,
      "grad_norm": 0.449565052986145,
      "learning_rate": 0.000758,
      "loss": 0.5758,
      "step": 243
    },
    {
      "epoch": 0.8299319727891157,
      "grad_norm": 0.41957902908325195,
      "learning_rate": 0.000757,
      "loss": 0.5254,
      "step": 244
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.3957310914993286,
      "learning_rate": 0.000756,
      "loss": 0.5507,
      "step": 245
    },
    {
      "epoch": 0.8367346938775511,
      "grad_norm": 0.44286149740219116,
      "learning_rate": 0.000755,
      "loss": 0.5536,
      "step": 246
    },
    {
      "epoch": 0.8401360544217688,
      "grad_norm": 0.4117234945297241,
      "learning_rate": 0.000754,
      "loss": 0.5067,
      "step": 247
    },
    {
      "epoch": 0.8435374149659864,
      "grad_norm": 0.39145931601524353,
      "learning_rate": 0.000753,
      "loss": 0.5348,
      "step": 248
    },
    {
      "epoch": 0.8469387755102041,
      "grad_norm": 0.37512120604515076,
      "learning_rate": 0.0007520000000000001,
      "loss": 0.5076,
      "step": 249
    },
    {
      "epoch": 0.8503401360544217,
      "grad_norm": 0.40351149439811707,
      "learning_rate": 0.000751,
      "loss": 0.5147,
      "step": 250
    },
    {
      "epoch": 0.8537414965986394,
      "grad_norm": 0.3551856577396393,
      "learning_rate": 0.00075,
      "loss": 0.4606,
      "step": 251
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.3692960739135742,
      "learning_rate": 0.000749,
      "loss": 0.5515,
      "step": 252
    },
    {
      "epoch": 0.8605442176870748,
      "grad_norm": 0.345319002866745,
      "learning_rate": 0.000748,
      "loss": 0.4429,
      "step": 253
    },
    {
      "epoch": 0.8639455782312925,
      "grad_norm": 0.4383028447628021,
      "learning_rate": 0.000747,
      "loss": 0.626,
      "step": 254
    },
    {
      "epoch": 0.8673469387755102,
      "grad_norm": 0.47970402240753174,
      "learning_rate": 0.000746,
      "loss": 0.5893,
      "step": 255
    },
    {
      "epoch": 0.8707482993197279,
      "grad_norm": 0.41267403960227966,
      "learning_rate": 0.000745,
      "loss": 0.4714,
      "step": 256
    },
    {
      "epoch": 0.8741496598639455,
      "grad_norm": 0.40424057841300964,
      "learning_rate": 0.000744,
      "loss": 0.49,
      "step": 257
    },
    {
      "epoch": 0.8775510204081632,
      "grad_norm": 0.4219982326030731,
      "learning_rate": 0.0007430000000000001,
      "loss": 0.548,
      "step": 258
    },
    {
      "epoch": 0.8809523809523809,
      "grad_norm": 0.43608415126800537,
      "learning_rate": 0.000742,
      "loss": 0.4441,
      "step": 259
    },
    {
      "epoch": 0.8843537414965986,
      "grad_norm": 0.44605138897895813,
      "learning_rate": 0.000741,
      "loss": 0.493,
      "step": 260
    },
    {
      "epoch": 0.8877551020408163,
      "grad_norm": 0.40419429540634155,
      "learning_rate": 0.00074,
      "loss": 0.5136,
      "step": 261
    },
    {
      "epoch": 0.891156462585034,
      "grad_norm": 0.4956050217151642,
      "learning_rate": 0.000739,
      "loss": 0.7117,
      "step": 262
    },
    {
      "epoch": 0.8945578231292517,
      "grad_norm": 1.43574857711792,
      "learning_rate": 0.000738,
      "loss": 0.6238,
      "step": 263
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 0.6344363689422607,
      "learning_rate": 0.000737,
      "loss": 0.8503,
      "step": 264
    },
    {
      "epoch": 0.9013605442176871,
      "grad_norm": 0.7301734685897827,
      "learning_rate": 0.000736,
      "loss": 0.7336,
      "step": 265
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 0.5390174984931946,
      "learning_rate": 0.000735,
      "loss": 0.4948,
      "step": 266
    },
    {
      "epoch": 0.9081632653061225,
      "grad_norm": 0.5571431517601013,
      "learning_rate": 0.000734,
      "loss": 0.5132,
      "step": 267
    },
    {
      "epoch": 0.9115646258503401,
      "grad_norm": 0.48324087262153625,
      "learning_rate": 0.000733,
      "loss": 0.6316,
      "step": 268
    },
    {
      "epoch": 0.9149659863945578,
      "grad_norm": 0.4615017771720886,
      "learning_rate": 0.000732,
      "loss": 0.5968,
      "step": 269
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 0.3870164453983307,
      "learning_rate": 0.000731,
      "loss": 0.4665,
      "step": 270
    },
    {
      "epoch": 0.9217687074829932,
      "grad_norm": 0.715110182762146,
      "learning_rate": 0.00073,
      "loss": 0.6873,
      "step": 271
    },
    {
      "epoch": 0.9251700680272109,
      "grad_norm": 0.35432589054107666,
      "learning_rate": 0.000729,
      "loss": 0.4502,
      "step": 272
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 0.3922071158885956,
      "learning_rate": 0.000728,
      "loss": 0.5633,
      "step": 273
    },
    {
      "epoch": 0.9319727891156463,
      "grad_norm": 0.3996284604072571,
      "learning_rate": 0.000727,
      "loss": 0.4766,
      "step": 274
    },
    {
      "epoch": 0.935374149659864,
      "grad_norm": 0.36501798033714294,
      "learning_rate": 0.000726,
      "loss": 0.5277,
      "step": 275
    },
    {
      "epoch": 0.9387755102040817,
      "grad_norm": 0.4204636514186859,
      "learning_rate": 0.000725,
      "loss": 0.7002,
      "step": 276
    },
    {
      "epoch": 0.9421768707482994,
      "grad_norm": 1.0333572626113892,
      "learning_rate": 0.000724,
      "loss": 0.5101,
      "step": 277
    },
    {
      "epoch": 0.9455782312925171,
      "grad_norm": 0.3794856667518616,
      "learning_rate": 0.000723,
      "loss": 0.4319,
      "step": 278
    },
    {
      "epoch": 0.9489795918367347,
      "grad_norm": 0.4260457456111908,
      "learning_rate": 0.000722,
      "loss": 0.5769,
      "step": 279
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.8489633798599243,
      "learning_rate": 0.000721,
      "loss": 0.8239,
      "step": 280
    },
    {
      "epoch": 0.95578231292517,
      "grad_norm": 0.5810376405715942,
      "learning_rate": 0.0007199999999999999,
      "loss": 0.795,
      "step": 281
    },
    {
      "epoch": 0.9591836734693877,
      "grad_norm": 0.4563407599925995,
      "learning_rate": 0.000719,
      "loss": 0.6892,
      "step": 282
    },
    {
      "epoch": 0.9625850340136054,
      "grad_norm": 0.6055763363838196,
      "learning_rate": 0.000718,
      "loss": 0.782,
      "step": 283
    },
    {
      "epoch": 0.9659863945578231,
      "grad_norm": 0.43870481848716736,
      "learning_rate": 0.000717,
      "loss": 0.4863,
      "step": 284
    },
    {
      "epoch": 0.9693877551020408,
      "grad_norm": 0.42391592264175415,
      "learning_rate": 0.000716,
      "loss": 0.4878,
      "step": 285
    },
    {
      "epoch": 0.9727891156462585,
      "grad_norm": 0.3957008421421051,
      "learning_rate": 0.000715,
      "loss": 0.4921,
      "step": 286
    },
    {
      "epoch": 0.9761904761904762,
      "grad_norm": 0.3897944986820221,
      "learning_rate": 0.000714,
      "loss": 0.5067,
      "step": 287
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 0.4360864758491516,
      "learning_rate": 0.000713,
      "loss": 0.5833,
      "step": 288
    },
    {
      "epoch": 0.9829931972789115,
      "grad_norm": 0.6862716674804688,
      "learning_rate": 0.000712,
      "loss": 0.8213,
      "step": 289
    },
    {
      "epoch": 0.9863945578231292,
      "grad_norm": 0.38923320174217224,
      "learning_rate": 0.0007109999999999999,
      "loss": 0.4344,
      "step": 290
    },
    {
      "epoch": 0.9897959183673469,
      "grad_norm": 0.3843585252761841,
      "learning_rate": 0.00071,
      "loss": 0.5038,
      "step": 291
    },
    {
      "epoch": 0.9931972789115646,
      "grad_norm": 0.8874002695083618,
      "learning_rate": 0.000709,
      "loss": 0.5686,
      "step": 292
    },
    {
      "epoch": 0.9965986394557823,
      "grad_norm": 0.7624781727790833,
      "learning_rate": 0.000708,
      "loss": 0.6898,
      "step": 293
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7040931582450867,
      "learning_rate": 0.000707,
      "loss": 0.6736,
      "step": 294
    },
    {
      "epoch": 1.0034013605442176,
      "grad_norm": 0.40800726413726807,
      "learning_rate": 0.0007059999999999999,
      "loss": 0.4537,
      "step": 295
    },
    {
      "epoch": 1.0068027210884354,
      "grad_norm": 0.4512588083744049,
      "learning_rate": 0.000705,
      "loss": 0.5015,
      "step": 296
    },
    {
      "epoch": 1.010204081632653,
      "grad_norm": 0.3979998528957367,
      "learning_rate": 0.000704,
      "loss": 0.5201,
      "step": 297
    },
    {
      "epoch": 1.0136054421768708,
      "grad_norm": 0.45417115092277527,
      "learning_rate": 0.000703,
      "loss": 0.5625,
      "step": 298
    },
    {
      "epoch": 1.0170068027210883,
      "grad_norm": 0.4893382787704468,
      "learning_rate": 0.0007019999999999999,
      "loss": 0.6438,
      "step": 299
    },
    {
      "epoch": 1.0204081632653061,
      "grad_norm": 0.435290664434433,
      "learning_rate": 0.000701,
      "loss": 0.4864,
      "step": 300
    },
    {
      "epoch": 1.0238095238095237,
      "grad_norm": 0.49187761545181274,
      "learning_rate": 0.0007,
      "loss": 0.6875,
      "step": 301
    },
    {
      "epoch": 1.0272108843537415,
      "grad_norm": 0.43112751841545105,
      "learning_rate": 0.000699,
      "loss": 0.4689,
      "step": 302
    },
    {
      "epoch": 1.030612244897959,
      "grad_norm": 0.45311981439590454,
      "learning_rate": 0.0006979999999999999,
      "loss": 0.5614,
      "step": 303
    },
    {
      "epoch": 1.034013605442177,
      "grad_norm": 0.5254350304603577,
      "learning_rate": 0.0006969999999999999,
      "loss": 0.7324,
      "step": 304
    },
    {
      "epoch": 1.0374149659863945,
      "grad_norm": 0.4273296594619751,
      "learning_rate": 0.000696,
      "loss": 0.5714,
      "step": 305
    },
    {
      "epoch": 1.0408163265306123,
      "grad_norm": 0.6309693455696106,
      "learning_rate": 0.000695,
      "loss": 0.6494,
      "step": 306
    },
    {
      "epoch": 1.0442176870748299,
      "grad_norm": 0.39257320761680603,
      "learning_rate": 0.000694,
      "loss": 0.5427,
      "step": 307
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.41000238060951233,
      "learning_rate": 0.0006929999999999999,
      "loss": 0.4817,
      "step": 308
    },
    {
      "epoch": 1.0510204081632653,
      "grad_norm": 0.38099968433380127,
      "learning_rate": 0.000692,
      "loss": 0.4118,
      "step": 309
    },
    {
      "epoch": 1.054421768707483,
      "grad_norm": 0.6279449462890625,
      "learning_rate": 0.000691,
      "loss": 0.781,
      "step": 310
    },
    {
      "epoch": 1.0578231292517006,
      "grad_norm": 0.4309636354446411,
      "learning_rate": 0.00069,
      "loss": 0.5313,
      "step": 311
    },
    {
      "epoch": 1.0612244897959184,
      "grad_norm": 0.6017267107963562,
      "learning_rate": 0.0006889999999999999,
      "loss": 0.7731,
      "step": 312
    },
    {
      "epoch": 1.064625850340136,
      "grad_norm": 0.389698326587677,
      "learning_rate": 0.0006879999999999999,
      "loss": 0.4908,
      "step": 313
    },
    {
      "epoch": 1.0680272108843538,
      "grad_norm": 0.39598727226257324,
      "learning_rate": 0.0006870000000000001,
      "loss": 0.5108,
      "step": 314
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 0.40859025716781616,
      "learning_rate": 0.0006860000000000001,
      "loss": 0.5654,
      "step": 315
    },
    {
      "epoch": 1.0748299319727892,
      "grad_norm": 0.3512226641178131,
      "learning_rate": 0.0006850000000000001,
      "loss": 0.4617,
      "step": 316
    },
    {
      "epoch": 1.0782312925170068,
      "grad_norm": 0.3770955801010132,
      "learning_rate": 0.000684,
      "loss": 0.4957,
      "step": 317
    },
    {
      "epoch": 1.0816326530612246,
      "grad_norm": 0.6281301975250244,
      "learning_rate": 0.000683,
      "loss": 0.6492,
      "step": 318
    },
    {
      "epoch": 1.0850340136054422,
      "grad_norm": 0.37951651215553284,
      "learning_rate": 0.0006820000000000001,
      "loss": 0.5165,
      "step": 319
    },
    {
      "epoch": 1.08843537414966,
      "grad_norm": 0.38276800513267517,
      "learning_rate": 0.0006810000000000001,
      "loss": 0.4804,
      "step": 320
    },
    {
      "epoch": 1.0918367346938775,
      "grad_norm": 0.40269121527671814,
      "learning_rate": 0.00068,
      "loss": 0.5691,
      "step": 321
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.4464435875415802,
      "learning_rate": 0.000679,
      "loss": 0.5744,
      "step": 322
    },
    {
      "epoch": 1.098639455782313,
      "grad_norm": 0.4071480333805084,
      "learning_rate": 0.0006780000000000001,
      "loss": 0.5283,
      "step": 323
    },
    {
      "epoch": 1.1020408163265305,
      "grad_norm": 0.3918437957763672,
      "learning_rate": 0.0006770000000000001,
      "loss": 0.4264,
      "step": 324
    },
    {
      "epoch": 1.1054421768707483,
      "grad_norm": 1.3635525703430176,
      "learning_rate": 0.0006760000000000001,
      "loss": 0.5592,
      "step": 325
    },
    {
      "epoch": 1.1088435374149659,
      "grad_norm": 0.3932267427444458,
      "learning_rate": 0.000675,
      "loss": 0.5241,
      "step": 326
    },
    {
      "epoch": 1.1122448979591837,
      "grad_norm": 0.3795621395111084,
      "learning_rate": 0.000674,
      "loss": 0.4095,
      "step": 327
    },
    {
      "epoch": 1.1156462585034013,
      "grad_norm": 0.40256258845329285,
      "learning_rate": 0.0006730000000000001,
      "loss": 0.487,
      "step": 328
    },
    {
      "epoch": 1.119047619047619,
      "grad_norm": 0.6347457766532898,
      "learning_rate": 0.0006720000000000001,
      "loss": 0.636,
      "step": 329
    },
    {
      "epoch": 1.1224489795918366,
      "grad_norm": 0.41634947061538696,
      "learning_rate": 0.000671,
      "loss": 0.5498,
      "step": 330
    },
    {
      "epoch": 1.1258503401360545,
      "grad_norm": 0.47330915927886963,
      "learning_rate": 0.00067,
      "loss": 0.471,
      "step": 331
    },
    {
      "epoch": 1.129251700680272,
      "grad_norm": 0.6402971744537354,
      "learning_rate": 0.0006690000000000001,
      "loss": 0.7382,
      "step": 332
    },
    {
      "epoch": 1.1326530612244898,
      "grad_norm": 0.6710578799247742,
      "learning_rate": 0.0006680000000000001,
      "loss": 0.7849,
      "step": 333
    },
    {
      "epoch": 1.1360544217687074,
      "grad_norm": 0.5600394606590271,
      "learning_rate": 0.0006670000000000001,
      "loss": 0.6448,
      "step": 334
    },
    {
      "epoch": 1.1394557823129252,
      "grad_norm": 0.41100645065307617,
      "learning_rate": 0.000666,
      "loss": 0.5196,
      "step": 335
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.3861831724643707,
      "learning_rate": 0.000665,
      "loss": 0.504,
      "step": 336
    },
    {
      "epoch": 1.1462585034013606,
      "grad_norm": 0.3683817386627197,
      "learning_rate": 0.0006640000000000001,
      "loss": 0.4525,
      "step": 337
    },
    {
      "epoch": 1.1496598639455782,
      "grad_norm": 0.44346073269844055,
      "learning_rate": 0.0006630000000000001,
      "loss": 0.61,
      "step": 338
    },
    {
      "epoch": 1.153061224489796,
      "grad_norm": 0.693675696849823,
      "learning_rate": 0.000662,
      "loss": 0.769,
      "step": 339
    },
    {
      "epoch": 1.1564625850340136,
      "grad_norm": 0.42861711978912354,
      "learning_rate": 0.000661,
      "loss": 0.6093,
      "step": 340
    },
    {
      "epoch": 1.1598639455782314,
      "grad_norm": 0.7969399094581604,
      "learning_rate": 0.00066,
      "loss": 0.6062,
      "step": 341
    },
    {
      "epoch": 1.163265306122449,
      "grad_norm": 0.7673967480659485,
      "learning_rate": 0.0006590000000000001,
      "loss": 0.6432,
      "step": 342
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.6721010804176331,
      "learning_rate": 0.0006580000000000001,
      "loss": 0.7722,
      "step": 343
    },
    {
      "epoch": 1.1700680272108843,
      "grad_norm": 0.3883205056190491,
      "learning_rate": 0.000657,
      "loss": 0.4751,
      "step": 344
    },
    {
      "epoch": 1.1734693877551021,
      "grad_norm": 0.4216683506965637,
      "learning_rate": 0.000656,
      "loss": 0.6855,
      "step": 345
    },
    {
      "epoch": 1.1768707482993197,
      "grad_norm": 0.3717578649520874,
      "learning_rate": 0.0006550000000000001,
      "loss": 0.497,
      "step": 346
    },
    {
      "epoch": 1.1802721088435375,
      "grad_norm": 0.40219786763191223,
      "learning_rate": 0.0006540000000000001,
      "loss": 0.4781,
      "step": 347
    },
    {
      "epoch": 1.183673469387755,
      "grad_norm": 0.40692979097366333,
      "learning_rate": 0.000653,
      "loss": 0.59,
      "step": 348
    },
    {
      "epoch": 1.1870748299319729,
      "grad_norm": 0.36847159266471863,
      "learning_rate": 0.000652,
      "loss": 0.5359,
      "step": 349
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 0.4211423695087433,
      "learning_rate": 0.000651,
      "loss": 0.5004,
      "step": 350
    },
    {
      "epoch": 1.193877551020408,
      "grad_norm": 0.4115622639656067,
      "learning_rate": 0.0006500000000000001,
      "loss": 0.4906,
      "step": 351
    },
    {
      "epoch": 1.1972789115646258,
      "grad_norm": 0.3641001880168915,
      "learning_rate": 0.0006490000000000001,
      "loss": 0.4894,
      "step": 352
    },
    {
      "epoch": 1.2006802721088436,
      "grad_norm": 0.4943673610687256,
      "learning_rate": 0.000648,
      "loss": 0.6228,
      "step": 353
    },
    {
      "epoch": 1.2040816326530612,
      "grad_norm": 0.4495214521884918,
      "learning_rate": 0.000647,
      "loss": 0.5397,
      "step": 354
    },
    {
      "epoch": 1.2074829931972788,
      "grad_norm": 0.37852492928504944,
      "learning_rate": 0.000646,
      "loss": 0.5304,
      "step": 355
    },
    {
      "epoch": 1.2108843537414966,
      "grad_norm": 0.40121936798095703,
      "learning_rate": 0.0006450000000000001,
      "loss": 0.5461,
      "step": 356
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 0.3602232336997986,
      "learning_rate": 0.000644,
      "loss": 0.4751,
      "step": 357
    },
    {
      "epoch": 1.217687074829932,
      "grad_norm": 0.37253430485725403,
      "learning_rate": 0.000643,
      "loss": 0.5327,
      "step": 358
    },
    {
      "epoch": 1.2210884353741496,
      "grad_norm": 0.4246891140937805,
      "learning_rate": 0.000642,
      "loss": 0.4736,
      "step": 359
    },
    {
      "epoch": 1.2244897959183674,
      "grad_norm": 0.38939180970191956,
      "learning_rate": 0.0006410000000000001,
      "loss": 0.529,
      "step": 360
    },
    {
      "epoch": 1.227891156462585,
      "grad_norm": 0.4046145975589752,
      "learning_rate": 0.00064,
      "loss": 0.5166,
      "step": 361
    },
    {
      "epoch": 1.2312925170068028,
      "grad_norm": 0.3693302273750305,
      "learning_rate": 0.000639,
      "loss": 0.4533,
      "step": 362
    },
    {
      "epoch": 1.2346938775510203,
      "grad_norm": 0.34521347284317017,
      "learning_rate": 0.000638,
      "loss": 0.4646,
      "step": 363
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 0.46562379598617554,
      "learning_rate": 0.000637,
      "loss": 0.4906,
      "step": 364
    },
    {
      "epoch": 1.2414965986394557,
      "grad_norm": 0.36545294523239136,
      "learning_rate": 0.0006360000000000001,
      "loss": 0.4718,
      "step": 365
    },
    {
      "epoch": 1.2448979591836735,
      "grad_norm": 0.4252525568008423,
      "learning_rate": 0.000635,
      "loss": 0.6579,
      "step": 366
    },
    {
      "epoch": 1.248299319727891,
      "grad_norm": 0.3643045127391815,
      "learning_rate": 0.000634,
      "loss": 0.4905,
      "step": 367
    },
    {
      "epoch": 1.251700680272109,
      "grad_norm": 0.3970935344696045,
      "learning_rate": 0.000633,
      "loss": 0.5945,
      "step": 368
    },
    {
      "epoch": 1.2551020408163265,
      "grad_norm": 0.38436582684516907,
      "learning_rate": 0.000632,
      "loss": 0.4953,
      "step": 369
    },
    {
      "epoch": 1.2585034013605443,
      "grad_norm": 0.6076159477233887,
      "learning_rate": 0.000631,
      "loss": 0.7043,
      "step": 370
    },
    {
      "epoch": 1.2619047619047619,
      "grad_norm": 0.33021047711372375,
      "learning_rate": 0.00063,
      "loss": 0.4402,
      "step": 371
    },
    {
      "epoch": 1.2653061224489797,
      "grad_norm": 0.43042343854904175,
      "learning_rate": 0.000629,
      "loss": 0.5828,
      "step": 372
    },
    {
      "epoch": 1.2687074829931972,
      "grad_norm": 0.39316216111183167,
      "learning_rate": 0.000628,
      "loss": 0.5453,
      "step": 373
    },
    {
      "epoch": 1.272108843537415,
      "grad_norm": 0.41656607389450073,
      "learning_rate": 0.0006270000000000001,
      "loss": 0.5801,
      "step": 374
    },
    {
      "epoch": 1.2755102040816326,
      "grad_norm": 0.4150383770465851,
      "learning_rate": 0.000626,
      "loss": 0.4296,
      "step": 375
    },
    {
      "epoch": 1.2789115646258504,
      "grad_norm": 0.3785956799983978,
      "learning_rate": 0.000625,
      "loss": 0.4479,
      "step": 376
    },
    {
      "epoch": 1.282312925170068,
      "grad_norm": 0.6505723595619202,
      "learning_rate": 0.000624,
      "loss": 0.7374,
      "step": 377
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.40604937076568604,
      "learning_rate": 0.000623,
      "loss": 0.6134,
      "step": 378
    },
    {
      "epoch": 1.2891156462585034,
      "grad_norm": 2.4362549781799316,
      "learning_rate": 0.000622,
      "loss": 0.6236,
      "step": 379
    },
    {
      "epoch": 1.2925170068027212,
      "grad_norm": 0.38253313302993774,
      "learning_rate": 0.000621,
      "loss": 0.6165,
      "step": 380
    },
    {
      "epoch": 1.2959183673469388,
      "grad_norm": 0.5368821620941162,
      "learning_rate": 0.00062,
      "loss": 0.7467,
      "step": 381
    },
    {
      "epoch": 1.2993197278911564,
      "grad_norm": 0.3616010248661041,
      "learning_rate": 0.000619,
      "loss": 0.4236,
      "step": 382
    },
    {
      "epoch": 1.3027210884353742,
      "grad_norm": 0.7438325881958008,
      "learning_rate": 0.0006180000000000001,
      "loss": 0.9002,
      "step": 383
    },
    {
      "epoch": 1.306122448979592,
      "grad_norm": 0.4542715549468994,
      "learning_rate": 0.000617,
      "loss": 0.6309,
      "step": 384
    },
    {
      "epoch": 1.3095238095238095,
      "grad_norm": 0.38195356726646423,
      "learning_rate": 0.000616,
      "loss": 0.535,
      "step": 385
    },
    {
      "epoch": 1.3129251700680271,
      "grad_norm": 0.4415518045425415,
      "learning_rate": 0.000615,
      "loss": 0.6222,
      "step": 386
    },
    {
      "epoch": 1.316326530612245,
      "grad_norm": 0.8887052536010742,
      "learning_rate": 0.000614,
      "loss": 0.6608,
      "step": 387
    },
    {
      "epoch": 1.3197278911564627,
      "grad_norm": 0.39624643325805664,
      "learning_rate": 0.000613,
      "loss": 0.4156,
      "step": 388
    },
    {
      "epoch": 1.3231292517006803,
      "grad_norm": 0.40051859617233276,
      "learning_rate": 0.000612,
      "loss": 0.4988,
      "step": 389
    },
    {
      "epoch": 1.3265306122448979,
      "grad_norm": 0.47968071699142456,
      "learning_rate": 0.000611,
      "loss": 0.6207,
      "step": 390
    },
    {
      "epoch": 1.3299319727891157,
      "grad_norm": 0.671830415725708,
      "learning_rate": 0.00061,
      "loss": 0.7986,
      "step": 391
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.3895026743412018,
      "learning_rate": 0.000609,
      "loss": 0.4191,
      "step": 392
    },
    {
      "epoch": 1.336734693877551,
      "grad_norm": 0.8768209218978882,
      "learning_rate": 0.000608,
      "loss": 0.6117,
      "step": 393
    },
    {
      "epoch": 1.3401360544217686,
      "grad_norm": 0.7755725383758545,
      "learning_rate": 0.000607,
      "loss": 0.5713,
      "step": 394
    },
    {
      "epoch": 1.3435374149659864,
      "grad_norm": 0.6575064063072205,
      "learning_rate": 0.000606,
      "loss": 0.6378,
      "step": 395
    },
    {
      "epoch": 1.346938775510204,
      "grad_norm": 0.4170999825000763,
      "learning_rate": 0.000605,
      "loss": 0.51,
      "step": 396
    },
    {
      "epoch": 1.3503401360544218,
      "grad_norm": 0.3921993374824524,
      "learning_rate": 0.000604,
      "loss": 0.4701,
      "step": 397
    },
    {
      "epoch": 1.3537414965986394,
      "grad_norm": 0.47719961404800415,
      "learning_rate": 0.000603,
      "loss": 0.6687,
      "step": 398
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 0.7570953369140625,
      "learning_rate": 0.000602,
      "loss": 0.6439,
      "step": 399
    },
    {
      "epoch": 1.3605442176870748,
      "grad_norm": 0.4186718761920929,
      "learning_rate": 0.000601,
      "loss": 0.6082,
      "step": 400
    },
    {
      "epoch": 1.3639455782312926,
      "grad_norm": 0.46274158358573914,
      "learning_rate": 0.0006,
      "loss": 0.6793,
      "step": 401
    },
    {
      "epoch": 1.3673469387755102,
      "grad_norm": 0.4808526635169983,
      "learning_rate": 0.000599,
      "loss": 0.6776,
      "step": 402
    },
    {
      "epoch": 1.370748299319728,
      "grad_norm": 0.4502228796482086,
      "learning_rate": 0.000598,
      "loss": 0.5208,
      "step": 403
    },
    {
      "epoch": 1.3741496598639455,
      "grad_norm": 0.45415255427360535,
      "learning_rate": 0.000597,
      "loss": 0.5764,
      "step": 404
    },
    {
      "epoch": 1.3775510204081631,
      "grad_norm": 0.44590941071510315,
      "learning_rate": 0.000596,
      "loss": 0.4759,
      "step": 405
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.5980026721954346,
      "learning_rate": 0.0005949999999999999,
      "loss": 0.7325,
      "step": 406
    },
    {
      "epoch": 1.3843537414965987,
      "grad_norm": 0.43080955743789673,
      "learning_rate": 0.000594,
      "loss": 0.67,
      "step": 407
    },
    {
      "epoch": 1.3877551020408163,
      "grad_norm": 0.6027704477310181,
      "learning_rate": 0.000593,
      "loss": 0.7743,
      "step": 408
    },
    {
      "epoch": 1.391156462585034,
      "grad_norm": 0.41942086815834045,
      "learning_rate": 0.000592,
      "loss": 0.5572,
      "step": 409
    },
    {
      "epoch": 1.3945578231292517,
      "grad_norm": 0.35550785064697266,
      "learning_rate": 0.0005909999999999999,
      "loss": 0.4571,
      "step": 410
    },
    {
      "epoch": 1.3979591836734695,
      "grad_norm": 0.3550693988800049,
      "learning_rate": 0.00059,
      "loss": 0.4977,
      "step": 411
    },
    {
      "epoch": 1.401360544217687,
      "grad_norm": 0.3887329697608948,
      "learning_rate": 0.000589,
      "loss": 0.4589,
      "step": 412
    },
    {
      "epoch": 1.4047619047619047,
      "grad_norm": 0.43224555253982544,
      "learning_rate": 0.000588,
      "loss": 0.5604,
      "step": 413
    },
    {
      "epoch": 1.4081632653061225,
      "grad_norm": 0.359114408493042,
      "learning_rate": 0.000587,
      "loss": 0.4737,
      "step": 414
    },
    {
      "epoch": 1.4115646258503403,
      "grad_norm": 0.4268055558204651,
      "learning_rate": 0.0005859999999999999,
      "loss": 0.6447,
      "step": 415
    },
    {
      "epoch": 1.4149659863945578,
      "grad_norm": 0.6068462133407593,
      "learning_rate": 0.000585,
      "loss": 0.5857,
      "step": 416
    },
    {
      "epoch": 1.4183673469387754,
      "grad_norm": 0.40475380420684814,
      "learning_rate": 0.000584,
      "loss": 0.6293,
      "step": 417
    },
    {
      "epoch": 1.4217687074829932,
      "grad_norm": 0.39443063735961914,
      "learning_rate": 0.000583,
      "loss": 0.4933,
      "step": 418
    },
    {
      "epoch": 1.4251700680272108,
      "grad_norm": 1.6135668754577637,
      "learning_rate": 0.0005819999999999999,
      "loss": 0.559,
      "step": 419
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.3952869176864624,
      "learning_rate": 0.0005809999999999999,
      "loss": 0.5662,
      "step": 420
    },
    {
      "epoch": 1.4319727891156462,
      "grad_norm": 0.38459664583206177,
      "learning_rate": 0.00058,
      "loss": 0.5553,
      "step": 421
    },
    {
      "epoch": 1.435374149659864,
      "grad_norm": 0.42242032289505005,
      "learning_rate": 0.000579,
      "loss": 0.5512,
      "step": 422
    },
    {
      "epoch": 1.4387755102040816,
      "grad_norm": 0.7074416279792786,
      "learning_rate": 0.000578,
      "loss": 0.7841,
      "step": 423
    },
    {
      "epoch": 1.4421768707482994,
      "grad_norm": 0.3948712646961212,
      "learning_rate": 0.0005769999999999999,
      "loss": 0.4625,
      "step": 424
    },
    {
      "epoch": 1.445578231292517,
      "grad_norm": 0.39049288630485535,
      "learning_rate": 0.000576,
      "loss": 0.4339,
      "step": 425
    },
    {
      "epoch": 1.4489795918367347,
      "grad_norm": 0.43866077065467834,
      "learning_rate": 0.000575,
      "loss": 0.636,
      "step": 426
    },
    {
      "epoch": 1.4523809523809523,
      "grad_norm": 0.37005776166915894,
      "learning_rate": 0.000574,
      "loss": 0.5546,
      "step": 427
    },
    {
      "epoch": 1.4557823129251701,
      "grad_norm": 1.0540742874145508,
      "learning_rate": 0.0005729999999999999,
      "loss": 0.4696,
      "step": 428
    },
    {
      "epoch": 1.4591836734693877,
      "grad_norm": 0.40099918842315674,
      "learning_rate": 0.0005719999999999999,
      "loss": 0.5235,
      "step": 429
    },
    {
      "epoch": 1.4625850340136055,
      "grad_norm": 0.41524532437324524,
      "learning_rate": 0.000571,
      "loss": 0.4879,
      "step": 430
    },
    {
      "epoch": 1.465986394557823,
      "grad_norm": 0.3626496195793152,
      "learning_rate": 0.00057,
      "loss": 0.4586,
      "step": 431
    },
    {
      "epoch": 1.469387755102041,
      "grad_norm": 0.4890810549259186,
      "learning_rate": 0.000569,
      "loss": 0.6084,
      "step": 432
    },
    {
      "epoch": 1.4727891156462585,
      "grad_norm": 0.3802206218242645,
      "learning_rate": 0.0005679999999999999,
      "loss": 0.5306,
      "step": 433
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 0.49415990710258484,
      "learning_rate": 0.000567,
      "loss": 0.5931,
      "step": 434
    },
    {
      "epoch": 1.4795918367346939,
      "grad_norm": 0.3322899341583252,
      "learning_rate": 0.000566,
      "loss": 0.4291,
      "step": 435
    },
    {
      "epoch": 1.4829931972789114,
      "grad_norm": 0.3742443025112152,
      "learning_rate": 0.000565,
      "loss": 0.4353,
      "step": 436
    },
    {
      "epoch": 1.4863945578231292,
      "grad_norm": 0.3908548057079315,
      "learning_rate": 0.0005639999999999999,
      "loss": 0.5936,
      "step": 437
    },
    {
      "epoch": 1.489795918367347,
      "grad_norm": 0.3927280902862549,
      "learning_rate": 0.0005629999999999999,
      "loss": 0.5871,
      "step": 438
    },
    {
      "epoch": 1.4931972789115646,
      "grad_norm": 0.35931596159935,
      "learning_rate": 0.0005620000000000001,
      "loss": 0.45,
      "step": 439
    },
    {
      "epoch": 1.4965986394557822,
      "grad_norm": 0.34835031628608704,
      "learning_rate": 0.0005610000000000001,
      "loss": 0.5206,
      "step": 440
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.35846203565597534,
      "learning_rate": 0.0005600000000000001,
      "loss": 0.5045,
      "step": 441
    },
    {
      "epoch": 1.5034013605442178,
      "grad_norm": 0.40809065103530884,
      "learning_rate": 0.000559,
      "loss": 0.6142,
      "step": 442
    },
    {
      "epoch": 1.5068027210884354,
      "grad_norm": 0.6810917854309082,
      "learning_rate": 0.000558,
      "loss": 0.8348,
      "step": 443
    },
    {
      "epoch": 1.510204081632653,
      "grad_norm": 0.7647306323051453,
      "learning_rate": 0.0005570000000000001,
      "loss": 0.6421,
      "step": 444
    },
    {
      "epoch": 1.5136054421768708,
      "grad_norm": 0.709998369216919,
      "learning_rate": 0.0005560000000000001,
      "loss": 0.6187,
      "step": 445
    },
    {
      "epoch": 1.5170068027210886,
      "grad_norm": 0.384067565202713,
      "learning_rate": 0.000555,
      "loss": 0.4957,
      "step": 446
    },
    {
      "epoch": 1.5204081632653061,
      "grad_norm": 0.3640534281730652,
      "learning_rate": 0.000554,
      "loss": 0.4512,
      "step": 447
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 0.5762338638305664,
      "learning_rate": 0.0005530000000000001,
      "loss": 0.6424,
      "step": 448
    },
    {
      "epoch": 1.5272108843537415,
      "grad_norm": 0.4416106641292572,
      "learning_rate": 0.0005520000000000001,
      "loss": 0.4576,
      "step": 449
    },
    {
      "epoch": 1.5306122448979593,
      "grad_norm": 0.34828102588653564,
      "learning_rate": 0.0005510000000000001,
      "loss": 0.4128,
      "step": 450
    },
    {
      "epoch": 1.534013605442177,
      "grad_norm": 0.408080130815506,
      "learning_rate": 0.00055,
      "loss": 0.4597,
      "step": 451
    },
    {
      "epoch": 1.5374149659863945,
      "grad_norm": 0.399862140417099,
      "learning_rate": 0.000549,
      "loss": 0.414,
      "step": 452
    },
    {
      "epoch": 1.5408163265306123,
      "grad_norm": 0.4569989740848541,
      "learning_rate": 0.0005480000000000001,
      "loss": 0.7137,
      "step": 453
    },
    {
      "epoch": 1.54421768707483,
      "grad_norm": 0.7800179719924927,
      "learning_rate": 0.0005470000000000001,
      "loss": 0.833,
      "step": 454
    },
    {
      "epoch": 1.5476190476190477,
      "grad_norm": 0.4819735586643219,
      "learning_rate": 0.000546,
      "loss": 0.632,
      "step": 455
    },
    {
      "epoch": 1.5510204081632653,
      "grad_norm": 0.6158478856086731,
      "learning_rate": 0.000545,
      "loss": 0.5483,
      "step": 456
    },
    {
      "epoch": 1.554421768707483,
      "grad_norm": 0.4295263886451721,
      "learning_rate": 0.0005440000000000001,
      "loss": 0.5147,
      "step": 457
    },
    {
      "epoch": 1.5578231292517006,
      "grad_norm": 0.8173981308937073,
      "learning_rate": 0.0005430000000000001,
      "loss": 0.4908,
      "step": 458
    },
    {
      "epoch": 1.5612244897959182,
      "grad_norm": 0.3341985046863556,
      "learning_rate": 0.0005420000000000001,
      "loss": 0.4163,
      "step": 459
    },
    {
      "epoch": 1.564625850340136,
      "grad_norm": 0.469303160905838,
      "learning_rate": 0.000541,
      "loss": 0.5828,
      "step": 460
    },
    {
      "epoch": 1.5680272108843538,
      "grad_norm": 0.456769734621048,
      "learning_rate": 0.00054,
      "loss": 0.6777,
      "step": 461
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.5907778739929199,
      "learning_rate": 0.0005390000000000001,
      "loss": 0.7933,
      "step": 462
    },
    {
      "epoch": 1.574829931972789,
      "grad_norm": 0.4301888644695282,
      "learning_rate": 0.0005380000000000001,
      "loss": 0.4894,
      "step": 463
    },
    {
      "epoch": 1.5782312925170068,
      "grad_norm": 0.47149863839149475,
      "learning_rate": 0.000537,
      "loss": 0.6981,
      "step": 464
    },
    {
      "epoch": 1.5816326530612246,
      "grad_norm": 0.3933788239955902,
      "learning_rate": 0.000536,
      "loss": 0.5377,
      "step": 465
    },
    {
      "epoch": 1.5850340136054422,
      "grad_norm": 0.4726092517375946,
      "learning_rate": 0.000535,
      "loss": 0.5887,
      "step": 466
    },
    {
      "epoch": 1.5884353741496597,
      "grad_norm": 0.38173550367355347,
      "learning_rate": 0.0005340000000000001,
      "loss": 0.525,
      "step": 467
    },
    {
      "epoch": 1.5918367346938775,
      "grad_norm": 0.44351181387901306,
      "learning_rate": 0.000533,
      "loss": 0.6535,
      "step": 468
    },
    {
      "epoch": 1.5952380952380953,
      "grad_norm": 0.34994083642959595,
      "learning_rate": 0.000532,
      "loss": 0.5043,
      "step": 469
    },
    {
      "epoch": 1.598639455782313,
      "grad_norm": 0.4097979962825775,
      "learning_rate": 0.000531,
      "loss": 0.5746,
      "step": 470
    },
    {
      "epoch": 1.6020408163265305,
      "grad_norm": 0.813221275806427,
      "learning_rate": 0.0005300000000000001,
      "loss": 0.7547,
      "step": 471
    },
    {
      "epoch": 1.6054421768707483,
      "grad_norm": 0.3924487829208374,
      "learning_rate": 0.0005290000000000001,
      "loss": 0.5077,
      "step": 472
    },
    {
      "epoch": 1.608843537414966,
      "grad_norm": 0.3554462194442749,
      "learning_rate": 0.000528,
      "loss": 0.4671,
      "step": 473
    },
    {
      "epoch": 1.6122448979591837,
      "grad_norm": 0.9214896559715271,
      "learning_rate": 0.000527,
      "loss": 0.4359,
      "step": 474
    },
    {
      "epoch": 1.6156462585034013,
      "grad_norm": 0.4194357097148895,
      "learning_rate": 0.000526,
      "loss": 0.5034,
      "step": 475
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 0.33744409680366516,
      "learning_rate": 0.0005250000000000001,
      "loss": 0.4196,
      "step": 476
    },
    {
      "epoch": 1.6224489795918369,
      "grad_norm": 0.4333532452583313,
      "learning_rate": 0.000524,
      "loss": 0.4844,
      "step": 477
    },
    {
      "epoch": 1.6258503401360545,
      "grad_norm": 0.41781342029571533,
      "learning_rate": 0.000523,
      "loss": 0.5519,
      "step": 478
    },
    {
      "epoch": 1.629251700680272,
      "grad_norm": 0.346861869096756,
      "learning_rate": 0.000522,
      "loss": 0.5044,
      "step": 479
    },
    {
      "epoch": 1.6326530612244898,
      "grad_norm": 0.3657882809638977,
      "learning_rate": 0.000521,
      "loss": 0.4513,
      "step": 480
    },
    {
      "epoch": 1.6360544217687076,
      "grad_norm": 0.415978342294693,
      "learning_rate": 0.0005200000000000001,
      "loss": 0.5261,
      "step": 481
    },
    {
      "epoch": 1.6394557823129252,
      "grad_norm": 0.32040631771087646,
      "learning_rate": 0.000519,
      "loss": 0.3781,
      "step": 482
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 0.3536011874675751,
      "learning_rate": 0.000518,
      "loss": 0.4078,
      "step": 483
    },
    {
      "epoch": 1.6462585034013606,
      "grad_norm": 0.5787283778190613,
      "learning_rate": 0.000517,
      "loss": 0.7651,
      "step": 484
    },
    {
      "epoch": 1.6496598639455784,
      "grad_norm": 0.416471928358078,
      "learning_rate": 0.0005160000000000001,
      "loss": 0.4902,
      "step": 485
    },
    {
      "epoch": 1.6530612244897958,
      "grad_norm": 0.793480634689331,
      "learning_rate": 0.000515,
      "loss": 0.777,
      "step": 486
    },
    {
      "epoch": 1.6564625850340136,
      "grad_norm": 0.5988873243331909,
      "learning_rate": 0.000514,
      "loss": 0.7917,
      "step": 487
    },
    {
      "epoch": 1.6598639455782314,
      "grad_norm": 0.4109790623188019,
      "learning_rate": 0.000513,
      "loss": 0.5045,
      "step": 488
    },
    {
      "epoch": 1.663265306122449,
      "grad_norm": 0.3379078507423401,
      "learning_rate": 0.000512,
      "loss": 0.3556,
      "step": 489
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.5525921583175659,
      "learning_rate": 0.0005110000000000001,
      "loss": 0.7592,
      "step": 490
    },
    {
      "epoch": 1.6700680272108843,
      "grad_norm": 0.3934229016304016,
      "learning_rate": 0.00051,
      "loss": 0.4396,
      "step": 491
    },
    {
      "epoch": 1.6734693877551021,
      "grad_norm": 0.4166005551815033,
      "learning_rate": 0.000509,
      "loss": 0.5933,
      "step": 492
    },
    {
      "epoch": 1.6768707482993197,
      "grad_norm": 0.43317338824272156,
      "learning_rate": 0.000508,
      "loss": 0.6424,
      "step": 493
    },
    {
      "epoch": 1.6802721088435373,
      "grad_norm": 0.44532108306884766,
      "learning_rate": 0.000507,
      "loss": 0.6689,
      "step": 494
    },
    {
      "epoch": 1.683673469387755,
      "grad_norm": 0.42965441942214966,
      "learning_rate": 0.000506,
      "loss": 0.5236,
      "step": 495
    },
    {
      "epoch": 1.6870748299319729,
      "grad_norm": 0.3567288815975189,
      "learning_rate": 0.000505,
      "loss": 0.4787,
      "step": 496
    },
    {
      "epoch": 1.6904761904761905,
      "grad_norm": 0.6344032287597656,
      "learning_rate": 0.000504,
      "loss": 0.6928,
      "step": 497
    },
    {
      "epoch": 1.693877551020408,
      "grad_norm": 0.3598591089248657,
      "learning_rate": 0.000503,
      "loss": 0.5217,
      "step": 498
    },
    {
      "epoch": 1.6972789115646258,
      "grad_norm": 0.3815860450267792,
      "learning_rate": 0.0005020000000000001,
      "loss": 0.4803,
      "step": 499
    },
    {
      "epoch": 1.7006802721088436,
      "grad_norm": 0.39335206151008606,
      "learning_rate": 0.000501,
      "loss": 0.5247,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1496715362304000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
